"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[244],{2020:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3/chapter-3/isaac-ros-vslam","title":"Isaac ROS and VSLAM","description":"This chapter covers Isaac ROS, NVIDIA\'s hardware-accelerated perception pipeline, and Visual SLAM (VSLAM) for humanoid robots. You\'ll learn how to leverage GPU acceleration for real-time computer vision and navigation.","source":"@site/docs/module-3/chapter-3/isaac-ros-vslam.md","sourceDirName":"module-3/chapter-3","slug":"/module-3/chapter-3/isaac-ros-vslam","permalink":"/ai-robo-learning/ur/docs/module-3/chapter-3/isaac-ros-vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-3/chapter-3/isaac-ros-vslam.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Isaac ROS and VSLAM"},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Sim","permalink":"/ai-robo-learning/ur/docs/module-3/chapter-2/isaac-sim"},"next":{"title":"Navigation with Nav2","permalink":"/ai-robo-learning/ur/docs/module-3/chapter-4/navigation-nav2"}}');var t=r(4848),a=r(8453);const o={sidebar_position:3,title:"Isaac ROS and VSLAM"},i="Isaac ROS and VSLAM",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding Isaac ROS",id:"understanding-isaac-ros",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:3},{value:"Isaac ROS Image Pipeline",id:"isaac-ros-image-pipeline",level:2},{value:"Basic Image Pipeline Setup",id:"basic-image-pipeline-setup",level:3},{value:"Isaac ROS NITROS (Network Interface for Time-sensitive Operations)",id:"isaac-ros-nitros-network-interface-for-time-sensitive-operations",level:3},{value:"Visual SLAM (VSLAM) Concepts",id:"visual-slam-vslam-concepts",level:2},{value:"Understanding Visual SLAM",id:"understanding-visual-slam",level:3},{value:"VSLAM Pipeline Components",id:"vslam-pipeline-components",level:3},{value:"Isaac ROS Visual SLAM Node",id:"isaac-ros-visual-slam-node",level:3},{value:"GPU-Accelerated Computer Vision",id:"gpu-accelerated-computer-vision",level:2},{value:"Isaac ROS Apriltag Detection",id:"isaac-ros-apriltag-detection",level:3},{value:"Integration with Navigation Systems",id:"integration-with-navigation-systems",level:2},{value:"VSLAM to Navigation Bridge",id:"vslam-to-navigation-bridge",level:3},{value:"Hands-on Lab: Isaac ROS VSLAM Integration",id:"hands-on-lab-isaac-ros-vslam-integration",level:2},{value:"Step 1: Create the Complete VSLAM System",id:"step-1-create-the-complete-vslam-system",level:3},{value:"Step 2: Test the VSLAM System",id:"step-2-test-the-vslam-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Isaac ROS Performance Tips",id:"isaac-ros-performance-tips",level:3},{value:"Configuration for Performance",id:"configuration-for-performance",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros-and-vslam",children:"Isaac ROS and VSLAM"})}),"\n",(0,t.jsx)(n.p,{children:"This chapter covers Isaac ROS, NVIDIA's hardware-accelerated perception pipeline, and Visual SLAM (VSLAM) for humanoid robots. You'll learn how to leverage GPU acceleration for real-time computer vision and navigation."}),"\n",(0,t.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,t.jsx)(n.p,{children:"In this chapter, you'll explore:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Isaac ROS architecture and components"}),"\n",(0,t.jsx)(n.li,{children:"Hardware-accelerated perception pipelines"}),"\n",(0,t.jsx)(n.li,{children:"Visual SLAM concepts and implementation"}),"\n",(0,t.jsx)(n.li,{children:"GPU-accelerated computer vision"}),"\n",(0,t.jsx)(n.li,{children:"Integration with navigation systems"}),"\n",(0,t.jsx)(n.li,{children:"Performance optimization techniques"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Module 1-3, Chapters 1-2"}),"\n",(0,t.jsx)(n.li,{children:"NVIDIA GPU with CUDA support"}),"\n",(0,t.jsx)(n.li,{children:"Isaac Sim knowledge"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,t.jsx)(n.li,{children:"Basic computer vision understanding"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"understanding-isaac-ros",children:"Understanding Isaac ROS"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS is NVIDIA's collection of hardware-accelerated perception packages that run on Jetson and GPU platforms. It includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": Hardware-accelerated image processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": GPU-accelerated AprilTag detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": Hardware-accelerated visual-inertial SLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS NITROS"}),": Network Interface for Time-sensitive, Responsive Operations in Switched networks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Manipulators"}),": GPU-accelerated manipulation algorithms"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Camera] --\x3e [Image Pipeline] --\x3e [VSLAM] --\x3e [Localization & Mapping]\n                |                   |\n            [Preprocessing]    [Feature Extraction]\n                |                   |\n            [GPU Acceleration]  [GPU Acceleration]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-image-pipeline",children:"Isaac ROS Image Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"basic-image-pipeline-setup",children:"Basic Image Pipeline Setup"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\n\nclass IsaacROSImagePipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_image_pipeline\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera images\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Publisher for processed images\n        self.processed_pub = self.create_publisher(\n            Image,\n            \'/camera/processed\',\n            10\n        )\n\n        # Publisher for feature points\n        self.features_pub = self.create_publisher(\n            # In practice, this would be a custom message for features\n            Image,  # Using Image as placeholder\n            \'/camera/features\',\n            10\n        )\n\n        self.get_logger().info(\'Isaac ROS Image Pipeline initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Apply hardware-accelerated preprocessing\n            processed_image = self.preprocess_image(cv_image)\n\n            # Extract features (simplified version)\n            features = self.extract_features(processed_image)\n\n            # Publish processed image\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, "bgr8")\n            processed_msg.header = msg.header\n            self.processed_pub.publish(processed_msg)\n\n            # Publish features (simplified)\n            features_msg = self.bridge.cv2_to_imgmsg(features, "mono8")\n            features_msg.header = msg.header\n            self.features_pub.publish(features_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {str(e)}\')\n\n    def preprocess_image(self, image):\n        """Apply preprocessing optimized for GPU acceleration"""\n        # In Isaac ROS, this would use hardware-accelerated operations\n        # For simulation, we\'ll use OpenCV operations\n\n        # Resize image if needed\n        h, w = image.shape[:2]\n        if w > 1280 or h > 720:  # Downscale large images\n            image = cv2.resize(image, (1280, 720))\n\n        # Apply basic preprocessing\n        # In Isaac ROS, this would be done with CUDA kernels\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        return gray\n\n    def extract_features(self, gray_image):\n        """Extract visual features from image"""\n        # In Isaac ROS, feature extraction is hardware-accelerated\n        # Using ORB as an example (in practice, Isaac ROS uses more advanced methods)\n\n        # Create ORB detector\n        orb = cv2.ORB_create(nfeatures=500)\n\n        # Find keypoints and descriptors\n        keypoints, descriptors = orb.detectAndCompute(gray_image, None)\n\n        # Draw keypoints on image\n        features_image = cv2.drawKeypoints(\n            gray_image, keypoints, None,\n            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n        )\n\n        return features_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSImagePipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-nitros-network-interface-for-time-sensitive-operations",children:"Isaac ROS NITROS (Network Interface for Time-sensitive Operations)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom rclpy.qos import QoSProfile, QoSDurabilityPolicy, QoSReliabilityPolicy\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacROSNitrosPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_nitros_pipeline\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Configure QoS profiles for time-sensitive operations\n        qos_profile = QoSProfile(\n            depth=1,\n            durability=QoSDurabilityPolicy.VOLATILE,\n            reliability=QoSReliabilityPolicy.BEST_EFFORT\n        )\n\n        # Subscribe to camera data with optimized QoS\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            qos_profile\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.info_callback,\n            qos_profile\n        )\n\n        # Publisher for optimized pipeline\n        self.optimized_pub = self.create_publisher(\n            Image,\n            \'/camera/optimized_output\',\n            qos_profile\n        )\n\n        # Store camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        self.get_logger().info(\'Isaac ROS NITROS Pipeline initialized\')\n\n    def info_callback(self, msg):\n        """Store camera calibration parameters"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        """Process image with optimized pipeline"""\n        try:\n            # Convert to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Apply optimized preprocessing pipeline\n            if self.camera_matrix is not None:\n                # Undistort image if calibration is available\n                cv_image = cv2.undistort(\n                    cv_image,\n                    self.camera_matrix,\n                    self.distortion_coeffs\n                )\n\n            # Apply hardware-accelerated operations\n            processed_image = self.hardware_accelerated_processing(cv_image)\n\n            # Publish with optimized QoS\n            output_msg = self.bridge.cv2_to_imgmsg(processed_image, "bgr8")\n            output_msg.header = msg.header\n            self.optimized_pub.publish(output_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in NITROS pipeline: {str(e)}\')\n\n    def hardware_accelerated_processing(self, image):\n        """Simulate hardware-accelerated processing"""\n        # In Isaac ROS, this would use CUDA/TensorRT acceleration\n        # For simulation, we\'ll use optimized OpenCV operations\n\n        # Edge detection (can be hardware accelerated)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Feature extraction (hardware accelerated in Isaac ROS)\n        orb = cv2.ORB_create(nfeatures=300)\n        keypoints, descriptors = orb.detectAndCompute(gray, None)\n\n        # Draw features on image\n        result = cv2.drawKeypoints(\n            image, keypoints, None,\n            color=(0, 255, 0),\n            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n        )\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSNitrosPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visual-slam-vslam-concepts",children:"Visual SLAM (VSLAM) Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-visual-slam",children:"Understanding Visual SLAM"}),"\n",(0,t.jsx)(n.p,{children:"Visual SLAM combines visual information from cameras with sensor fusion to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a map of the environment"}),"\n",(0,t.jsx)(n.li,{children:"Localize the robot within that map"}),"\n",(0,t.jsx)(n.li,{children:"Track the robot's movement over time"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"vslam-pipeline-components",children:"VSLAM Pipeline Components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Detection"}),": Identify distinctive points in images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Matching"}),": Match features across frames"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pose Estimation"}),": Estimate camera/robot pose"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mapping"}),": Build a 3D map of the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Loop Closure"}),": Detect when returning to known locations"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-visual-slam-node",children:"Isaac ROS Visual SLAM Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom collections import deque\nimport threading\n\nclass IsaacROSVisualSLAM(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_visual_slam\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera and IMU data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_odom\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/visual_pose\', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # VSLAM state\n        self.prev_image = None\n        self.prev_keypoints = None\n        self.trajectory = deque(maxlen=1000)\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\n        self.imu_data = None\n\n        # Feature detector (ORB in this example)\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n\n        # Lock for thread safety\n        self.slam_lock = threading.Lock()\n\n        self.get_logger().info(\'Isaac ROS Visual SLAM initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming camera image for SLAM"""\n        with self.slam_lock:\n            try:\n                # Convert to OpenCV\n                current_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n                if self.prev_image is not None:\n                    # Extract features from current image\n                    current_keypoints, current_descriptors = self.feature_detector.detectAndCompute(\n                        current_image, None\n                    )\n\n                    if (self.prev_keypoints is not None and\n                        current_keypoints is not None and\n                        len(self.prev_keypoints) > 10 and\n                        len(current_keypoints) > 10):\n\n                        # Match features between previous and current frames\n                        matches = self.match_features(\n                            self.prev_descriptors, current_descriptors\n                        )\n\n                        if len(matches) >= 10:  # Need enough matches for reliable pose\n                            # Estimate motion between frames\n                            motion = self.estimate_motion(\n                                self.prev_keypoints, current_keypoints, matches\n                            )\n\n                            if motion is not None:\n                                # Update pose\n                                self.update_pose(motion)\n\n                                # Publish odometry\n                                self.publish_odometry(msg.header)\n\n                                # Publish TF transform\n                                self.publish_transform(msg.header)\n\n                # Store current frame data for next iteration\n                self.prev_image = current_image\n                self.prev_keypoints = self.feature_detector.detect(current_image, None)\n                self.prev_descriptors = self.feature_detector.compute(\n                    current_image, self.prev_keypoints\n                )[1]\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in VSLAM: {str(e)}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for sensor fusion"""\n        self.imu_data = {\n            \'angular_velocity\': np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ]),\n            \'linear_acceleration\': np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ])\n        }\n\n    def match_features(self, descriptors1, descriptors2):\n        """Match features between two sets of descriptors"""\n        if descriptors1 is None or descriptors2 is None:\n            return []\n\n        # Use FLANN matcher for efficient matching\n        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n        matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.75 * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def estimate_motion(self, prev_kp, curr_kp, matches):\n        """Estimate motion between two frames"""\n        if len(matches) < 10:\n            return None\n\n        # Get matched points\n        prev_points = np.float32([prev_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        curr_points = np.float32([curr_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate essential matrix\n        E, mask = cv2.findEssentialMat(\n            curr_points, prev_points,\n            focal=500, pp=(320, 240), method=cv2.RANSAC, prob=0.999, threshold=1.0\n        )\n\n        if E is not None:\n            # Recover pose from essential matrix\n            _, R, t, _ = cv2.recoverPose(E, curr_points, prev_points)\n\n            # Create transformation matrix\n            transform = np.eye(4)\n            transform[:3, :3] = R\n            transform[:3, 3] = t.flatten()\n\n            return transform\n        else:\n            return None\n\n    def update_pose(self, motion):\n        """Update the current pose based on motion"""\n        self.current_pose = self.current_pose @ motion\n\n        # Store in trajectory\n        position = self.current_pose[:3, 3]\n        self.trajectory.append(position)\n\n    def publish_odometry(self, header):\n        """Publish odometry message"""\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = "map"\n        odom_msg.child_frame_id = "base_link"\n\n        # Set position\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Convert rotation matrix to quaternion\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        odom_msg.pose.pose.orientation.w = qw\n        odom_msg.pose.pose.orientation.x = qx\n        odom_msg.pose.pose.orientation.y = qy\n        odom_msg.pose.pose.orientation.z = qz\n\n        self.odom_pub.publish(odom_msg)\n\n    def publish_transform(self, header):\n        """Publish TF transform"""\n        t = TransformStamped()\n        t.header.stamp = header.stamp\n        t.header.frame_id = "map"\n        t.child_frame_id = "base_link"\n\n        t.transform.translation.x = self.current_pose[0, 3]\n        t.transform.translation.y = self.current_pose[1, 3]\n        t.transform.translation.z = self.current_pose[2, 3]\n\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        t.transform.rotation.w = qw\n        t.transform.rotation.x = qx\n        t.transform.rotation.y = qy\n        t.transform.rotation.z = qz\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSVisualSLAM()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"gpu-accelerated-computer-vision",children:"GPU-Accelerated Computer Vision"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-apriltag-detection",children:"Isaac ROS Apriltag Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseArray, Pose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacROSApriltagDetector(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_apriltag_detector\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.info_callback,\n            10\n        )\n\n        # Publisher for detected tags\n        self.tag_pub = self.create_publisher(PoseArray, \'/apriltag_poses\', 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # AprilTag detector parameters\n        self.tag_size = 0.16  # 16cm tag size\n\n        # In Isaac ROS, this would use hardware-accelerated AprilTag detection\n        # For simulation, we\'ll use the apriltag library\n        try:\n            import apriltag\n            self.detector = apriltag.Detector()\n        except ImportError:\n            self.get_logger().error("AprilTag library not found. Install with: pip install apriltag")\n            self.detector = None\n\n        self.get_logger().info(\'Isaac ROS AprilTag Detector initialized\')\n\n    def info_callback(self, msg):\n        """Store camera calibration parameters"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        """Process image for AprilTag detection"""\n        if self.detector is None:\n            return\n\n        try:\n            # Convert to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n            # Detect AprilTags\n            detections = self.detector.detect(cv_image)\n\n            if detections:\n                # Create pose array for detected tags\n                pose_array = PoseArray()\n                pose_array.header = msg.header\n                pose_array.header.frame_id = "camera_link"\n\n                for detection in detections:\n                    # Get tag pose if camera parameters are available\n                    if self.camera_matrix is not None:\n                        pose = self.estimate_tag_pose(detection)\n                        if pose:\n                            pose_array.poses.append(pose)\n\n                # Publish detected tag poses\n                self.tag_pub.publish(pose_array)\n\n                self.get_logger().info(f\'Detected {len(detections)} AprilTag(s)\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in AprilTag detection: {str(e)}\')\n\n    def estimate_tag_pose(self, detection):\n        """Estimate 3D pose of AprilTag"""\n        if self.camera_matrix is None:\n            return None\n\n        # Get tag corners\n        corners = detection.corners.astype(np.float32)\n\n        # Object points (3D points of tag corners in tag coordinate system)\n        obj_points = np.array([\n            [-self.tag_size/2, -self.tag_size/2, 0],\n            [self.tag_size/2, -self.tag_size/2, 0],\n            [self.tag_size/2, self.tag_size/2, 0],\n            [-self.tag_size/2, self.tag_size/2, 0]\n        ], dtype=np.float32)\n\n        # Image points (2D points of tag corners in image)\n        img_points = corners\n\n        # Solve PnP to get rotation and translation vectors\n        success, rvec, tvec = cv2.solvePnP(\n            obj_points, img_points,\n            self.camera_matrix, self.distortion_coeffs\n        )\n\n        if success:\n            # Convert rotation vector to rotation matrix\n            R, _ = cv2.Rodrigues(rvec)\n\n            # Create pose message\n            pose = Pose()\n            pose.position.x = tvec[0, 0]\n            pose.position.y = tvec[1, 0]\n            pose.position.z = tvec[2, 0]\n\n            # Convert rotation matrix to quaternion\n            qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n            pose.orientation.w = qw\n            pose.orientation.x = qx\n            pose.orientation.y = qy\n            pose.orientation.z = qz\n\n            return pose\n\n        return None\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSApriltagDetector()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-navigation-systems",children:"Integration with Navigation Systems"}),"\n",(0,t.jsx)(n.h3,{id:"vslam-to-navigation-bridge",children:"VSLAM to Navigation Bridge"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nfrom nav_msgs.msg import Odometry, Path\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom tf2_ros import TransformListener, Buffer\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom collections import deque\n\nclass VSLAMToNavigationBridge(Node):\n    def __init__(self):\n        super().__init__('vslam_to_nav_bridge')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to VSLAM output\n        self.vslam_pose_sub = self.create_subscription(\n            PoseStamped,\n            '/visual_pose',\n            self.vslam_pose_callback,\n            10\n        )\n\n        self.vslam_odom_sub = self.create_subscription(\n            Odometry,\n            '/visual_odom',\n            self.vslam_odom_callback,\n            10\n        )\n\n        # Publishers for navigation system\n        self.nav_pose_pub = self.create_publisher(\n            PoseStamped, '/amcl_pose', 10\n        )\n\n        self.nav_path_pub = self.create_publisher(\n            Path, '/trajectory_path', 10\n        )\n\n        self.nav_marker_pub = self.create_publisher(\n            MarkerArray, '/vslam_markers', 10\n        )\n\n        # TF listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Store trajectory\n        self.trajectory = deque(maxlen=1000)\n        self.last_pose_time = self.get_clock().now()\n\n        self.get_logger().info('VSLAM to Navigation Bridge initialized')\n\n    def vslam_pose_callback(self, msg):\n        \"\"\"Process VSLAM pose and forward to navigation\"\"\"\n        # Convert VSLAM pose to navigation format\n        nav_pose = PoseStamped()\n        nav_pose.header = msg.header\n        nav_pose.pose = msg.pose\n\n        # Publish to navigation system\n        self.nav_pose_pub.publish(nav_pose)\n\n        # Add to trajectory\n        self.trajectory.append({\n            'position': (msg.pose.position.x, msg.pose.position.y, msg.pose.position.z),\n            'timestamp': msg.header.stamp\n        })\n\n        # Publish trajectory path\n        self.publish_trajectory_path()\n\n    def vslam_odom_callback(self, msg):\n        \"\"\"Process VSLAM odometry\"\"\"\n        # In a real system, you might want to fuse VSLAM with other odometry sources\n        # For now, we'll just log the odometry\n        self.get_logger().info(\n            f'VSLAM Odometry: ({msg.pose.pose.position.x:.2f}, '\n            f'{msg.pose.pose.position.y:.2f}, {msg.pose.pose.position.z:.2f})'\n        )\n\n    def publish_trajectory_path(self):\n        \"\"\"Publish trajectory as Path message\"\"\"\n        if len(self.trajectory) < 2:\n            return\n\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \"map\"\n\n        for point_data in self.trajectory:\n            pose_stamped = PoseStamped()\n            pose_stamped.header.frame_id = \"map\"\n            pose_stamped.pose.position.x = point_data['position'][0]\n            pose_stamped.pose.position.y = point_data['position'][1]\n            pose_stamped.pose.position.z = point_data['position'][2]\n\n            # Set orientation to identity (for now)\n            pose_stamped.pose.orientation.w = 1.0\n\n            path_msg.poses.append(pose_stamped)\n\n        self.nav_path_pub.publish(path_msg)\n\n    def publish_vslam_markers(self):\n        \"\"\"Publish visualization markers for VSLAM features\"\"\"\n        marker_array = MarkerArray()\n\n        # Create markers for trajectory\n        trajectory_marker = Marker()\n        trajectory_marker.header.frame_id = \"map\"\n        trajectory_marker.header.stamp = self.get_clock().now().to_msg()\n        trajectory_marker.ns = \"vslam_trajectory\"\n        trajectory_marker.id = 0\n        trajectory_marker.type = Marker.LINE_STRIP\n        trajectory_marker.action = Marker.ADD\n\n        trajectory_marker.pose.orientation.w = 1.0\n        trajectory_marker.scale.x = 0.05  # Line width\n\n        trajectory_marker.color.r = 0.0\n        trajectory_marker.color.g = 1.0\n        trajectory_marker.color.b = 0.0\n        trajectory_marker.color.a = 1.0\n\n        for point_data in self.trajectory:\n            point = PointStamped()\n            point.point.x = point_data['position'][0]\n            point.point.y = point_data['position'][1]\n            point.point.z = point_data['position'][2]\n            trajectory_marker.points.append(point.point)\n\n        marker_array.markers.append(trajectory_marker)\n\n        self.nav_marker_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VSLAMToNavigationBridge()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-lab-isaac-ros-vslam-integration",children:"Hands-on Lab: Isaac ROS VSLAM Integration"}),"\n",(0,t.jsx)(n.p,{children:"In this lab, you'll create a complete VSLAM system that integrates with navigation."}),"\n",(0,t.jsx)(n.h3,{id:"step-1-create-the-complete-vslam-system",children:"Step 1: Create the Complete VSLAM System"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"complete_vslam_system.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Odometry\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom collections import deque\nimport threading\nimport time\n\nclass CompleteVSLAMSystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_vslam_system\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to sensors\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.info_callback,\n            10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'/vslam_odom\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/vslam_pose\', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # VSLAM state\n        self.prev_image = None\n        self.prev_keypoints = None\n        self.prev_descriptors = None\n        self.trajectory = deque(maxlen=1000)\n        self.current_pose = np.eye(4)\n        self.imu_data = None\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Feature detector and matcher\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # Lock for thread safety\n        self.vslam_lock = threading.Lock()\n\n        # Timing\n        self.last_process_time = time.time()\n\n        self.get_logger().info(\'Complete VSLAM System initialized\')\n\n    def info_callback(self, msg):\n        """Store camera calibration parameters"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def imu_callback(self, msg):\n        """Process IMU data for sensor fusion"""\n        self.imu_data = {\n            \'angular_velocity\': np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ]),\n            \'linear_acceleration\': np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ]),\n            \'orientation\': np.array([\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z,\n                msg.orientation.w\n            ])\n        }\n\n    def image_callback(self, msg):\n        """Process incoming camera image for VSLAM"""\n        with self.vslam_lock:\n            try:\n                # Convert to OpenCV\n                current_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n                # Check processing rate to avoid overwhelming the system\n                current_time = time.time()\n                if current_time - self.last_process_time < 0.1:  # 10 Hz max\n                    return\n                self.last_process_time = current_time\n\n                if self.prev_image is not None and self.camera_matrix is not None:\n                    # Extract features from current image\n                    current_keypoints, current_descriptors = self.feature_detector.detectAndCompute(\n                        current_image, None\n                    )\n\n                    if (self.prev_keypoints is not None and\n                        current_keypoints is not None and\n                        current_descriptors is not None and\n                        len(self.prev_keypoints) > 10 and\n                        len(current_keypoints) > 10):\n\n                        # Match features between previous and current frames\n                        matches = self.matcher.knnMatch(\n                            self.prev_descriptors, current_descriptors, k=2\n                        )\n\n                        # Apply Lowe\'s ratio test\n                        good_matches = []\n                        for match_pair in matches:\n                            if len(match_pair) == 2:\n                                m, n = match_pair\n                                if m.distance < 0.75 * n.distance:\n                                    good_matches.append(m)\n\n                        if len(good_matches) >= 10:  # Need enough matches for reliable pose\n                            # Get matched points\n                            prev_points = np.float32([\n                                self.prev_keypoints[m.queryIdx].pt for m in good_matches\n                            ]).reshape(-1, 1, 2)\n                            curr_points = np.float32([\n                                current_keypoints[m.trainIdx].pt for m in good_matches\n                            ]).reshape(-1, 1, 2)\n\n                            # Estimate essential matrix\n                            E, mask = cv2.findEssentialMat(\n                                curr_points, prev_points,\n                                cameraMatrix=self.camera_matrix,\n                                method=cv2.RANSAC,\n                                prob=0.999,\n                                threshold=1.0\n                            )\n\n                            if E is not None:\n                                # Recover pose from essential matrix\n                                _, R, t, _ = cv2.recoverPose(\n                                    E, curr_points, prev_points,\n                                    cameraMatrix=self.camera_matrix\n                                )\n\n                                # Create transformation matrix\n                                motion = np.eye(4)\n                                motion[:3, :3] = R\n                                motion[:3, 3] = t.flatten()\n\n                                # Apply motion to current pose\n                                self.current_pose = self.current_pose @ motion\n\n                                # Store in trajectory\n                                position = self.current_pose[:3, 3]\n                                self.trajectory.append({\n                                    \'position\': position,\n                                    \'timestamp\': msg.header.stamp\n                                })\n\n                                # Publish odometry and pose\n                                self.publish_odometry(msg.header)\n                                self.publish_pose(msg.header)\n\n                                self.get_logger().info(\n                                    f\'VSLAM: Position (x={position[0]:.2f}, \'\n                                    f\'y={position[1]:.2f}, z={position[2]:.2f}), \'\n                                    f\'Matches: {len(good_matches)}\'\n                                )\n\n                # Store current frame data for next iteration\n                self.prev_image = current_image\n                self.prev_keypoints = current_keypoints\n                self.prev_descriptors = current_descriptors\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in VSLAM processing: {str(e)}\')\n\n    def publish_odometry(self, header):\n        """Publish odometry message"""\n        odom_msg = Odometry()\n        odom_msg.header = header\n        odom_msg.header.frame_id = "map"\n        odom_msg.child_frame_id = "base_link"\n\n        # Set position\n        odom_msg.pose.pose.position.x = float(self.current_pose[0, 3])\n        odom_msg.pose.pose.position.y = float(self.current_pose[1, 3])\n        odom_msg.pose.pose.position.z = float(self.current_pose[2, 3])\n\n        # Convert rotation matrix to quaternion\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        odom_msg.pose.pose.orientation.w = qw\n        odom_msg.pose.pose.orientation.x = qx\n        odom_msg.pose.pose.orientation.y = qy\n        odom_msg.pose.pose.orientation.z = qz\n\n        # Set zero velocity (in practice, estimate from pose differences)\n        odom_msg.twist.twist.linear.x = 0.0\n        odom_msg.twist.twist.linear.y = 0.0\n        odom_msg.twist.twist.linear.z = 0.0\n        odom_msg.twist.twist.angular.x = 0.0\n        odom_msg.twist.twist.angular.y = 0.0\n        odom_msg.twist.twist.angular.z = 0.0\n\n        self.odom_pub.publish(odom_msg)\n\n    def publish_pose(self, header):\n        """Publish pose message"""\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = "map"\n\n        pose_msg.pose.position.x = float(self.current_pose[0, 3])\n        pose_msg.pose.position.y = float(self.current_pose[1, 3])\n        pose_msg.pose.position.z = float(self.current_pose[2, 3])\n\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        pose_msg.pose.orientation.w = qw\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n\n        self.pose_pub.publish(pose_msg)\n\n        # Publish TF transform\n        self.publish_transform(header)\n\n    def publish_transform(self, header):\n        """Publish TF transform"""\n        from geometry_msgs.msg import TransformStamped\n        t = TransformStamped()\n        t.header.stamp = header.stamp\n        t.header.frame_id = "map"\n        t.child_frame_id = "base_link"\n\n        t.transform.translation.x = float(self.current_pose[0, 3])\n        t.transform.translation.y = float(self.current_pose[1, 3])\n        t.transform.translation.z = float(self.current_pose[2, 3])\n\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        t.transform.rotation.w = qw\n        t.transform.rotation.x = qx\n        t.transform.rotation.y = qy\n        t.transform.rotation.z = qz\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CompleteVSLAMSystem()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down VSLAM system...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-test-the-vslam-system",children:"Step 2: Test the VSLAM System"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Make sure you have the required dependencies:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip3 install opencv-python apriltag numpy\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Run the VSLAM system:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python3 complete_vslam_system.py\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"In another terminal, play a camera feed or use a simulated camera:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# If using a real camera\nros2 run v4l2_camera v4l2_camera_node\n\n# Or use a sample image sequence\nros2 run image_publisher image_publisher_node /path/to/image/directory\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-performance-tips",children:"Isaac ROS Performance Tips"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pipeline Optimization"}),": Use NITROS for time-sensitive operations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Management"}),": Reuse buffers and minimize memory allocations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Utilization"}),": Maximize GPU occupancy for compute-intensive tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Threading"}),": Use appropriate threading models for different components"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Compression"}),": Compress data when transmitting over networks"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"configuration-for-performance",children:"Configuration for Performance"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, QoSHistoryPolicy, QoSReliabilityPolicy\n\nclass IsaacROSOptimizedConfig(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_optimized_config')\n\n        # Optimized QoS profiles for different data types\n        self.high_freq_qos = QoSProfile(\n            history=QoSHistoryPolicy.KEEP_LAST,\n            depth=1,\n            reliability=QoSReliabilityPolicy.BEST_EFFORT\n        )\n\n        self.low_freq_qos = QoSProfile(\n            history=QoSHistoryPolicy.KEEP_LAST,\n            depth=10,\n            reliability=QoSReliabilityPolicy.RELIABLE\n        )\n\n        # Apply performance optimizations\n        self.apply_performance_config()\n\n    def apply_performance_config(self):\n        \"\"\"Apply performance optimizations\"\"\"\n        # Set CPU affinity for critical threads\n        import os\n        os.sched_setaffinity(0, [0, 1])  # Bind to first two CPU cores\n\n        # Set process priority\n        import psutil\n        p = psutil.Process()\n        p.nice(-10)  # Higher priority\n\n        self.get_logger().info('Performance optimizations applied')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSOptimizedConfig()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Acceleration"}),": Leverage GPU/CUDA acceleration whenever possible"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pipeline Design"}),": Design efficient data pipelines with minimal bottlenecks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Calibration"}),": Ensure proper camera calibration for accurate VSLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Management"}),": Use appropriate feature detectors for your environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Combine VSLAM with IMU and other sensors for robustness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validation"}),": Continuously validate SLAM results against ground truth when available"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you'll be ready to learn about Nav2 path planning for humanoid robots in Chapter 4."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var s=r(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);