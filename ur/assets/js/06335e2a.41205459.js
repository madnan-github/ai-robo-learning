"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[994],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},9436:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/chapter-4/computer-vision-robotics","title":"Computer Vision for Robotics","description":"This chapter covers computer vision techniques specifically designed for robotics applications, including object detection, tracking, recognition, and 3D scene understanding that enable robots to perceive and interact with their environment.","source":"@site/docs/module-4/chapter-4/computer-vision-robotics.md","sourceDirName":"module-4/chapter-4","slug":"/module-4/chapter-4/computer-vision-robotics","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-4/computer-vision-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-4/chapter-4/computer-vision-robotics.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Computer Vision for Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-3/cognitive-planning-llms"},"next":{"title":"Object Manipulation","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-5/object-manipulation"}}');var r=i(4848),o=i(8453);const s={sidebar_position:4,title:"Computer Vision for Robotics"},a="Computer Vision for Robotics",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Robot Vision Systems and Camera Calibration",id:"robot-vision-systems-and-camera-calibration",level:2},{value:"Camera Calibration for Robotics",id:"camera-calibration-for-robotics",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"YOLO-based Object Detection for Robotics",id:"yolo-based-object-detection-for-robotics",level:3},{value:"Visual Tracking and SLAM",id:"visual-tracking-and-slam",level:2},{value:"Feature-Based Visual Tracking",id:"feature-based-visual-tracking",level:3},{value:"3D Reconstruction and Depth Estimation",id:"3d-reconstruction-and-depth-estimation",level:2},{value:"Stereo Vision and 3D Reconstruction",id:"stereo-vision-and-3d-reconstruction",level:3},{value:"Vision-Based Manipulation",id:"vision-based-manipulation",level:2},{value:"Object Manipulation with Vision Feedback",id:"object-manipulation-with-vision-feedback",level:3},{value:"Real-time Vision Optimization",id:"real-time-vision-optimization",level:2},{value:"Optimized Vision Processing Pipeline",id:"optimized-vision-processing-pipeline",level:3},{value:"Hands-on Lab: Complete Vision System",id:"hands-on-lab-complete-vision-system",level:2},{value:"Step 1: Create the Vision System Launch File",id:"step-1-create-the-vision-system-launch-file",level:3},{value:"Step 2: Create the Complete Vision Node",id:"step-2-create-the-complete-vision-node",level:3},{value:"Step 3: Test the Vision System",id:"step-3-test-the-vision-system",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"computer-vision-for-robotics",children:"Computer Vision for Robotics"})}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers computer vision techniques specifically designed for robotics applications, including object detection, tracking, recognition, and 3D scene understanding that enable robots to perceive and interact with their environment."}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you'll explore:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robot vision systems and camera calibration"}),"\n",(0,r.jsx)(n.li,{children:"Object detection and recognition for robotics"}),"\n",(0,r.jsx)(n.li,{children:"Visual tracking and SLAM"}),"\n",(0,r.jsx)(n.li,{children:"3D reconstruction and depth estimation"}),"\n",(0,r.jsx)(n.li,{children:"Vision-based manipulation"}),"\n",(0,r.jsx)(n.li,{children:"Real-time vision optimization"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completion of Module 1-4, Chapters 1-3"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ROS 2 messaging"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of image processing"}),"\n",(0,r.jsx)(n.li,{children:"Experience with Python and OpenCV"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of coordinate systems and transformations"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"robot-vision-systems-and-camera-calibration",children:"Robot Vision Systems and Camera Calibration"}),"\n",(0,r.jsx)(n.h3,{id:"camera-calibration-for-robotics",children:"Camera Calibration for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Camera calibration is crucial for accurate vision-based robotics applications:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass CameraCalibrator(Node):\n    def __init__(self):\n        super().__init__('camera_calibrator')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publisher for camera info\n        self.info_pub = self.create_publisher(CameraInfo, '/camera/camera_info', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Calibration parameters\n        self.calibration_data = {\n            'camera_matrix': None,\n            'distortion_coeffs': None,\n            'rvecs': None,\n            'tvecs': None\n        }\n\n        # Chessboard parameters for calibration\n        self.chessboard_size = (9, 6)  # Internal corners\n        self.square_size = 0.025  # 2.5cm squares\n\n        # Calibration images storage\n        self.calibration_images = []\n        self.max_calibration_images = 20\n\n        # Threading for calibration\n        self.calibration_lock = threading.Lock()\n\n        self.get_logger().info('Camera Calibrator initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process camera image for calibration\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Check if this image is suitable for calibration\n            if self.is_good_calibration_image(cv_image):\n                with self.calibration_lock:\n                    if len(self.calibration_images) < self.max_calibration_images:\n                        self.calibration_images.append(cv_image.copy())\n                        self.get_logger().info(f'Added calibration image {len(self.calibration_images)}/{self.max_calibration_images}')\n\n                        # Perform calibration when we have enough images\n                        if len(self.calibration_images) == self.max_calibration_images:\n                            self.perform_calibration()\n\n        except Exception as e:\n            self.get_logger().error(f'Image callback error: {str(e)}')\n\n    def is_good_calibration_image(self, image):\n        \"\"\"Check if image is suitable for calibration\"\"\"\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Find chessboard corners\n        ret, corners = cv2.findChessboardCorners(\n            gray, self.chessboard_size,\n            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\n        )\n\n        # Check if corners were found and image quality\n        if ret:\n            # Refine corner positions\n            corners_refined = cv2.cornerSubPix(\n                gray, corners, (11, 11), (-1, -1),\n                criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n            )\n            return True\n\n        return False\n\n    def perform_calibration(self):\n        \"\"\"Perform camera calibration\"\"\"\n        self.get_logger().info('Starting camera calibration...')\n\n        # Prepare object points\n        objp = np.zeros((self.chessboard_size[0] * self.chessboard_size[1], 3), np.float32)\n        objp[:, :2] = np.mgrid[0:self.chessboard_size[0], 0:self.chessboard_size[1]].T.reshape(-1, 2)\n        objp *= self.square_size  # Scale to real-world units\n\n        # Arrays to store object points and image points from all images\n        objpoints = []  # 3d points in real world space\n        imgpoints = []  # 2d points in image plane\n\n        # Process calibration images\n        for img in self.calibration_images:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n            # Find chessboard corners\n            ret, corners = cv2.findChessboardCorners(gray, self.chessboard_size, None)\n\n            if ret:\n                objpoints.append(objp)\n\n                # Refine corner positions\n                corners_refined = cv2.cornerSubPix(\n                    gray, corners, (11, 11), (-1, -1),\n                    criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n                )\n                imgpoints.append(corners_refined)\n\n        if len(objpoints) > 0 and len(imgpoints) > 0:\n            # Perform calibration\n            ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n                objpoints, imgpoints, gray.shape[::-1], None, None\n            )\n\n            if ret:\n                # Store calibration data\n                self.calibration_data = {\n                    'camera_matrix': camera_matrix,\n                    'distortion_coeffs': dist_coeffs,\n                    'rvecs': rvecs,\n                    'tvecs': tvecs\n                }\n\n                # Publish camera info\n                self.publish_camera_info(camera_matrix, dist_coeffs, img.shape)\n\n                self.get_logger().info(f'Calibration successful! Reprojection error: {ret:.4f}')\n                self.get_logger().info(f'Camera matrix:\\n{camera_matrix}')\n                self.get_logger().info(f'Distortion coefficients: {dist_coeffs.flatten()}')\n\n            else:\n                self.get_logger().error('Calibration failed')\n        else:\n            self.get_logger().error('No valid calibration patterns found')\n\n    def publish_camera_info(self, camera_matrix, dist_coeffs, img_shape):\n        \"\"\"Publish camera calibration info\"\"\"\n        info_msg = CameraInfo()\n        info_msg.header.stamp = self.get_clock().now().to_msg()\n        info_msg.header.frame_id = \"camera_link\"\n\n        info_msg.height = img_shape[0]\n        info_msg.width = img_shape[1]\n\n        # Camera matrix (3x3) as 9-element array\n        info_msg.k = camera_matrix.flatten().tolist()\n\n        # Distortion coefficients\n        info_msg.d = dist_coeffs.flatten().tolist()\n\n        # Rectification matrix (identity for monocular camera)\n        info_msg.r = [1, 0, 0, 0, 1, 0, 0, 0, 1]\n\n        # Projection matrix\n        info_msg.p = [\n            camera_matrix[0, 0], 0, camera_matrix[0, 2], 0,\n            0, camera_matrix[1, 1], camera_matrix[1, 2], 0,\n            0, 0, 1, 0\n        ]\n\n        self.info_pub.publish(info_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CameraCalibrator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down camera calibrator...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,r.jsx)(n.h3,{id:"yolo-based-object-detection-for-robotics",children:"YOLO-based Object Detection for Robotics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass YOLOObjectDetector(Node):\n    def __init__(self):\n        super().__init__('yolo_object_detector')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(String, '/object_detections', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/detection_markers', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # YOLO parameters\n        self.confidence_threshold = 0.5\n        self.nms_threshold = 0.4\n\n        # Load YOLO model (for this example, we'll use a mock detector)\n        # In practice, you would load a real YOLO model\n        try:\n            # self.net = cv2.dnn.readNetFromDarknet('yolo.cfg', 'yolo.weights')\n            # self.layer_names = self.net.getLayerNames()\n            # self.output_layers = [self.layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n            self.yolo_loaded = True\n            self.get_logger().info('YOLO model loaded')\n        except:\n            self.yolo_loaded = False\n            self.get_logger().warn('YOLO model not available, using mock detector')\n\n        # COCO class names\n        self.class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        # Threading\n        self.detection_lock = threading.Lock()\n\n        self.get_logger().info('YOLO Object Detector initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process image for object detection\"\"\"\n        with self.detection_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n                if self.yolo_loaded:\n                    detections = self.detect_objects_yolo(cv_image)\n                else:\n                    # Mock detection for simulation\n                    detections = self.mock_detection(cv_image)\n\n                # Draw detections on image (optional)\n                output_image = self.draw_detections(cv_image, detections)\n\n                # Publish detections\n                self.publish_detections(detections, msg.header)\n\n                # Publish visualization markers\n                self.publish_detection_markers(detections, msg.header)\n\n            except Exception as e:\n                self.get_logger().error(f'Image callback error: {str(e)}')\n\n    def mock_detection(self, image):\n        \"\"\"Mock object detection for simulation\"\"\"\n        height, width = image.shape[:2]\n\n        # Create some mock detections\n        mock_detections = [\n            {\n                'label': 'person',\n                'confidence': 0.85,\n                'bbox': [int(width * 0.4), int(height * 0.3), int(width * 0.6), int(height * 0.7)],\n                'center': [int(width * 0.5), int(height * 0.5)]\n            },\n            {\n                'label': 'chair',\n                'confidence': 0.78,\n                'bbox': [int(width * 0.2), int(height * 0.5), int(width * 0.35), int(height * 0.8)],\n                'center': [int(width * 0.275), int(height * 0.65)]\n            }\n        ]\n\n        return mock_detections\n\n    def detect_objects_yolo(self, image):\n        \"\"\"Perform object detection using YOLO\"\"\"\n        height, width, channels = image.shape\n\n        # Create blob from image\n        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n        self.net.setInput(blob)\n        outs = self.net.forward(self.output_layers)\n\n        # Process outputs\n        class_ids = []\n        confidences = []\n        boxes = []\n\n        for out in outs:\n            for detection in out:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > self.confidence_threshold:\n                    # Object detected\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    # Rectangle coordinates\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confidence_threshold, self.nms_threshold)\n\n        # Format detections\n        detections = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                x, y, w, h = boxes[i]\n                detections.append({\n                    'label': self.class_names[class_ids[i]] if class_ids[i] < len(self.class_names) else 'unknown',\n                    'confidence': confidences[i],\n                    'bbox': [x, y, x + w, y + h],\n                    'center': [x + w // 2, y + h // 2]\n                })\n\n        return detections\n\n    def draw_detections(self, image, detections):\n        \"\"\"Draw detection results on image\"\"\"\n        output_image = image.copy()\n\n        for detection in detections:\n            x1, y1, x2, y2 = map(int, detection['bbox'])\n            label = f\"{detection['label']}: {detection['confidence']:.2f}\"\n\n            # Draw bounding box\n            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw center point\n            center_x, center_y = detection['center']\n            cv2.circle(output_image, (center_x, center_y), 5, (0, 0, 255), -1)\n\n            # Draw label\n            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n            cv2.rectangle(output_image, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), (0, 255, 0), cv2.FILLED)\n            cv2.putText(output_image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n\n        return output_image\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish object detections\"\"\"\n        detection_data = []\n        for detection in detections:\n            detection_data.append({\n                'label': detection['label'],\n                'confidence': detection['confidence'],\n                'bbox': detection['bbox'],\n                'center': detection['center']\n            })\n\n        detection_msg = String()\n        detection_msg.data = str(detection_data)\n        self.detection_pub.publish(detection_msg)\n\n    def publish_detection_markers(self, detections, header):\n        \"\"\"Publish visualization markers for detections\"\"\"\n        marker_array = MarkerArray()\n\n        for i, detection in enumerate(detections):\n            # Create marker for bounding box\n            marker = Marker()\n            marker.header = header\n            marker.ns = \"object_detections\"\n            marker.id = i\n            marker.type = Marker.LINE_STRIP\n            marker.action = Marker.ADD\n\n            # Set position (will be set properly in a real implementation with depth)\n            marker.pose.position.z = 1.0\n            marker.pose.orientation.w = 1.0\n\n            # Set scale\n            marker.scale.x = 0.02  # Line width\n\n            # Set color based on confidence\n            confidence = detection['confidence']\n            marker.color.r = 1.0 - confidence  # Red decreases with confidence\n            marker.color.g = confidence  # Green increases with confidence\n            marker.color.b = 0.0\n            marker.color.a = 1.0\n\n            # Define points for the bounding box\n            x1, y1, x2, y2 = detection['bbox']\n            points = [\n                Point(x=x1, y=y1, z=0),\n                Point(x=x2, y=y1, z=0),\n                Point(x=x2, y=y2, z=0),\n                Point(x=x1, y=y2, z=0),\n                Point(x=x1, y=y1, z=0)  # Close the loop\n            ]\n\n            marker.points = points\n            marker_array.markers.append(marker)\n\n            # Create text marker for label\n            text_marker = Marker()\n            text_marker.header = header\n            text_marker.ns = \"detection_labels\"\n            text_marker.id = i + len(detections)  # Different ID to avoid conflict\n            text_marker.type = Marker.TEXT_VIEW_FACING\n            text_marker.action = Marker.ADD\n\n            text_marker.pose.position.x = x1\n            text_marker.pose.position.y = y1 - 10\n            text_marker.pose.position.z = 1.0\n            text_marker.pose.orientation.w = 1.0\n\n            text_marker.scale.z = 0.2  # Text size\n            text_marker.color.r = 1.0\n            text_marker.color.g = 1.0\n            text_marker.color.b = 1.0\n            text_marker.color.a = 1.0\n\n            text_marker.text = f\"{detection['label']} ({detection['confidence']:.2f})\"\n\n            marker_array.markers.append(text_marker)\n\n        self.marker_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = YOLOObjectDetector()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down YOLO object detector...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"visual-tracking-and-slam",children:"Visual Tracking and SLAM"}),"\n",(0,r.jsx)(n.h3,{id:"feature-based-visual-tracking",children:"Feature-Based Visual Tracking"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Float32\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass FeatureTracker(Node):\n    def __init__(self):\n        super().__init__('feature_tracker')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.tracked_point_pub = self.create_publisher(PointStamped, '/tracked_point', 10)\n        self.tracking_quality_pub = self.create_publisher(Float32, '/tracking_quality', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Tracking parameters\n        self.feature_params = {\n            'maxCorners': 100,\n            'qualityLevel': 0.3,\n            'minDistance': 7,\n            'blockSize': 7\n        }\n\n        self.lk_params = {\n            'winSize': (15, 15),\n            'maxLevel': 2,\n            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        }\n\n        # Tracking state\n        self.prev_image = None\n        self.prev_points = None\n        self.track_quality = 0.0\n        self.track_id = 0\n\n        # Threading lock\n        self.tracking_lock = threading.Lock()\n\n        self.get_logger().info('Feature Tracker initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process image for feature tracking\"\"\"\n        with self.tracking_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, \"mono8\")\n\n                if self.prev_image is not None and self.prev_points is not None:\n                    # Calculate optical flow\n                    new_points, status, error = cv2.calcOpticalFlowPyrLK(\n                        self.prev_image, cv_image, self.prev_points, None, **self.lk_params\n                    )\n\n                    # Select good points\n                    good_new = new_points[status == 1]\n                    good_old = self.prev_points[status == 1]\n\n                    if len(good_new) > 0:\n                        # Calculate tracking quality\n                        self.track_quality = len(good_new) / len(self.prev_points)\n\n                        # Update tracked point (use the first tracked point as example)\n                        tracked_point = good_new[0]\n\n                        point_msg = PointStamped()\n                        point_msg.header = msg.header\n                        point_msg.point.x = float(tracked_point[0])\n                        point_msg.point.y = float(tracked_point[1])\n                        point_msg.point.z = 0.0  # Depth would come from stereo or depth sensor\n\n                        self.tracked_point_pub.publish(point_msg)\n\n                        # Publish tracking quality\n                        quality_msg = Float32()\n                        quality_msg.data = self.track_quality\n                        self.tracking_quality_pub.publish(quality_msg)\n\n                        # Update points for next iteration\n                        self.prev_points = good_new.reshape(-1, 1, 2)\n\n                        self.get_logger().info(f'Tracked {len(good_new)} points, quality: {self.track_quality:.2f}')\n                    else:\n                        # Lost track, need to reinitialize\n                        self.prev_points = None\n                        self.track_quality = 0.0\n                        self.get_logger().info('Lost track, reinitializing...')\n\n                else:\n                    # Initialize tracking points\n                    self.prev_points = cv2.goodFeaturesToTrack(\n                        cv_image, mask=None, **self.feature_params\n                    )\n                    if self.prev_points is not None:\n                        self.get_logger().info(f'Initialized {len(self.prev_points)} tracking points')\n\n                # Store current image for next iteration\n                self.prev_image = cv_image\n\n            except Exception as e:\n                self.get_logger().error(f'Tracking error: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = FeatureTracker()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down feature tracker...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"3d-reconstruction-and-depth-estimation",children:"3D Reconstruction and Depth Estimation"}),"\n",(0,r.jsx)(n.h3,{id:"stereo-vision-and-3d-reconstruction",children:"Stereo Vision and 3D Reconstruction"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass StereoReconstruction(Node):\n    def __init__(self):\n        super().__init__(\'stereo_reconstruction\')\n\n        # Subscribe to stereo images\n        self.left_sub = self.create_subscription(\n            Image,\n            \'/stereo/left/image_rect\',\n            self.left_image_callback,\n            10\n        )\n\n        self.right_sub = self.create_subscription(\n            Image,\n            \'/stereo/right/image_rect\',\n            self.right_image_callback,\n            10\n        )\n\n        # Publishers\n        self.point_pub = self.create_publisher(PointStamped, \'/reconstructed_point\', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Stereo parameters\n        self.stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Camera parameters (these should be calibrated)\n        self.camera_matrix_left = np.array([\n            [615.0, 0.0, 320.0],\n            [0.0, 615.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        self.camera_matrix_right = np.array([\n            [615.0, 0.0, 320.0],\n            [0.0, 615.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n\n        # Baseline (distance between cameras) - example value\n        self.baseline = 0.12  # meters\n\n        # Image storage\n        self.left_image = None\n        self.right_image = None\n        self.images_lock = threading.Lock()\n\n        self.get_logger().info(\'Stereo Reconstruction initialized\')\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        with self.images_lock:\n            self.left_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        with self.images_lock:\n            self.right_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n    def reconstruct_3d_point(self, u, v, disparity):\n        """Reconstruct 3D point from disparity"""\n        if disparity == 0:\n            return None\n\n        # Calculate 3D coordinates\n        x = (u - self.camera_matrix_left[0, 2]) * self.baseline / disparity\n        y = (v - self.camera_matrix_left[1, 2]) * self.baseline / disparity\n        z = self.camera_matrix_left[0, 0] * self.baseline / disparity\n\n        return np.array([x, y, z])\n\n    def process_stereo_pair(self):\n        """Process stereo image pair for 3D reconstruction"""\n        with self.images_lock:\n            if self.left_image is None or self.right_image is None:\n                return\n\n            # Compute disparity map\n            disparity = self.stereo.compute(self.left_image, self.right_image).astype(np.float32) / 16.0\n\n            # Find a point of interest (for example, center of image)\n            height, width = disparity.shape\n            center_u, center_v = width // 2, height // 2\n\n            # Get disparity at center point\n            center_disparity = disparity[center_v, center_u]\n\n            if center_disparity > 0:\n                # Reconstruct 3D point\n                point_3d = self.reconstruct_3d_point(center_u, center_v, center_disparity)\n\n                if point_3d is not None:\n                    # Publish reconstructed point\n                    point_msg = PointStamped()\n                    point_msg.header.stamp = self.get_clock().now().to_msg()\n                    point_msg.header.frame_id = "camera_link"\n                    point_msg.point.x = float(point_3d[0])\n                    point_msg.point.y = float(point_3d[1])\n                    point_msg.point.z = float(point_3d[2])\n\n                    self.point_pub.publish(point_msg)\n\n                    self.get_logger().info(f\'Reconstructed 3D point: ({point_3d[0]:.2f}, {point_3d[1]:.2f}, {point_3d[2]:.2f})\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = StereoReconstruction()\n\n    # Timer to process stereo pairs\n    timer = node.create_timer(0.1, node.process_stereo_pair)\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down stereo reconstruction...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"vision-based-manipulation",children:"Vision-Based Manipulation"}),"\n",(0,r.jsx)(n.h3,{id:"object-manipulation-with-vision-feedback",children:"Object Manipulation with Vision Feedback"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass VisionBasedManipulator(Node):\n    def __init__(self):\n        super().__init__(\'vision_based_manipulator\')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.target_pose_pub = self.create_publisher(PoseStamped, \'/manipulation_target\', 10)\n        self.manipulation_status_pub = self.create_publisher(String, \'/manipulation_status\', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Manipulation state\n        self.current_task = None\n        self.target_object = None\n        self.object_position = None\n\n        # Object detection parameters\n        self.object_templates = {}  # Would store template images for matching\n        self.detection_threshold = 0.7\n\n        # Threading\n        self.manipulation_lock = threading.Lock()\n\n        self.get_logger().info(\'Vision-Based Manipulator initialized\')\n\n    def image_callback(self, msg):\n        """Process image for manipulation tasks"""\n        with self.manipulation_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n                # If we have a manipulation task, look for the target object\n                if self.current_task and self.target_object:\n                    object_found, position = self.find_object(cv_image, self.target_object)\n\n                    if object_found and position:\n                        self.object_position = position\n                        self.publish_target_pose(position, msg.header)\n                        self.get_logger().info(f\'Found {self.target_object} at {position}\')\n\n                        # Check if object is in manipulable position\n                        if self.is_object_manipulable(position):\n                            self.execute_manipulation(position)\n                    else:\n                        self.get_logger().info(f\'Could not find {self.target_object}\')\n\n            except Exception as e:\n                self.get_logger().error(f\'Manipulation error: {str(e)}\')\n\n    def find_object(self, image, object_name):\n        """Find specific object in image"""\n        # For this example, we\'ll use color-based detection\n        # In practice, you might use template matching, feature detection, or deep learning\n\n        if object_name == \'red_cup\':\n            return self.find_red_object(image)\n        elif object_name == \'blue_box\':\n            return self.find_blue_object(image)\n        else:\n            # Generic object detection could go here\n            return self.generic_object_detection(image, object_name)\n\n    def find_red_object(self, image):\n        """Find red-colored objects in image"""\n        # Convert to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n\n        # Create masks for red color\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Find the largest contour (assuming it\'s the target object)\n            largest_contour = max(contours, key=cv2.contourArea)\n\n            if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                # Get the center of the contour\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n                    return True, (cx, cy)\n\n        return False, None\n\n    def find_blue_object(self, image):\n        """Find blue-colored objects in image"""\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for blue color\n        lower_blue = np.array([100, 50, 50])\n        upper_blue = np.array([130, 255, 255])\n\n        # Create mask for blue color\n        mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Find the largest contour\n            largest_contour = max(contours, key=cv2.contourArea)\n\n            if cv2.contourArea(largest_contour) > 100:\n                # Get the center of the contour\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n                    return True, (cx, cy)\n\n        return False, None\n\n    def generic_object_detection(self, image, object_name):\n        """Generic object detection method"""\n        # This would typically use a pre-trained model\n        # For simulation, return a fixed position\n        height, width = image.shape[:2]\n        return True, (width // 2, height // 2)  # Center of image\n\n    def is_object_manipulable(self, position):\n        """Check if object is in manipulable position"""\n        # In a real system, this would check if the object is reachable by the robot arm\n        # For simulation, assume object at center is manipulable\n        if position:\n            height, width = 480, 640  # Assuming standard image size\n            center_x, center_y = width // 2, height // 2\n\n            # Check if object is roughly in the center (within 10% of image)\n            x, y = position\n            tolerance = 0.1  # 10% tolerance\n            return (abs(x - center_x) < width * tolerance and\n                    abs(y - center_y) < height * tolerance)\n\n        return False\n\n    def publish_target_pose(self, position, header):\n        """Publish target pose for manipulation"""\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = "camera_link"\n\n        # Convert image coordinates to 3D pose\n        # This is simplified - in reality, you\'d need depth information\n        pose_msg.pose.position.x = float(position[0])\n        pose_msg.pose.position.y = float(position[1])\n        pose_msg.pose.position.z = 0.0  # Depth would come from other sensors\n\n        # Set orientation (for manipulation, you might want to approach from above)\n        pose_msg.pose.orientation.w = 1.0\n\n        self.target_pose_pub.publish(pose_msg)\n\n    def execute_manipulation(self, position):\n        """Execute manipulation task"""\n        self.get_logger().info(f\'Executing manipulation for object at {position}\')\n\n        # Publish manipulation status\n        status_msg = String()\n        status_msg.data = f"manipulating_object_at_{position[0]}_{position[1]}"\n        self.manipulation_status_pub.publish(status_msg)\n\n        # In a real system, this would send commands to the robot arm\n        # For simulation, just log the action\n        self.get_logger().info(\'Manipulation command sent to robot arm\')\n\n    def start_manipulation_task(self, object_name):\n        """Start a manipulation task"""\n        with self.manipulation_lock:\n            self.current_task = \'manipulation\'\n            self.target_object = object_name\n            self.object_position = None\n            self.get_logger().info(f\'Started manipulation task for {object_name}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionBasedManipulator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down vision-based manipulator...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"real-time-vision-optimization",children:"Real-time Vision Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"optimized-vision-processing-pipeline",children:"Optimized Vision Processing Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Float32\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nimport time\nfrom collections import deque\n\nclass OptimizedVisionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'optimized_vision_pipeline\')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Publishers for performance metrics\n        self.fps_pub = self.create_publisher(Float32, \'/vision_fps\', 10)\n        self.processing_time_pub = self.create_publisher(Float32, \'/processing_time\', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Processing parameters\n        self.target_fps = 30.0\n        self.processing_rate = 1  # Process every Nth frame\n        self.frame_counter = 0\n\n        # Performance tracking\n        self.frame_times = deque(maxlen=30)  # Track last 30 frames\n        self.last_frame_time = time.time()\n\n        # Threading\n        self.processing_queue = deque(maxlen=2)  # Limit queue size\n        self.processing_thread = threading.Thread(target=self.processing_worker, daemon=True)\n        self.processing_thread.start()\n\n        # Processing flags\n        self.enable_detection = True\n        self.enable_tracking = True\n        self.enable_optimization = True\n\n        # Optimization parameters\n        self.image_scale_factor = 0.5  # Scale down for faster processing\n        self.roi_enabled = False\n        self.roi = (0, 0, 1, 1)  # (x, y, width, height) as fraction of image\n\n        # Performance timer\n        self.perf_timer = self.create_timer(1.0, self.publish_performance_metrics)\n\n        self.get_logger().info(\'Optimized Vision Pipeline initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming image with optimization"""\n        # Throttle processing rate\n        self.frame_counter += 1\n        if self.frame_counter % self.processing_rate != 0:\n            return\n\n        # Add to processing queue (non-blocking)\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Apply optimizations\n            if self.enable_optimization:\n                cv_image = self.apply_optimizations(cv_image)\n\n            self.processing_queue.append((cv_image, msg.header))\n        except Exception as e:\n            self.get_logger().error(f\'Image callback error: {str(e)}\')\n\n    def apply_optimizations(self, image):\n        """Apply various optimizations to image"""\n        # Scale image down for faster processing\n        if self.image_scale_factor < 1.0:\n            new_size = (int(image.shape[1] * self.image_scale_factor),\n                       int(image.shape[0] * self.image_scale_factor))\n            image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n\n        # Apply ROI if enabled\n        if self.roi_enabled:\n            h, w = image.shape[:2]\n            x1 = int(self.roi[0] * w)\n            y1 = int(self.roi[1] * h)\n            x2 = int((self.roi[0] + self.roi[2]) * w)\n            y2 = int((self.roi[1] + self.roi[3]) * h)\n            image = image[y1:y2, x1:x2]\n\n        return image\n\n    def processing_worker(self):\n        """Background processing worker"""\n        while rclpy.ok():\n            try:\n                if self.processing_queue:\n                    image, header = self.processing_queue.popleft()\n\n                    # Record start time for performance tracking\n                    start_time = time.time()\n\n                    # Perform vision processing\n                    if self.enable_detection:\n                        processed_image = self.perform_detection(image)\n                    else:\n                        processed_image = image\n\n                    # Record processing time\n                    processing_time = time.time() - start_time\n                    self.frame_times.append(processing_time)\n\n                    # Adjust processing rate based on performance\n                    self.adjust_processing_rate(processing_time)\n\n            except IndexError:\n                # Queue is empty, sleep briefly\n                time.sleep(0.001)\n            except Exception as e:\n                self.get_logger().error(f\'Processing worker error: {str(e)}\')\n\n    def perform_detection(self, image):\n        """Perform vision detection tasks"""\n        # This is where you\'d implement your specific vision algorithms\n        # For example: object detection, feature extraction, etc.\n\n        # Simple example: edge detection\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Convert back to color for consistency\n        processed = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n\n        return processed\n\n    def adjust_processing_rate(self, processing_time):\n        """Adjust processing rate based on performance"""\n        if len(self.frame_times) < 10:\n            return\n\n        avg_processing_time = sum(self.frame_times) / len(self.frame_times)\n        current_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n\n        # Adjust processing rate to maintain target FPS\n        if current_fps > self.target_fps * 1.1:  # Too fast, can process more\n            self.processing_rate = max(1, self.processing_rate - 1)\n        elif current_fps < self.target_fps * 0.9:  # Too slow, process less\n            self.processing_rate = min(10, self.processing_rate + 1)\n\n    def publish_performance_metrics(self):\n        """Publish performance metrics"""\n        if self.frame_times:\n            avg_processing_time = sum(self.frame_times) / len(self.frame_times)\n            current_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0\n\n            # Publish FPS\n            fps_msg = Float32()\n            fps_msg.data = current_fps\n            self.fps_pub.publish(fps_msg)\n\n            # Publish processing time\n            time_msg = Float32()\n            time_msg.data = avg_processing_time\n            self.processing_time_pub.publish(time_msg)\n\n            self.get_logger().info(f\'Vision pipeline - FPS: {current_fps:.2f}, Processing time: {avg_processing_time:.4f}s, Rate: {self.processing_rate}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OptimizedVisionPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down optimized vision pipeline...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-lab-complete-vision-system",children:"Hands-on Lab: Complete Vision System"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you'll integrate all vision components into a complete system."}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-the-vision-system-launch-file",children:"Step 1: Create the Vision System Launch File"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"vision_system_launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Vision system nodes\n    calibrator = Node(\n        package='ai_robo_learning',\n        executable='camera_calibrator',\n        name='camera_calibrator',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    detector = Node(\n        package='ai_robo_learning',\n        executable='yolo_object_detector',\n        name='yolo_object_detector',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    tracker = Node(\n        package='ai_robo_learning',\n        executable='feature_tracker',\n        name='feature_tracker',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    stereo = Node(\n        package='ai_robo_learning',\n        executable='stereo_reconstruction',\n        name='stereo_reconstruction',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    manipulator = Node(\n        package='ai_robo_learning',\n        executable='vision_based_manipulator',\n        name='vision_based_manipulator',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    optimizer = Node(\n        package='ai_robo_learning',\n        executable='optimized_vision_pipeline',\n        name='optimized_vision_pipeline',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add all nodes\n    ld.add_action(calibrator)\n    ld.add_action(detector)\n    ld.add_action(tracker)\n    ld.add_action(stereo)\n    ld.add_action(manipulator)\n    ld.add_action(optimizer)\n\n    return ld\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-create-the-complete-vision-node",children:"Step 2: Create the Complete Vision Node"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"complete_vision_system.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped, PoseStamped\nfrom std_msgs.msg import String, Float32\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nimport time\nfrom collections import deque\n\nclass CompleteVisionSystem(Node):\n    def __init__(self):\n        super().__init__('complete_vision_system')\n\n        # Subscribe to camera image\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(String, '/vision_detections', 10)\n        self.tracking_pub = self.create_publisher(PointStamped, '/tracked_object', 10)\n        self.status_pub = self.create_publisher(String, '/vision_status', 10)\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Vision system components\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.current_image = None\n\n        # Feature tracking\n        self.prev_image = None\n        self.prev_points = None\n        self.feature_params = {\n            'maxCorners': 100,\n            'qualityLevel': 0.3,\n            'minDistance': 7,\n            'blockSize': 7\n        }\n\n        self.lk_params = {\n            'winSize': (15, 15),\n            'maxLevel': 2,\n            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        }\n\n        # Object detection (simplified)\n        self.object_detection_enabled = True\n        self.tracking_enabled = True\n\n        # Performance tracking\n        self.frame_times = deque(maxlen=30)\n        self.last_frame_time = time.time()\n\n        # Threading\n        self.vision_lock = threading.Lock()\n\n        # System status\n        self.system_active = True\n        self.vision_quality = 0.0\n\n        self.get_logger().info('Complete Vision System initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process camera image with all vision components\"\"\"\n        with self.vision_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n                # Record frame time for performance tracking\n                current_time = time.time()\n                if self.last_frame_time > 0:\n                    frame_time = current_time - self.last_frame_time\n                    self.frame_times.append(frame_time)\n                self.last_frame_time = current_time\n\n                # Update current image\n                self.current_image = cv_image\n\n                # Perform object detection\n                if self.object_detection_enabled:\n                    detections = self.simple_object_detection(cv_image)\n                    if detections:\n                        detection_msg = String()\n                        detection_msg.data = str(detections)\n                        self.detection_pub.publish(detection_msg)\n\n                # Perform feature tracking\n                if self.tracking_enabled:\n                    self.perform_tracking(cv_image, msg.header)\n\n                # Update vision quality metric\n                self.update_vision_quality()\n\n                # Publish system status\n                status_msg = String()\n                status_msg.data = f\"active:{self.system_active},quality:{self.vision_quality:.2f}\"\n                self.status_pub.publish(status_msg)\n\n            except Exception as e:\n                self.get_logger().error(f'Vision system error: {str(e)}')\n\n    def simple_object_detection(self, image):\n        \"\"\"Simple object detection for demonstration\"\"\"\n        # Convert to HSV for color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define ranges for common colors\n        color_ranges = {\n            'red': (np.array([0, 50, 50]), np.array([10, 255, 255])),\n            'blue': (np.array([100, 50, 50]), np.array([130, 255, 255])),\n            'green': (np.array([40, 50, 50]), np.array([80, 255, 255]))\n        }\n\n        detections = []\n        for color_name, (lower, upper) in color_ranges.items():\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                if cv2.contourArea(contour) > 500:  # Minimum area threshold\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x = x + w // 2\n                    center_y = y + h // 2\n\n                    detections.append({\n                        'color': color_name,\n                        'bbox': [x, y, x + w, y + h],\n                        'center': [center_x, center_y],\n                        'area': cv2.contourArea(contour)\n                    })\n\n        return detections\n\n    def perform_tracking(self, image, header):\n        \"\"\"Perform feature tracking\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        if self.prev_image is not None and self.prev_points is not None:\n            # Calculate optical flow\n            new_points, status, error = cv2.calcOpticalFlowPyrLK(\n                self.prev_image, gray, self.prev_points, None, **self.lk_params\n            )\n\n            # Select good points\n            good_new = new_points[status == 1]\n            good_old = self.prev_points[status == 1]\n\n            if len(good_new) > 0:\n                # Publish the first tracked point as example\n                tracked_point = good_new[0]\n\n                point_msg = PointStamped()\n                point_msg.header = header\n                point_msg.point.x = float(tracked_point[0])\n                point_msg.point.y = float(tracked_point[1])\n                point_msg.point.z = 0.0  # Depth would come from other sensors\n\n                self.tracking_pub.publish(point_msg)\n\n                # Update points for next iteration\n                self.prev_points = good_new.reshape(-1, 1, 2)\n            else:\n                # Lost track, reinitialize\n                self.prev_points = None\n        else:\n            # Initialize tracking points\n            self.prev_points = cv2.goodFeaturesToTrack(\n                gray, mask=None, **self.feature_params\n            )\n\n        # Store current image for next iteration\n        self.prev_image = gray\n\n    def update_vision_quality(self):\n        \"\"\"Update vision system quality metric\"\"\"\n        if len(self.frame_times) > 0:\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n            current_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n\n            # Quality metric based on FPS (target 30 FPS)\n            self.vision_quality = min(1.0, current_fps / 30.0)\n        else:\n            self.vision_quality = 0.0\n\n    def get_vision_status(self):\n        \"\"\"Get current vision system status\"\"\"\n        return {\n            'active': self.system_active,\n            'quality': self.vision_quality,\n            'fps': 1.0 / self.frame_times[-1] if self.frame_times else 0,\n            'detection_enabled': self.object_detection_enabled,\n            'tracking_enabled': self.tracking_enabled\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CompleteVisionSystem()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down complete vision system...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-test-the-vision-system",children:"Step 3: Test the Vision System"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Make sure you have the required dependencies:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install opencv-python numpy\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Run the complete vision system:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 complete_vision_system.py\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Test with a camera feed or simulated images:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# If using a real camera\nros2 run v4l2_camera v4l2_camera_node\n\n# Or use a sample image publisher\nros2 run image_publisher image_publisher_node --ros-args -p file_name:=/path/to/test/image.jpg\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibration"}),": Always calibrate cameras before using vision algorithms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": Optimize algorithms for real-time performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Handle various lighting conditions and environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validation"}),": Continuously validate vision results against ground truth"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Modularity"}),": Design vision components to be modular and reusable"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling for vision failures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Management"}),": Efficiently manage computational resources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Testing"}),": Thoroughly test vision systems in various scenarios"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you'll be ready to learn about object manipulation in Chapter 5, where you'll explore how robots can interact with and manipulate objects in their environment."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}}}]);