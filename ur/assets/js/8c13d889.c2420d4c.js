"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[353],{7948:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/chapter-2/voice-command-processing","title":"Voice Command Processing","description":"This chapter covers voice command processing for humanoid robots, including speech recognition, natural language understanding, and voice-to-action conversion using OpenAI Whisper and other speech processing technologies.","source":"@site/docs/module-4/chapter-2/voice-command-processing.md","sourceDirName":"module-4/chapter-2","slug":"/module-4/chapter-2/voice-command-processing","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-2/voice-command-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-4/chapter-2/voice-command-processing.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice Command Processing"},"sidebar":"tutorialSidebar","previous":{"title":"AI Integration in Robotics","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-1/ai-integration"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-3/cognitive-planning-llms"}}');var r=t(4848),i=t(8453);const s={sidebar_position:2,title:"Voice Command Processing"},a="Voice Command Processing",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Speech Recognition Systems for Robotics",id:"speech-recognition-systems-for-robotics",level:2},{value:"Overview of Speech Recognition in Robotics",id:"overview-of-speech-recognition-in-robotics",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Basic Voice Command Processing Node",id:"basic-voice-command-processing-node",level:3},{value:"Integration with OpenAI Whisper",id:"integration-with-openai-whisper",level:2},{value:"Whisper-based Voice Recognition",id:"whisper-based-voice-recognition",level:3},{value:"Voice Activity Detection",id:"voice-activity-detection",level:2},{value:"Advanced Voice Activity Detection Node",id:"advanced-voice-activity-detection-node",level:3},{value:"Natural Language Processing for Commands",id:"natural-language-processing-for-commands",level:2},{value:"Command Understanding with NLP",id:"command-understanding-with-nlp",level:3},{value:"Multi-Language Voice Commands",id:"multi-language-voice-commands",level:2},{value:"Multi-Language Voice Processing",id:"multi-language-voice-processing",level:3},{value:"Hands-on Lab: Complete Voice Command System",id:"hands-on-lab-complete-voice-command-system",level:2},{value:"Step 1: Create the Voice Command System Launch File",id:"step-1-create-the-voice-command-system-launch-file",level:3},{value:"Step 2: Create the Complete Voice Command Node",id:"step-2-create-the-complete-voice-command-node",level:3},{value:"Step 3: Test the Voice Command System",id:"step-3-test-the-voice-command-system",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"voice-command-processing",children:"Voice Command Processing"})}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers voice command processing for humanoid robots, including speech recognition, natural language understanding, and voice-to-action conversion using OpenAI Whisper and other speech processing technologies."}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you'll explore:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Speech recognition systems for robotics"}),"\n",(0,r.jsx)(n.li,{children:"Voice command interpretation"}),"\n",(0,r.jsx)(n.li,{children:"Integration with OpenAI Whisper"}),"\n",(0,r.jsx)(n.li,{children:"Natural language processing for commands"}),"\n",(0,r.jsx)(n.li,{children:"Voice activity detection"}),"\n",(0,r.jsx)(n.li,{children:"Multi-language voice command support"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completion of Module 1-3"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ROS 2 messaging"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of audio processing"}),"\n",(0,r.jsx)(n.li,{children:"Experience with Python and AI libraries"}),"\n",(0,r.jsx)(n.li,{children:"Audio input hardware (microphone)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"speech-recognition-systems-for-robotics",children:"Speech Recognition Systems for Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"overview-of-speech-recognition-in-robotics",children:"Overview of Speech Recognition in Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Speech recognition in robotics involves converting spoken language into text that can be processed by the robot's AI systems. For humanoid robots, this enables natural human-robot interaction through voice commands."}),"\n",(0,r.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Input"}),": Microphones or microphone arrays"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Activity Detection"}),": Distinguishing speech from background noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech-to-Text"}),": Converting audio to text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning of commands"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Mapping"}),": Converting commands to robot actions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"basic-voice-command-processing-node",children:"Basic Voice Command Processing Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import AudioData\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\n\nclass VoiceCommandProcessor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_processor\')\n\n        # Publisher for recognized text\n        self.text_pub = self.create_publisher(String, \'/recognized_text\', 10)\n\n        # Publisher for robot commands\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 4000  # Adjust based on environment\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio source (microphone)\n        try:\n            self.microphone = sr.Microphone()\n        except OSError:\n            self.get_logger().warn(\'Microphone not found, using simulation mode\')\n            self.microphone = None\n\n        # Command queue for processing\n        self.command_queue = queue.Queue()\n\n        # Start voice recognition thread\n        self.recognition_thread = threading.Thread(target=self.voice_recognition_loop, daemon=True)\n        self.recognition_thread.start()\n\n        # Timer for periodic processing\n        self.timer = self.create_timer(0.1, self.process_commands)\n\n        self.get_logger().info(\'Voice Command Processor initialized\')\n\n    def voice_recognition_loop(self):\n        """Main loop for voice recognition"""\n        if self.microphone is None:\n            self.get_logger().warn(\'Cannot start voice recognition - no microphone available\')\n            return\n\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)  # Adjust for background noise\n\n        self.get_logger().info(\'Voice recognition started, listening for commands...\')\n\n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Process audio in separate thread to avoid blocking\n                threading.Thread(\n                    target=self.recognize_audio,\n                    args=(audio,),\n                    daemon=True\n                ).start()\n\n            except sr.WaitTimeoutError:\n                # No speech detected, continue listening\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Voice recognition error: {str(e)}\')\n                time.sleep(0.1)  # Brief pause before retrying\n\n    def recognize_audio(self, audio):\n        """Recognize audio and add to command queue"""\n        try:\n            # Use Google Web Speech API (requires internet)\n            text = self.recognizer.recognize_google(audio)\n\n            # Add recognized text to queue\n            self.command_queue.put(text)\n\n            self.get_logger().info(f\'Recognized: "{text}"\')\n\n            # Publish recognized text\n            text_msg = String()\n            text_msg.data = text\n            self.text_pub.publish(text_msg)\n\n        except sr.UnknownValueError:\n            self.get_logger().info(\'Could not understand audio\')\n        except sr.RequestError as e:\n            self.get_logger().error(f\'Could not request results from speech recognition service; {e}\')\n        except Exception as e:\n            self.get_logger().error(f\'Error in audio recognition: {str(e)}\')\n\n    def process_commands(self):\n        """Process recognized commands"""\n        # Process all available commands in queue\n        while not self.command_queue.empty():\n            try:\n                command_text = self.command_queue.get_nowait()\n                self.process_command(command_text)\n            except queue.Empty:\n                break\n\n    def process_command(self, command_text):\n        """Process a single voice command"""\n        # Convert to lowercase for easier processing\n        command_lower = command_text.lower().strip()\n\n        # Define command mappings\n        command_mappings = {\n            \'move forward\': self.move_forward,\n            \'go forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'go backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'stop\': self.stop_robot,\n            \'halt\': self.stop_robot,\n            \'spin left\': self.spin_left,\n            \'spin right\': self.spin_right\n        }\n\n        # Check for command matches\n        for command_phrase, command_func in command_mappings.items():\n            if command_phrase in command_lower:\n                self.get_logger().info(f\'Executing command: {command_phrase}\')\n                command_func()\n                return\n\n        # If no exact match, try fuzzy matching or NLP processing\n        self.process_fuzzy_command(command_lower)\n\n    def move_forward(self):\n        """Move robot forward"""\n        cmd = Twist()\n        cmd.linear.x = 0.3  # Forward speed\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def move_backward(self):\n        """Move robot backward"""\n        cmd = Twist()\n        cmd.linear.x = -0.3  # Backward speed\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def turn_left(self):\n        """Turn robot left"""\n        cmd = Twist()\n        cmd.linear.x = 0.1  # Small forward movement while turning\n        cmd.angular.z = 0.5  # Left turn\n        self.cmd_pub.publish(cmd)\n\n    def turn_right(self):\n        """Turn robot right"""\n        cmd = Twist()\n        cmd.linear.x = 0.1  # Small forward movement while turning\n        cmd.angular.z = -0.5  # Right turn\n        self.cmd_pub.publish(cmd)\n\n    def spin_left(self):\n        """Spin robot left in place"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.8  # Fast left spin\n        self.cmd_pub.publish(cmd)\n\n    def spin_right(self):\n        """Spin robot right in place"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = -0.8  # Fast right spin\n        self.cmd_pub.publish(cmd)\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def process_fuzzy_command(self, command_lower):\n        """Process commands using fuzzy matching or NLP"""\n        # Simple keyword-based fuzzy matching\n        if \'forward\' in command_lower or \'ahead\' in command_lower:\n            self.move_forward()\n        elif \'backward\' in command_lower or \'back\' in command_lower:\n            self.move_backward()\n        elif \'left\' in command_lower:\n            self.turn_left()\n        elif \'right\' in command_lower:\n            self.turn_right()\n        elif \'stop\' in command_lower or \'halt\' in command_lower:\n            self.stop_robot()\n        else:\n            self.get_logger().info(f\'Unknown command: "{command_lower}", ignoring\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down voice command processor...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-openai-whisper",children:"Integration with OpenAI Whisper"}),"\n",(0,r.jsx)(n.h3,{id:"whisper-based-voice-recognition",children:"Whisper-based Voice Recognition"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport time\nimport io\n\nclass WhisperVoiceProcessor(Node):\n    def __init__(self):\n        super().__init__(\'whisper_voice_processor\')\n\n        # Publisher for recognized text\n        self.text_pub = self.create_publisher(String, \'/whisper_recognized_text\', 10)\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Buffer size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.record_seconds = 5  # Maximum recording length\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # Audio stream\n        try:\n            self.stream = self.audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n            self.microphone_available = True\n        except Exception as e:\n            self.get_logger().warn(f\'Could not initialize audio stream: {e}\')\n            self.microphone_available = False\n\n        # Command queue\n        self.command_queue = queue.Queue()\n\n        # Start audio recording thread\n        self.recording_thread = threading.Thread(target=self.audio_recording_loop, daemon=True)\n        self.recording_thread.start()\n\n        # Timer for processing\n        self.timer = self.create_timer(0.1, self.process_commands)\n\n        # Initialize Whisper model (this will be loaded later)\n        self.whisper_model = None\n        self.load_whisper_model()\n\n        self.get_logger().info(\'Whisper Voice Processor initialized\')\n\n    def load_whisper_model(self):\n        """Load Whisper model for speech recognition"""\n        try:\n            import whisper\n            # Load model (you can choose different sizes: tiny, base, small, medium, large)\n            self.whisper_model = whisper.load_model("base")\n            self.get_logger().info(\'Whisper model loaded successfully\')\n        except ImportError:\n            self.get_logger().error(\'Whisper library not found. Install with: pip install openai-whisper\')\n            self.whisper_model = None\n        except Exception as e:\n            self.get_logger().error(f\'Error loading Whisper model: {str(e)}\')\n            self.whisper_model = None\n\n    def audio_recording_loop(self):\n        """Main loop for audio recording"""\n        if not self.microphone_available:\n            self.get_logger().warn(\'Audio recording not available - no microphone\')\n            return\n\n        self.get_logger().info(\'Audio recording started, listening for commands...\')\n\n        while rclpy.ok():\n            try:\n                # Record audio\n                frames = []\n                silent_chunks = 0\n                max_silent_chunks = 20  # About 0.5 seconds of silence to stop\n\n                # Listen for voice activity\n                while rclpy.ok():\n                    data = self.stream.read(self.chunk, exception_on_overflow=False)\n                    frames.append(data)\n\n                    # Simple voice activity detection based on amplitude\n                    audio_data = np.frombuffer(data, dtype=np.int16)\n                    amplitude = np.sqrt(np.mean(audio_data**2))\n\n                    if amplitude < 500:  # Threshold for silence\n                        silent_chunks += 1\n                        if silent_chunks > max_silent_chunks:\n                            break  # Stop recording after silence\n                    else:\n                        silent_chunks = 0  # Reset on voice activity\n\n                    # Limit recording length\n                    if len(frames) > self.rate * self.record_seconds / self.chunk:\n                        break\n\n                # If we recorded something\n                if len(frames) > max_silent_chunks:\n                    # Convert to WAV format for Whisper\n                    wav_buffer = self.create_wav_buffer(frames)\n\n                    # Process with Whisper in separate thread\n                    threading.Thread(\n                        target=self.transcribe_audio,\n                        args=(wav_buffer,),\n                        daemon=True\n                    ).start()\n\n            except Exception as e:\n                self.get_logger().error(f\'Audio recording error: {str(e)}\')\n                time.sleep(0.1)\n\n    def create_wav_buffer(self, frames):\n        """Create WAV buffer from audio frames"""\n        wav_buffer = io.BytesIO()\n\n        # Create WAV file\n        with wave.open(wav_buffer, \'wb\') as wav_file:\n            wav_file.setnchannels(self.channels)\n            wav_file.setsampwidth(self.audio.get_sample_size(self.format))\n            wav_file.setframerate(self.rate)\n            wav_file.writeframes(b\'\'.join(frames))\n\n        wav_buffer.seek(0)\n        return wav_buffer\n\n    def transcribe_audio(self, wav_buffer):\n        """Transcribe audio using Whisper"""\n        if self.whisper_model is None:\n            return\n\n        try:\n            # Save buffer to temporary file for Whisper\n            import tempfile\n            import os\n\n            with tempfile.NamedTemporaryFile(suffix=\'.wav\', delete=False) as temp_file:\n                temp_file.write(wav_buffer.getvalue())\n                temp_filename = temp_file.name\n\n            # Transcribe using Whisper\n            result = self.whisper_model.transcribe(temp_filename)\n            text = result[\'text\'].strip()\n\n            # Clean up temporary file\n            os.unlink(temp_filename)\n\n            if text:  # If transcription is not empty\n                self.get_logger().info(f\'Whisper recognized: "{text}"\')\n\n                # Add to command queue\n                self.command_queue.put(text)\n\n                # Publish recognized text\n                text_msg = String()\n                text_msg.data = text\n                self.text_pub.publish(text_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Whisper transcription error: {str(e)}\')\n\n    def process_commands(self):\n        """Process recognized commands"""\n        # Process all available commands in queue\n        while not self.command_queue.empty():\n            try:\n                command_text = self.command_queue.get_nowait()\n                self.process_command(command_text)\n            except queue.Empty:\n                break\n\n    def process_command(self, command_text):\n        """Process a single voice command from Whisper"""\n        # This would typically forward to a higher-level command processor\n        # For now, just log the command\n        self.get_logger().info(f\'Processing Whisper command: {command_text}\')\n\n    def destroy_node(self):\n        """Clean up audio resources"""\n        if self.microphone_available:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperVoiceProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down Whisper voice processor...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"voice-activity-detection",children:"Voice Activity Detection"}),"\n",(0,r.jsx)(n.h3,{id:"advanced-voice-activity-detection-node",children:"Advanced Voice Activity Detection Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Bool, Float32\nfrom sensor_msgs.msg import AudioData\nimport numpy as np\nimport pyaudio\nimport threading\nimport time\nfrom collections import deque\n\nclass VoiceActivityDetector(Node):\n    def __init__(self):\n        super().__init__(\'voice_activity_detector\')\n\n        # Publishers\n        self.vad_status_pub = self.create_publisher(Bool, \'/voice_activity\', 10)\n        self.energy_pub = self.create_publisher(Float32, \'/audio_energy\', 10)\n\n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        # VAD parameters\n        self.energy_threshold = 1000  # Adjust based on environment\n        self.silence_duration_threshold = 0.5  # seconds of silence to end speech\n        self.speech_duration_threshold = 0.1   # minimum speech duration\n\n        # Audio analysis\n        self.audio = pyaudio.PyAudio()\n        self.energy_history = deque(maxlen=100)  # Store recent energy values\n        self.speech_active = False\n        self.silence_start_time = None\n        self.speech_start_time = None\n\n        try:\n            self.stream = self.audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n            self.microphone_available = True\n        except Exception as e:\n            self.get_logger().warn(f\'Could not initialize audio stream: {e}\')\n            self.microphone_available = False\n\n        # Start VAD thread\n        self.vad_thread = threading.Thread(target=self.vad_loop, daemon=True)\n        self.vad_thread.start()\n\n        self.get_logger().info(\'Voice Activity Detector initialized\')\n\n    def vad_loop(self):\n        """Main loop for voice activity detection"""\n        if not self.microphone_available:\n            return\n\n        self.get_logger().info(\'Voice activity detection started\')\n\n        while rclpy.ok():\n            try:\n                # Read audio data\n                data = self.stream.read(self.chunk, exception_on_overflow=False)\n\n                # Convert to numpy array\n                audio_data = np.frombuffer(data, dtype=np.int16)\n\n                # Calculate energy (RMS)\n                energy = np.sqrt(np.mean(audio_data**2))\n\n                # Update energy history\n                self.energy_history.append(energy)\n\n                # Publish current energy\n                energy_msg = Float32()\n                energy_msg.data = float(energy)\n                self.energy_pub.publish(energy_msg)\n\n                # Detect voice activity\n                is_speech = self.detect_voice_activity(energy)\n\n                # Update speech state\n                if is_speech and not self.speech_active:\n                    # Speech started\n                    self.speech_active = True\n                    self.speech_start_time = time.time()\n                    self.publish_vad_status(True)\n                    self.get_logger().info(\'Speech detected\')\n                elif not is_speech and self.speech_active:\n                    # Check if speech has ended (enough silence)\n                    if self.silence_start_time is None:\n                        self.silence_start_time = time.time()\n                    elif time.time() - self.silence_start_time > self.silence_duration_threshold:\n                        # Speech ended\n                        speech_duration = time.time() - self.speech_start_time\n                        if speech_duration >= self.speech_duration_threshold:\n                            self.get_logger().info(f\'Speech ended (duration: {speech_duration:.2f}s)\')\n\n                        self.speech_active = False\n                        self.silence_start_time = None\n                        self.publish_vad_status(False)\n\n                time.sleep(0.01)  # 100Hz processing rate\n\n            except Exception as e:\n                self.get_logger().error(f\'VAD loop error: {str(e)}\')\n                time.sleep(0.1)\n\n    def detect_voice_activity(self, energy):\n        """Detect voice activity based on energy threshold"""\n        # Use adaptive threshold based on recent history\n        if len(self.energy_history) > 10:\n            recent_avg = np.mean(list(self.energy_history)[-10:])\n            adaptive_threshold = recent_avg * 2.0  # 2x the recent average\n\n            # Use the higher of fixed and adaptive threshold\n            threshold = max(self.energy_threshold, adaptive_threshold)\n        else:\n            threshold = self.energy_threshold\n\n        return energy > threshold\n\n    def publish_vad_status(self, is_speech):\n        """Publish voice activity status"""\n        status_msg = Bool()\n        status_msg.data = is_speech\n        self.vad_status_pub.publish(status_msg)\n\n    def destroy_node(self):\n        """Clean up audio resources"""\n        if self.microphone_available:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceActivityDetector()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down voice activity detector...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"natural-language-processing-for-commands",children:"Natural Language Processing for Commands"}),"\n",(0,r.jsx)(n.h3,{id:"command-understanding-with-nlp",children:"Command Understanding with NLP"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path\nimport spacy\nimport re\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass CommandIntent:\n    \"\"\"Data class for command intent\"\"\"\n    action: str\n    direction: Optional[str] = None\n    distance: Optional[float] = None\n    angle: Optional[float] = None\n    target: Optional[str] = None\n\nclass NLPCommandProcessor(Node):\n    def __init__(self):\n        super().__init__('nlp_command_processor')\n\n        # Subscribe to recognized text\n        self.text_sub = self.create_subscription(\n            String,\n            '/recognized_text',\n            self.text_callback,\n            10\n        )\n\n        # Publishers\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)\n        self.intent_pub = self.create_publisher(String, '/command_intent', 10)\n\n        # Load spaCy model for NLP\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n            self.get_logger().info('spaCy model loaded successfully')\n        except OSError:\n            self.get_logger().error('spaCy model not found. Install with: python -m spacy download en_core_web_sm')\n            self.nlp = None\n\n        # Command action mappings\n        self.action_keywords = {\n            'move': ['move', 'go', 'walk', 'travel', 'navigate'],\n            'turn': ['turn', 'rotate', 'spin', 'pivot'],\n            'stop': ['stop', 'halt', 'pause', 'stand', 'freeze'],\n            'approach': ['approach', 'come to', 'go to', 'reach', 'get to'],\n            'find': ['find', 'locate', 'search for', 'look for', 'detect']\n        }\n\n        # Direction keywords\n        self.direction_keywords = {\n            'forward': ['forward', 'ahead', 'straight', 'front'],\n            'backward': ['backward', 'back', 'reverse'],\n            'left': ['left', 'port'],\n            'right': ['right', 'starboard']\n        }\n\n        # Distance/angle keywords\n        self.distance_units = ['meter', 'meters', 'm', 'centimeter', 'centimeters', 'cm', 'foot', 'feet', 'ft']\n        self.angle_units = ['degree', 'degrees', 'deg']\n\n        self.get_logger().info('NLP Command Processor initialized')\n\n    def text_callback(self, msg):\n        \"\"\"Process recognized text for command understanding\"\"\"\n        if self.nlp is None:\n            return\n\n        text = msg.data.strip()\n        if not text:\n            return\n\n        self.get_logger().info(f'Processing text: \"{text}\"')\n\n        # Parse command intent\n        intent = self.parse_command_intent(text)\n\n        if intent:\n            # Publish intent for monitoring\n            intent_msg = String()\n            intent_msg.data = f\"Action: {intent.action}, Direction: {intent.direction}, Distance: {intent.distance}, Angle: {intent.angle}, Target: {intent.target}\"\n            self.intent_pub.publish(intent_msg)\n\n            # Execute command\n            self.execute_command(intent)\n\n    def parse_command_intent(self, text: str) -> Optional[CommandIntent]:\n        \"\"\"Parse natural language command to intent\"\"\"\n        doc = self.nlp(text.lower())\n\n        # Extract action\n        action = self.extract_action(text)\n        if not action:\n            return None\n\n        # Extract direction\n        direction = self.extract_direction(text)\n\n        # Extract distance\n        distance = self.extract_distance(text)\n\n        # Extract angle\n        angle = self.extract_angle(text)\n\n        # Extract target/object\n        target = self.extract_target(doc)\n\n        return CommandIntent(\n            action=action,\n            direction=direction,\n            distance=distance,\n            angle=angle,\n            target=target\n        )\n\n    def extract_action(self, text: str) -> Optional[str]:\n        \"\"\"Extract the main action from text\"\"\"\n        text_lower = text.lower()\n\n        for action, keywords in self.action_keywords.items():\n            for keyword in keywords:\n                if keyword in text_lower:\n                    return action\n\n        return None\n\n    def extract_direction(self, text: str) -> Optional[str]:\n        \"\"\"Extract direction from text\"\"\"\n        text_lower = text.lower()\n\n        for direction, keywords in self.direction_keywords.items():\n            for keyword in keywords:\n                if keyword in text_lower:\n                    return direction\n\n        return None\n\n    def extract_distance(self, text: str) -> Optional[float]:\n        \"\"\"Extract distance from text\"\"\"\n        # Look for number followed by distance unit\n        pattern = r'(\\d+(?:\\.\\d+)?)\\s*(meter|meters|m|centimeter|centimeters|cm|foot|feet|ft)'\n        match = re.search(pattern, text.lower())\n\n        if match:\n            value = float(match.group(1))\n            unit = match.group(2)\n\n            # Convert to meters\n            if unit in ['centimeter', 'centimeters', 'cm']:\n                return value / 100.0\n            elif unit in ['foot', 'feet', 'ft']:\n                return value * 0.3048\n            else:  # meters\n                return value\n\n        return None\n\n    def extract_angle(self, text: str) -> Optional[float]:\n        \"\"\"Extract angle from text\"\"\"\n        # Look for number followed by angle unit\n        pattern = r'(\\d+(?:\\.\\d+)?)\\s*(degree|degrees|deg)'\n        match = re.search(pattern, text.lower())\n\n        if match:\n            value = float(match.group(1))\n            # Convert to radians for ROS\n            return value * 3.14159 / 180.0\n\n        return None\n\n    def extract_target(self, doc) -> Optional[str]:\n        \"\"\"Extract target object from parsed document\"\"\"\n        # Look for nouns that might be targets\n        for token in doc:\n            if token.pos_ in ['NOUN', 'PROPN'] and token.dep_ in ['dobj', 'pobj']:\n                return token.text\n\n        # Look for named entities\n        for ent in doc.ents:\n            if ent.label_ in ['OBJECT', 'PERSON', 'FAC', 'GPE']:\n                return ent.text\n\n        return None\n\n    def execute_command(self, intent: CommandIntent):\n        \"\"\"Execute the parsed command intent\"\"\"\n        if intent.action == 'move':\n            self.execute_move_command(intent)\n        elif intent.action == 'turn':\n            self.execute_turn_command(intent)\n        elif intent.action == 'stop':\n            self.execute_stop_command()\n        elif intent.action == 'approach':\n            self.execute_approach_command(intent)\n        elif intent.action == 'find':\n            self.execute_find_command(intent)\n        else:\n            self.get_logger().info(f'Unknown action: {intent.action}')\n\n    def execute_move_command(self, intent: CommandIntent):\n        \"\"\"Execute move command\"\"\"\n        cmd = Twist()\n\n        if intent.direction == 'forward':\n            cmd.linear.x = 0.3 if intent.distance is None else min(0.5, intent.distance)\n        elif intent.direction == 'backward':\n            cmd.linear.x = -0.3 if intent.distance is None else -min(0.5, intent.distance)\n        elif intent.direction == 'left':\n            cmd.angular.z = 0.5 if intent.angle is None else intent.angle\n        elif intent.direction == 'right':\n            cmd.angular.z = -0.5 if intent.angle is None else -intent.angle\n\n        self.cmd_pub.publish(cmd)\n\n    def execute_turn_command(self, intent: CommandIntent):\n        \"\"\"Execute turn command\"\"\"\n        cmd = Twist()\n\n        if intent.direction == 'left':\n            cmd.angular.z = 0.5 if intent.angle is None else intent.angle\n        elif intent.direction == 'right':\n            cmd.angular.z = -0.5 if intent.angle is None else -intent.angle\n\n        self.cmd_pub.publish(cmd)\n\n    def execute_stop_command(self):\n        \"\"\"Execute stop command\"\"\"\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def execute_approach_command(self, intent: CommandIntent):\n        \"\"\"Execute approach command\"\"\"\n        if intent.target:\n            self.get_logger().info(f'Approaching target: {intent.target}')\n            # In a real system, this would involve navigation to the target\n            # For now, just move forward\n            cmd = Twist()\n            cmd.linear.x = 0.2\n            self.cmd_pub.publish(cmd)\n\n    def execute_find_command(self, intent: CommandIntent):\n        \"\"\"Execute find command\"\"\"\n        if intent.target:\n            self.get_logger().info(f'Looking for: {intent.target}')\n            # In a real system, this would involve object detection\n            # For now, just rotate to scan\n            cmd = Twist()\n            cmd.angular.z = 0.3  # Slow rotation to look around\n            self.cmd_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NLPCommandProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down NLP command processor...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"multi-language-voice-commands",children:"Multi-Language Voice Commands"}),"\n",(0,r.jsx)(n.h3,{id:"multi-language-voice-processing",children:"Multi-Language Voice Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Int8\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\n\nclass MultiLanguageVoiceProcessor(Node):\n    def __init__(self):\n        super().__init__('multi_language_voice_processor')\n\n        # Publishers\n        self.text_pub = self.create_publisher(String, '/multilang_recognized_text', 10)\n        self.language_pub = self.create_publisher(Int8, '/detected_language', 10)\n\n        # Supported languages\n        self.supported_languages = {\n            'en': 'English',\n            'es': 'Spanish',\n            'fr': 'French',\n            'de': 'German',\n            'it': 'Italian',\n            'pt': 'Portuguese',\n            'ru': 'Russian',\n            'ja': 'Japanese',\n            'ko': 'Korean',\n            'zh': 'Chinese'\n        }\n\n        # Current language (default to English)\n        self.current_language = 'en'\n        self.language_names = list(self.supported_languages.keys())\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 4000\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Audio source\n        try:\n            self.microphone = sr.Microphone()\n        except OSError:\n            self.get_logger().warn('Microphone not found, using simulation mode')\n            self.microphone = None\n\n        # Command queue\n        self.command_queue = queue.Queue()\n\n        # Start recognition thread\n        self.recognition_thread = threading.Thread(target=self.recognition_loop, daemon=True)\n        self.recognition_thread.start()\n\n        # Timer for processing\n        self.timer = self.create_timer(0.1, self.process_commands)\n\n        self.get_logger().info('Multi-Language Voice Processor initialized')\n\n    def recognition_loop(self):\n        \"\"\"Main loop for multi-language recognition\"\"\"\n        if self.microphone is None:\n            return\n\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        self.get_logger().info('Multi-language voice recognition started')\n\n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Try recognition in current language first, then try others if needed\n                text = self.recognize_with_language(audio, self.current_language)\n\n                if text:\n                    self.get_logger().info(f'Recognized ({self.current_language}): \"{text}\"')\n\n                    # Add to queue\n                    self.command_queue.put((text, self.current_language))\n\n                    # Publish text\n                    text_msg = String()\n                    text_msg.data = f\"[{self.current_language}] {text}\"\n                    self.text_pub.publish(text_msg)\n\n                    # Publish language\n                    lang_msg = Int8()\n                    lang_msg.data = list(self.language_names).index(self.current_language)\n                    self.language_pub.publish(lang_msg)\n\n            except sr.WaitTimeoutError:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Multi-lang recognition error: {str(e)}')\n                time.sleep(0.1)\n\n    def recognize_with_language(self, audio, language_code):\n        \"\"\"Recognize audio with specific language\"\"\"\n        try:\n            # Use Google Web Speech API with specific language\n            text = self.recognizer.recognize_google(audio, language=language_code)\n            return text\n        except sr.UnknownValueError:\n            # Try other languages if current one fails\n            for other_lang in self.language_names:\n                if other_lang != language_code:\n                    try:\n                        text = self.recognizer.recognize_google(audio, language=other_lang)\n                        # Update current language\n                        self.current_language = other_lang\n                        self.get_logger().info(f'Switched to language: {other_lang}')\n                        return text\n                    except sr.UnknownValueError:\n                        continue\n            return None\n        except sr.RequestError as e:\n            self.get_logger().error(f'Speech recognition request error: {e}')\n            return None\n\n    def process_commands(self):\n        \"\"\"Process recognized commands\"\"\"\n        while not self.command_queue.empty():\n            try:\n                text, language = self.command_queue.get_nowait()\n                self.process_multilang_command(text, language)\n            except queue.Empty:\n                break\n\n    def process_multilang_command(self, text, language):\n        \"\"\"Process command in specific language\"\"\"\n        # Convert to lowercase for processing\n        text_lower = text.lower()\n\n        # Language-specific command mappings\n        command_mappings = self.get_command_mappings(language)\n\n        # Check for command matches\n        for command_phrase, command_func in command_mappings.items():\n            if command_phrase.lower() in text_lower:\n                self.get_logger().info(f'Executing {language} command: {command_phrase}')\n                command_func()\n                return\n\n        # If no match, log the unrecognized command\n        self.get_logger().info(f'Unrecognized {language} command: \"{text}\"')\n\n    def get_command_mappings(self, language):\n        \"\"\"Get command mappings for specific language\"\"\"\n        if language == 'en':  # English\n            return {\n                'move forward': self.move_forward,\n                'go forward': self.move_forward,\n                'move backward': self.move_backward,\n                'go backward': self.move_backward,\n                'turn left': self.turn_left,\n                'turn right': self.turn_right,\n                'stop': self.stop_robot,\n                'halt': self.stop_robot\n            }\n        elif language == 'es':  # Spanish\n            return {\n                'mover adelante': self.move_forward,\n                'ir adelante': self.move_forward,\n                'mover atr\xe1s': self.move_backward,\n                'girar izquierda': self.turn_left,\n                'girar derecha': self.turn_right,\n                'parar': self.stop_robot,\n                'detenerse': self.stop_robot\n            }\n        elif language == 'fr':  # French\n            return {\n                'avancer': self.move_forward,\n                'reculer': self.move_backward,\n                'tourner \xe0 gauche': self.turn_left,\n                'tourner \xe0 droite': self.turn_right,\n                'arr\xeater': self.stop_robot,\n                'stop': self.stop_robot\n            }\n        elif language == 'de':  # German\n            return {\n                'vorw\xe4rts': self.move_forward,\n                'r\xfcckw\xe4rts': self.move_backward,\n                'links': self.turn_left,\n                'rechts': self.turn_right,\n                'halten': self.stop_robot,\n                'stopp': self.stop_robot\n            }\n        else:  # Default to English for other languages\n            return {\n                'move forward': self.move_forward,\n                'go forward': self.move_forward,\n                'move backward': self.move_backward,\n                'go backward': self.move_backward,\n                'turn left': self.turn_left,\n                'turn right': self.turn_right,\n                'stop': self.stop_robot,\n                'halt': self.stop_robot\n            }\n\n    def move_forward(self):\n        \"\"\"Move robot forward\"\"\"\n        from geometry_msgs.msg import Twist\n        cmd = Twist()\n        cmd.linear.x = 0.3\n        cmd.angular.z = 0.0\n        self.create_publisher(Twist, '/cmd_vel', 10).publish(cmd)\n\n    def move_backward(self):\n        \"\"\"Move robot backward\"\"\"\n        from geometry_msgs.msg import Twist\n        cmd = Twist()\n        cmd.linear.x = -0.3\n        cmd.angular.z = 0.0\n        self.create_publisher(Twist, '/cmd_vel', 10).publish(cmd)\n\n    def turn_left(self):\n        \"\"\"Turn robot left\"\"\"\n        from geometry_msgs.msg import Twist\n        cmd = Twist()\n        cmd.linear.x = 0.1\n        cmd.angular.z = 0.5\n        self.create_publisher(Twist, '/cmd_vel', 10).publish(cmd)\n\n    def turn_right(self):\n        \"\"\"Turn robot right\"\"\"\n        from geometry_msgs.msg import Twist\n        cmd = Twist()\n        cmd.linear.x = 0.1\n        cmd.angular.z = -0.5\n        self.create_publisher(Twist, '/cmd_vel', 10).publish(cmd)\n\n    def stop_robot(self):\n        \"\"\"Stop robot movement\"\"\"\n        from geometry_msgs.msg import Twist\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.create_publisher(Twist, '/cmd_vel', 10).publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultiLanguageVoiceProcessor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down multi-language voice processor...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-lab-complete-voice-command-system",children:"Hands-on Lab: Complete Voice Command System"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you'll create a complete voice command system that integrates all components."}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-the-voice-command-system-launch-file",children:"Step 1: Create the Voice Command System Launch File"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"voice_command_system_launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Voice command system nodes\n    voice_processor = Node(\n        package='ai_robo_learning',\n        executable='voice_command_processor',\n        name='voice_command_processor',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    whisper_processor = Node(\n        package='ai_robo_learning',\n        executable='whisper_voice_processor',\n        name='whisper_voice_processor',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    vad_detector = Node(\n        package='ai_robo_learning',\n        executable='voice_activity_detector',\n        name='voice_activity_detector',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    nlp_processor = Node(\n        package='ai_robo_learning',\n        executable='nlp_command_processor',\n        name='nlp_command_processor',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    multilang_processor = Node(\n        package='ai_robo_learning',\n        executable='multi_language_voice_processor',\n        name='multi_language_voice_processor',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add all nodes\n    ld.add_action(voice_processor)\n    ld.add_action(whisper_processor)\n    ld.add_action(vad_detector)\n    ld.add_action(nlp_processor)\n    ld.add_action(multilang_processor)\n\n    return ld\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-create-the-complete-voice-command-node",children:"Step 2: Create the Complete Voice Command Node"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"complete_voice_command_system.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import AudioData\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\nimport numpy as np\nimport pyaudio\nfrom collections import deque\n\nclass CompleteVoiceCommandSystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_voice_command_system\')\n\n        # Publishers\n        self.text_pub = self.create_publisher(String, \'/complete_recognized_text\', 10)\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.status_pub = self.create_publisher(Bool, \'/voice_system_active\', 10)\n\n        # Initialize components\n        self.setup_speech_recognition()\n        self.setup_audio_input()\n        self.setup_command_processing()\n\n        # System state\n        self.system_active = True\n        self.listening_enabled = True\n        self.command_history = deque(maxlen=50)\n\n        # Start processing threads\n        self.recognition_thread = threading.Thread(target=self.recognition_loop, daemon=True)\n        self.recognition_thread.start()\n\n        # Timer for system monitoring\n        self.timer = self.create_timer(1.0, self.system_monitor)\n\n        self.get_logger().info(\'Complete Voice Command System initialized\')\n\n    def setup_speech_recognition(self):\n        """Setup speech recognition components"""\n        self.recognizer = sr.Recognizer()\n        self.recognizer.energy_threshold = 4000\n        self.recognizer.dynamic_energy_threshold = True\n\n    def setup_audio_input(self):\n        """Setup audio input components"""\n        self.rate = 16000\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        self.audio = pyaudio.PyAudio()\n\n        try:\n            self.microphone = sr.Microphone()\n            self.stream = self.audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.rate,\n                input=True,\n                frames_per_buffer=self.chunk\n            )\n            self.microphone_available = True\n        except Exception as e:\n            self.get_logger().warn(f\'Could not initialize audio: {e}\')\n            self.microphone_available = False\n\n    def setup_command_processing(self):\n        """Setup command processing components"""\n        self.command_queue = queue.Queue()\n        self.command_mappings = {\n            \'move forward\': self.move_forward,\n            \'go forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'go backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'spin left\': self.spin_left,\n            \'spin right\': self.spin_right,\n            \'stop\': self.stop_robot,\n            \'halt\': self.stop_robot,\n            \'help\': self.show_help\n        }\n\n    def recognition_loop(self):\n        """Main recognition loop"""\n        if not self.microphone_available:\n            self.get_logger().warn(\'Voice recognition not available\')\n            return\n\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        self.get_logger().info(\'Voice command system listening...\')\n\n        while rclpy.ok() and self.system_active:\n            try:\n                if self.listening_enabled:\n                    with self.microphone as source:\n                        audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                    # Process audio\n                    threading.Thread(\n                        target=self.process_audio,\n                        args=(audio,),\n                        daemon=True\n                    ).start()\n\n            except sr.WaitTimeoutError:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Recognition loop error: {str(e)}\')\n                time.sleep(0.1)\n\n    def process_audio(self, audio):\n        """Process audio and recognize speech"""\n        try:\n            # Try Google Speech Recognition\n            text = self.recognizer.recognize_google(audio)\n            self.process_recognized_text(text)\n\n        except sr.UnknownValueError:\n            self.get_logger().info(\'Could not understand audio\')\n        except sr.RequestError as e:\n            self.get_logger().error(f\'Speech recognition error: {e}\')\n\n    def process_recognized_text(self, text):\n        """Process recognized text"""\n        if not text.strip():\n            return\n\n        self.get_logger().info(f\'Recognized: "{text}"\')\n\n        # Add to history\n        self.command_history.append({\n            \'text\': text,\n            \'timestamp\': time.time()\n        })\n\n        # Publish recognized text\n        text_msg = String()\n        text_msg.data = text\n        self.text_pub.publish(text_msg)\n\n        # Process command\n        self.process_command(text)\n\n    def process_command(self, command_text):\n        """Process voice command"""\n        command_lower = command_text.lower().strip()\n\n        # Check for exact matches first\n        for cmd_phrase, cmd_func in self.command_mappings.items():\n            if cmd_phrase in command_lower:\n                self.get_logger().info(f\'Executing command: {cmd_phrase}\')\n                cmd_func()\n                return\n\n        # Check for fuzzy matches\n        self.process_fuzzy_command(command_lower)\n\n    def process_fuzzy_command(self, command_lower):\n        """Process commands using fuzzy matching"""\n        # Simple keyword matching\n        if any(word in command_lower for word in [\'forward\', \'ahead\', \'straight\']):\n            self.move_forward()\n        elif any(word in command_lower for word in [\'backward\', \'back\', \'reverse\']):\n            self.move_backward()\n        elif \'left\' in command_lower:\n            if \'spin\' in command_lower or \'turn\' in command_lower:\n                self.spin_left()\n            else:\n                self.turn_left()\n        elif \'right\' in command_lower:\n            if \'spin\' in command_lower or \'turn\' in command_lower:\n                self.spin_right()\n            else:\n                self.turn_right()\n        elif any(word in command_lower for word in [\'stop\', \'halt\', \'pause\']):\n            self.stop_robot()\n        else:\n            self.get_logger().info(f\'Unknown command: "{command_lower}"\')\n\n    def move_forward(self):\n        """Move robot forward"""\n        cmd = Twist()\n        cmd.linear.x = 0.3\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def move_backward(self):\n        """Move robot backward"""\n        cmd = Twist()\n        cmd.linear.x = -0.3\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def turn_left(self):\n        """Turn robot left"""\n        cmd = Twist()\n        cmd.linear.x = 0.1\n        cmd.angular.z = 0.5\n        self.cmd_pub.publish(cmd)\n\n    def turn_right(self):\n        """Turn robot right"""\n        cmd = Twist()\n        cmd.linear.x = 0.1\n        cmd.angular.z = -0.5\n        self.cmd_pub.publish(cmd)\n\n    def spin_left(self):\n        """Spin robot left"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.8\n        self.cmd_pub.publish(cmd)\n\n    def spin_right(self):\n        """Spin robot right"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = -0.8\n        self.cmd_pub.publish(cmd)\n\n    def stop_robot(self):\n        """Stop robot"""\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        self.cmd_pub.publish(cmd)\n\n    def show_help(self):\n        """Show available commands"""\n        help_text = """\n        Available voice commands:\n        - Move forward/Go forward\n        - Move backward/Go backward\n        - Turn left/Turn right\n        - Spin left/Spin right\n        - Stop/Halt\n        """\n        self.get_logger().info(help_text)\n\n    def system_monitor(self):\n        """Monitor system status"""\n        status_msg = Bool()\n        status_msg.data = self.system_active\n        self.status_pub.publish(status_msg)\n\n        # Log system status periodically\n        self.get_logger().info(f\'Voice system active: {self.system_active}, Commands in history: {len(self.command_history)}\')\n\n    def destroy_node(self):\n        """Clean up resources"""\n        self.system_active = False\n        if self.microphone_available:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CompleteVoiceCommandSystem()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down complete voice command system...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-test-the-voice-command-system",children:"Step 3: Test the Voice Command System"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Make sure you have the required dependencies:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install speechrecognition pyaudio numpy\n# For Whisper (optional):\npip3 install openai-whisper\n# For spaCy (optional):\npython -m spacy download en_core_web_sm\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Run the complete voice command system:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 complete_voice_command_system.py\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsxs)(n.li,{children:["Speak commands to your microphone such as:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Move forward"'}),"\n",(0,r.jsx)(n.li,{children:'"Turn left"'}),"\n",(0,r.jsx)(n.li,{children:'"Stop"'}),"\n",(0,r.jsx)(n.li,{children:'"Spin right"'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust Recognition"}),": Implement multiple fallback recognition methods"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Activity Detection"}),": Use VAD to reduce processing when not speaking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Reduction"}),": Apply audio filtering to improve recognition quality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Validation"}),": Verify commands make sense before executing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error Handling"}),": Gracefully handle recognition failures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Consider privacy implications of always-listening systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-language Support"}),": Design systems to support multiple languages"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you'll be ready to learn about cognitive planning with LLMs in Chapter 3, where you'll explore how to use large language models to translate natural language commands into sequences of robot actions."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(6540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);