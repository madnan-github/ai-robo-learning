"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[560],{843:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/chapter-3/cognitive-planning-llms","title":"Cognitive Planning with LLMs","description":"This chapter covers using Large Language Models (LLMs) for cognitive planning in robotics, enabling robots to understand complex natural language commands and translate them into executable action sequences.","source":"@site/docs/module-4/chapter-3/cognitive-planning-llms.md","sourceDirName":"module-4/chapter-3","slug":"/module-4/chapter-3/cognitive-planning-llms","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-3/cognitive-planning-llms","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-4/chapter-3/cognitive-planning-llms.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Cognitive Planning with LLMs"},"sidebar":"tutorialSidebar","previous":{"title":"Voice Command Processing","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-2/voice-command-processing"},"next":{"title":"Computer Vision for Robotics","permalink":"/ai-robo-learning/ur/docs/module-4/chapter-4/computer-vision-robotics"}}');var s=a(4848),i=a(8453);const o={sidebar_position:3,title:"Cognitive Planning with LLMs"},r="Cognitive Planning with LLMs",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"LLM Integration for Robotic Planning",id:"llm-integration-for-robotic-planning",level:2},{value:"Overview of LLM-Based Planning",id:"overview-of-llm-based-planning",level:3},{value:"Basic LLM Integration Node",id:"basic-llm-integration-node",level:3},{value:"Advanced Planning with Context Awareness",id:"advanced-planning-with-context-awareness",level:2},{value:"Context-Aware Cognitive Planning",id:"context-aware-cognitive-planning",level:3},{value:"Task Decomposition and Sequencing",id:"task-decomposition-and-sequencing",level:2},{value:"Hierarchical Task Planner",id:"hierarchical-task-planner",level:3},{value:"Multi-Modal Reasoning",id:"multi-modal-reasoning",level:2},{value:"Multi-Modal Cognitive Planner",id:"multi-modal-cognitive-planner",level:3},{value:"Safety and Validation in LLM-Driven Robotics",id:"safety-and-validation-in-llm-driven-robotics",level:2},{value:"Safe LLM Planning with Validation",id:"safe-llm-planning-with-validation",level:3},{value:"Hands-on Lab: Complete Cognitive Planning System",id:"hands-on-lab-complete-cognitive-planning-system",level:2},{value:"Step 1: Create the Cognitive Planning System Launch File",id:"step-1-create-the-cognitive-planning-system-launch-file",level:3},{value:"Step 2: Create the Complete Cognitive Planning Node",id:"step-2-create-the-complete-cognitive-planning-node",level:3},{value:"Step 3: Test the Cognitive Planning System",id:"step-3-test-the-cognitive-planning-system",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,s.jsx)(e.p,{children:"This chapter covers using Large Language Models (LLMs) for cognitive planning in robotics, enabling robots to understand complex natural language commands and translate them into executable action sequences."}),"\n",(0,s.jsx)(e.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsx)(e.p,{children:"In this chapter, you'll explore:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"LLM integration for robotic planning"}),"\n",(0,s.jsx)(e.li,{children:"Natural language command interpretation"}),"\n",(0,s.jsx)(e.li,{children:"Task decomposition and sequencing"}),"\n",(0,s.jsx)(e.li,{children:"Context-aware planning"}),"\n",(0,s.jsx)(e.li,{children:"Multi-modal reasoning"}),"\n",(0,s.jsx)(e.li,{children:"Safety and validation in LLM-driven robotics"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Completion of Module 1-4, Chapter 2"}),"\n",(0,s.jsx)(e.li,{children:"Understanding of ROS 2 messaging and services"}),"\n",(0,s.jsx)(e.li,{children:"Basic knowledge of AI/ML concepts"}),"\n",(0,s.jsx)(e.li,{children:"Experience with Python and API integration"}),"\n",(0,s.jsx)(e.li,{children:"Understanding of robot action execution"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"llm-integration-for-robotic-planning",children:"LLM Integration for Robotic Planning"}),"\n",(0,s.jsx)(e.h3,{id:"overview-of-llm-based-planning",children:"Overview of LLM-Based Planning"}),"\n",(0,s.jsx)(e.p,{children:"Large Language Models can serve as cognitive planners for robots by:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Understanding Natural Language"}),": Interpreting complex commands in human language"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking down high-level goals into executable steps"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Context Reasoning"}),": Using environmental and situational context"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Sequencing"}),": Planning the order of robot actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Validation"}),": Ensuring planned actions are safe and feasible"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"basic-llm-integration-node",children:"Basic LLM Integration Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nimport openai\nimport json\nimport re\nimport threading\nfrom queue import Queue\n\nclass LLMCognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'llm_cognitive_planner\')\n\n        # Subscribe to natural language commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for planning results\n        self.plan_pub = self.create_publisher(String, \'/cognitive_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'/planning_status\', 10)\n\n        # LLM configuration\n        self.api_key = None  # Should be set via environment variable\n        self.model = "gpt-3.5-turbo"  # or "gpt-4" for better performance\n        self.max_tokens = 500\n        self.temperature = 0.3\n\n        # Planning queue\n        self.planning_queue = Queue()\n        self.planning_thread = threading.Thread(target=self.planning_worker, daemon=True)\n        self.planning_thread.start()\n\n        # Robot capabilities and environment context\n        self.robot_capabilities = [\n            "move_forward", "move_backward", "turn_left", "turn_right",\n            "spin_left", "spin_right", "stop", "pick_object", "place_object",\n            "navigate_to", "detect_object", "grasp", "release"\n        ]\n\n        self.environment_context = {\n            "room_layout": "The room has a door to the north, a window to the east, and a table in the center.",\n            "object_locations": {\n                "book": [1.0, 2.0, 0.0],\n                "cup": [1.5, 1.5, 0.0],\n                "chair": [0.5, 0.5, 0.0]\n            },\n            "robot_position": [0.0, 0.0, 0.0]\n        }\n\n        self.get_logger().info(\'LLM Cognitive Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received command: "{command}"\')\n            self.planning_queue.put(command)\n\n    def planning_worker(self):\n        """Worker thread for LLM-based planning"""\n        while rclpy.ok():\n            try:\n                command = self.planning_queue.get(timeout=1.0)\n                self.process_command_with_llm(command)\n            except:\n                continue  # Timeout, continue loop\n\n    def process_command_with_llm(self, command):\n        """Process command using LLM"""\n        try:\n            # Prepare the prompt for the LLM\n            prompt = self.create_planning_prompt(command)\n\n            # Call the LLM API\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.get_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            # Extract the plan from the response\n            plan_json = response.choices[0].message.content.strip()\n\n            # Validate and publish the plan\n            if self.validate_plan(plan_json):\n                self.publish_plan(plan_json)\n                self.get_logger().info(f\'Generated plan for command: "{command}"\')\n            else:\n                self.get_logger().error(\'Invalid plan generated by LLM\')\n\n        except Exception as e:\n            self.get_logger().error(f\'LLM planning error: {str(e)}\')\n            # Publish error status\n            status_msg = String()\n            status_msg.data = f"error: {str(e)}"\n            self.status_pub.publish(status_msg)\n\n    def create_planning_prompt(self, command):\n        """Create prompt for LLM planning"""\n        prompt = f"""\n        You are a cognitive planner for a humanoid robot. Your task is to decompose a natural language command into a sequence of executable robot actions.\n\n        Robot capabilities: {\', \'.join(self.robot_capabilities)}\n        Environment context: {json.dumps(self.environment_context, indent=2)}\n\n        Command: "{command}"\n\n        Please respond with a JSON object containing:\n        1. "actions": A list of robot actions to execute\n        2. "reasoning": Brief explanation of your plan\n        3. "estimated_time": Estimated time to complete the task in seconds\n\n        Each action should be a dictionary with:\n        - "action": The specific action to take\n        - "parameters": Any required parameters (position, object, etc.)\n        - "description": Human-readable description of the action\n\n        Example response format:\n        {{\n            "actions": [\n                {{\n                    "action": "navigate_to",\n                    "parameters": {{"x": 1.0, "y": 2.0}},\n                    "description": "Move to position (1.0, 2.0)"\n                }},\n                {{\n                    "action": "pick_object",\n                    "parameters": {{"object": "book"}},\n                    "description": "Pick up the book"\n                }}\n            ],\n            "reasoning": "The user wants me to get the book from the table. I need to navigate to the table and then pick up the book.",\n            "estimated_time": 30\n        }}\n        """\n        return prompt\n\n    def get_system_prompt(self):\n        """Get system prompt for LLM"""\n        return """\n        You are an expert robotic cognitive planner. Your role is to translate natural language commands into structured action plans for a humanoid robot.\n\n        Guidelines:\n        1. Always consider robot safety and physical constraints\n        2. Use only the robot capabilities provided\n        3. Provide realistic time estimates\n        4. Include error handling in your plans when appropriate\n        5. Ensure the plan is executable in the given environment\n        6. Break complex tasks into simple, sequential actions\n        7. If a command is ambiguous, make reasonable assumptions based on context\n        8. Return only valid JSON with the specified structure\n        """\n\n    def validate_plan(self, plan_json_str):\n        """Validate the plan returned by LLM"""\n        try:\n            plan = json.loads(plan_json_str)\n\n            # Check required fields\n            required_fields = [\'actions\', \'reasoning\', \'estimated_time\']\n            if not all(field in plan for field in required_fields):\n                return False\n\n            # Validate actions\n            if not isinstance(plan[\'actions\'], list):\n                return False\n\n            for action in plan[\'actions\']:\n                if not isinstance(action, dict):\n                    return False\n                if \'action\' not in action:\n                    return False\n                # Check if action is in robot capabilities\n                if action[\'action\'] not in self.robot_capabilities:\n                    # Allow navigation and other common actions that might not be explicitly listed\n                    pass\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception:\n            return False\n\n    def publish_plan(self, plan_json_str):\n        """Publish the generated plan"""\n        plan_msg = String()\n        plan_msg.data = plan_json_str\n        self.plan_pub.publish(plan_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMCognitivePlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down LLM cognitive planner...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"advanced-planning-with-context-awareness",children:"Advanced Planning with Context Awareness"}),"\n",(0,s.jsx)(e.h3,{id:"context-aware-cognitive-planning",children:"Context-Aware Cognitive Planning"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom sensor_msgs.msg import LaserScan, Image\nfrom nav_msgs.msg import OccupancyGrid\nimport openai\nimport json\nimport threading\nfrom queue import Queue\nimport time\n\nclass ContextAwarePlanner(Node):\n    def __init__(self):\n        super().__init__(\'context_aware_planner\')\n\n        # Subscribe to natural language commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        # Subscribe to environmental sensors\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'/map\',\n            self.map_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'/context_aware_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'/context_status\', 10)\n\n        # LLM configuration\n        self.model = "gpt-4"  # Using GPT-4 for better reasoning\n        self.max_tokens = 800\n        self.temperature = 0.2\n\n        # Context data\n        self.current_scan = None\n        self.current_map = None\n        self.last_update_time = time.time()\n        self.context_update_interval = 5.0  # seconds\n\n        # Planning queue\n        self.planning_queue = Queue()\n        self.planning_thread = threading.Thread(target=self.planning_worker, daemon=True)\n        self.planning_thread.start()\n\n        # Robot and environment context\n        self.robot_state = {\n            "position": [0.0, 0.0, 0.0],\n            "orientation": [0.0, 0.0, 0.0, 1.0],  # quaternion\n            "battery_level": 100.0,\n            "current_task": None\n        }\n\n        self.environment_state = {\n            "obstacles": [],\n            "navigable_areas": [],\n            "object_locations": {},\n            "room_layout": "unknown"\n        }\n\n        self.get_logger().info(\'Context-Aware Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command with context"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received command: "{command}"\')\n\n            # Update context if needed\n            self.update_context()\n\n            # Add to planning queue\n            self.planning_queue.put({\n                \'command\': command,\n                \'timestamp\': time.time(),\n                \'context\': self.get_current_context()\n            })\n\n    def scan_callback(self, msg):\n        """Process laser scan for obstacle detection"""\n        self.current_scan = msg\n\n    def map_callback(self, msg):\n        """Process occupancy grid for map information"""\n        self.current_map = msg\n\n    def update_context(self):\n        """Update environmental context from sensors"""\n        current_time = time.time()\n        if current_time - self.last_update_time < self.context_update_interval:\n            return  # Don\'t update too frequently\n\n        # Update obstacles from laser scan\n        if self.current_scan is not None:\n            self.update_obstacles_from_scan()\n\n        # Update map information\n        if self.current_map is not None:\n            self.update_map_context()\n\n        self.last_update_time = current_time\n\n    def update_obstacles_from_scan(self):\n        """Extract obstacle information from laser scan"""\n        scan = self.current_scan\n        obstacles = []\n\n        # Process scan ranges to identify obstacles\n        for i, range_val in enumerate(scan.ranges):\n            if range_val > scan.range_min and range_val < scan.range_max:\n                angle = scan.angle_min + i * scan.angle_increment\n                x = range_val * __import__(\'math\').cos(angle)\n                y = range_val * __import__(\'math\').sin(angle)\n\n                # Only add obstacles within a reasonable distance\n                if range_val < 2.0:  # 2 meters threshold\n                    obstacles.append({\n                        \'x\': x,\n                        \'y\': y,\n                        \'distance\': range_val,\n                        \'angle\': angle\n                    })\n\n        self.environment_state[\'obstacles\'] = obstacles\n\n    def update_map_context(self):\n        """Update map context from occupancy grid"""\n        # Extract relevant information from the map\n        map_info = self.current_map.info\n        self.environment_state[\'room_layout\'] = f"Map size: {map_info.width}x{map_info.height}, resolution: {map_info.resolution}"\n\n    def get_current_context(self):\n        """Get current environmental and robot context"""\n        return {\n            "robot_state": self.robot_state,\n            "environment_state": self.environment_state,\n            "timestamp": time.time()\n        }\n\n    def planning_worker(self):\n        """Worker thread for context-aware planning"""\n        while rclpy.ok():\n            try:\n                plan_request = self.planning_queue.get(timeout=1.0)\n                self.process_command_with_context(plan_request)\n            except:\n                continue\n\n    def process_command_with_context(self, plan_request):\n        """Process command using LLM with environmental context"""\n        command = plan_request[\'command\']\n        context = plan_request[\'context\']\n\n        try:\n            # Create detailed prompt with context\n            prompt = self.create_contextual_prompt(command, context)\n\n            # Call LLM API\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.get_contextual_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            plan_json = response.choices[0].message.content.strip()\n\n            # Validate and publish plan\n            if self.validate_contextual_plan(plan_json):\n                self.publish_contextual_plan(plan_json, command)\n                self.get_logger().info(f\'Generated contextual plan for: "{command}"\')\n            else:\n                self.get_logger().error(\'Invalid contextual plan generated\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Contextual planning error: {str(e)}\')\n            status_msg = String()\n            status_msg.data = f"error: {str(e)}"\n            self.status_pub.publish(status_msg)\n\n    def create_contextual_prompt(self, command, context):\n        """Create prompt with environmental context"""\n        return f"""\n        You are an advanced cognitive planner for a humanoid robot. Consider the current environmental context when creating plans.\n\n        Current robot state: {json.dumps(context[\'robot_state\'], indent=2)}\n        Current environment state: {json.dumps(context[\'environment_state\'], indent=2)}\n\n        Natural language command: "{command}"\n\n        Create a detailed action plan that accounts for:\n        1. Current obstacles and navigable areas\n        2. Robot\'s current position and battery level\n        3. Environmental constraints and layout\n        4. Safety considerations based on obstacle positions\n\n        Respond with a JSON object containing:\n        - "actions": List of sequential actions with parameters\n        - "safety_considerations": How the plan accounts for obstacles/safety\n        - "alternative_paths": Other possible approaches if primary plan fails\n        - "estimated_time": Time estimate considering environmental factors\n        - "confidence": Your confidence level in this plan (0-1)\n\n        Each action should include:\n        - "action": The specific action\n        - "parameters": Required parameters\n        - "preconditions": Conditions that must be true before execution\n        - "expected_outcome": What should happen after execution\n        """\n\n    def get_contextual_system_prompt(self):\n        """System prompt for contextual planning"""\n        return """\n        You are an expert contextual robotic planner. Your role is to create safe, efficient robot plans that account for environmental constraints and real-time sensor data.\n\n        Key considerations:\n        1. Prioritize safety - avoid obstacles and dangerous situations\n        2. Consider robot\'s current state (position, battery, etc.)\n        3. Account for environmental constraints (obstacles, layout)\n        4. Provide multiple options when uncertainty exists\n        5. Include safety checks and validation steps\n        6. Consider time efficiency while maintaining safety\n        7. Plan for contingencies and error recovery\n        8. Return only valid JSON with the specified structure\n        """\n\n    def validate_contextual_plan(self, plan_json_str):\n        """Validate contextual plan"""\n        try:\n            plan = json.loads(plan_json_str)\n\n            required_fields = [\'actions\', \'safety_considerations\', \'estimated_time\', \'confidence\']\n            if not all(field in plan for field in required_fields):\n                return False\n\n            if not isinstance(plan[\'actions\'], list):\n                return False\n\n            if not (0 <= plan.get(\'confidence\', 0) <= 1):\n                return False\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception:\n            return False\n\n    def publish_contextual_plan(self, plan_json_str, original_command):\n        """Publish contextual plan with metadata"""\n        # Add metadata to the plan\n        plan_data = json.loads(plan_json_str)\n        plan_data[\'original_command\'] = original_command\n        plan_data[\'generation_time\'] = time.time()\n\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan_data)\n        self.plan_pub.publish(plan_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ContextAwarePlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down context-aware planner...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"task-decomposition-and-sequencing",children:"Task Decomposition and Sequencing"}),"\n",(0,s.jsx)(e.h3,{id:"hierarchical-task-planner",children:"Hierarchical Task Planner"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nimport openai\nimport json\nimport threading\nfrom queue import Queue\nimport time\nfrom typing import List, Dict, Any\n\nclass HierarchicalTaskPlanner(Node):\n    def __init__(self):\n        super().__init__(\'hierarchical_task_planner\')\n\n        # Subscribe to high-level commands\n        self.high_level_sub = self.create_subscription(\n            String,\n            \'/high_level_command\',\n            self.high_level_callback,\n            10\n        )\n\n        # Publishers\n        self.task_pub = self.create_publisher(String, \'/decomposed_tasks\', 10)\n        self.subtask_pub = self.create_publisher(String, \'/subtasks\', 10)\n\n        # LLM configuration\n        self.model = "gpt-4"\n        self.max_tokens = 1000\n        self.temperature = 0.1\n\n        # Task queues\n        self.high_level_queue = Queue()\n        self.task_decomposition_thread = threading.Thread(\n            target=self.task_decomposition_worker, daemon=True\n        )\n        self.task_decomposition_thread.start()\n\n        # Task execution context\n        self.current_tasks = {}\n        self.task_dependencies = {}\n\n        # Robot capabilities and constraints\n        self.robot_capabilities = {\n            "navigation": {\n                "max_speed": 0.5,\n                "turn_speed": 0.8,\n                "step_height": 0.1\n            },\n            "manipulation": {\n                "max_weight": 2.0,\n                "reach_distance": 1.0,\n                "precision": "high"\n            },\n            "sensors": {\n                "camera": True,\n                "lidar": True,\n                "imu": True,\n                "microphone": True\n            }\n        }\n\n        self.get_logger().info(\'Hierarchical Task Planner initialized\')\n\n    def high_level_callback(self, msg):\n        """Process high-level command for decomposition"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received high-level command: "{command}"\')\n            self.high_level_queue.put({\n                \'command\': command,\n                \'timestamp\': time.time(),\n                \'task_id\': f"task_{int(time.time() * 1000)}"\n            })\n\n    def task_decomposition_worker(self):\n        """Worker thread for task decomposition"""\n        while rclpy.ok():\n            try:\n                task_request = self.high_level_queue.get(timeout=1.0)\n                self.decompose_task(task_request)\n            except:\n                continue\n\n    def decompose_task(self, task_request):\n        """Decompose high-level task into subtasks using LLM"""\n        command = task_request[\'command\']\n        task_id = task_request[\'task_id\']\n\n        try:\n            # Create decomposition prompt\n            prompt = self.create_decomposition_prompt(command)\n\n            # Call LLM for task decomposition\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.get_decomposition_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            decomposition_json = response.choices[0].message.content.strip()\n\n            # Validate and process decomposition\n            if self.validate_decomposition(decomposition_json):\n                decomposition = json.loads(decomposition_json)\n                self.process_decomposition(decomposition, task_id, command)\n                self.get_logger().info(f\'Decomposed task {task_id}: "{command}"\')\n            else:\n                self.get_logger().error(f\'Invalid decomposition for task {task_id}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Task decomposition error: {str(e)}\')\n\n    def create_decomposition_prompt(self, command):\n        """Create prompt for task decomposition"""\n        return f"""\n        You are a hierarchical task planner for a humanoid robot. Decompose the following high-level command into a structured task hierarchy.\n\n        Robot capabilities and constraints: {json.dumps(self.robot_capabilities, indent=2)}\n\n        High-level command: "{command}"\n\n        Decompose this into a hierarchical structure with:\n        1. Main tasks (high-level steps)\n        2. Subtasks (detailed actions for each main task)\n        3. Primitives (basic robot actions for each subtask)\n\n        Consider:\n        - Task dependencies and ordering\n        - Resource requirements\n        - Safety constraints\n        - Environmental context\n        - Robot capabilities\n\n        Return a JSON object with structure:\n        {{\n            "task_id": "unique identifier",\n            "original_command": "the original command",\n            "main_tasks": [\n                {{\n                    "id": "main_task_1",\n                    "description": "what this task accomplishes",\n                    "subtasks": [\n                        {{\n                            "id": "subtask_1_1",\n                            "description": "detailed subtask description",\n                            "primitives": [\n                                {{\n                                    "action": "robot primitive action",\n                                    "parameters": {{"param": "value"}},\n                                    "preconditions": ["condition1", "condition2"],\n                                    "postconditions": ["result1", "result2"],\n                                    "estimated_duration": 5.0\n                                }}\n                            ],\n                            "dependencies": ["other_subtask_id"],  # tasks that must complete first\n                            "success_criteria": "how to verify this subtask succeeded"\n                        }}\n                    ],\n                    "success_criteria": "how to verify this main task succeeded"\n                }}\n            ],\n            "overall_success_criteria": "how to verify the entire command was completed",\n            "estimated_total_time": 120.0,\n            "risk_assessment": "potential risks and mitigation strategies"\n        }}\n\n        Example for command "Clean the room":\n        {{\n            "task_id": "task_001",\n            "original_command": "Clean the room",\n            "main_tasks": [\n                {{\n                    "id": "navigation",\n                    "description": "Navigate to cleaning locations",\n                    "subtasks": [\n                        {{\n                            "id": "move_to_dining_table",\n                            "description": "Move to the dining table area",\n                            "primitives": [\n                                {{\n                                    "action": "navigate_to",\n                                    "parameters": {{"x": 2.0, "y": 1.5}},\n                                    "preconditions": ["robot_is_idle"],\n                                    "postconditions": ["robot_at_destination"],\n                                    "estimated_duration": 15.0\n                                }}\n                            ],\n                            "dependencies": [],\n                            "success_criteria": "Robot has reached the dining table"\n                        }}\n                    ],\n                    "success_criteria": "Robot has navigated to all cleaning locations"\n                }}\n            ],\n            "overall_success_criteria": "Room is clean as specified",\n            "estimated_total_time": 1800.0,\n            "risk_assessment": "Risk of collision with furniture, mitigation: use obstacle avoidance"\n        }}\n        """\n\n    def get_decomposition_system_prompt(self):\n        """System prompt for task decomposition"""\n        return """\n        You are an expert hierarchical task decomposition system for robotics. Your role is to break down complex commands into executable task hierarchies.\n\n        Requirements:\n        1. Create logical task hierarchies with clear dependencies\n        2. Include detailed preconditions and postconditions\n        3. Consider resource constraints and capabilities\n        4. Account for safety and risk mitigation\n        5. Provide realistic time estimates\n        6. Structure tasks for parallel execution where possible\n        7. Include error recovery strategies\n        8. Return only valid JSON with the specified structure\n        """\n\n    def validate_decomposition(self, decomposition_json_str):\n        """Validate task decomposition"""\n        try:\n            decomposition = json.loads(decomposition_json_str)\n\n            required_fields = [\'task_id\', \'main_tasks\', \'overall_success_criteria\']\n            if not all(field in decomposition for field in required_fields):\n                return False\n\n            # Validate main tasks structure\n            for main_task in decomposition.get(\'main_tasks\', []):\n                if \'subtasks\' not in main_task:\n                    return False\n                for subtask in main_task.get(\'subtasks\', []):\n                    if \'primitives\' not in subtask:\n                        return False\n                    for primitive in subtask.get(\'primitives\', []):\n                        if \'action\' not in primitive:\n                            return False\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception:\n            return False\n\n    def process_decomposition(self, decomposition, task_id, original_command):\n        """Process the decomposed task structure"""\n        # Store task in current tasks\n        self.current_tasks[task_id] = {\n            \'decomposition\': decomposition,\n            \'status\': \'pending\',\n            \'start_time\': time.time()\n        }\n\n        # Publish main tasks\n        main_tasks_msg = String()\n        main_tasks_msg.data = json.dumps({\n            \'task_id\': task_id,\n            \'main_tasks\': decomposition[\'main_tasks\'],\n            \'original_command\': original_command\n        })\n        self.task_pub.publish(main_tasks_msg)\n\n        # Publish subtasks for execution\n        for main_task in decomposition[\'main_tasks\']:\n            for subtask in main_task.get(\'subtasks\', []):\n                subtask_msg = String()\n                subtask_msg.data = json.dumps({\n                    \'task_id\': task_id,\n                    \'main_task_id\': main_task[\'id\'],\n                    \'subtask\': subtask\n                })\n                self.subtask_pub.publish(subtask_msg)\n\n    def update_task_status(self, task_id, status):\n        """Update status of a task"""\n        if task_id in self.current_tasks:\n            self.current_tasks[task_id][\'status\'] = status\n            self.current_tasks[task_id][\'last_update\'] = time.time()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HierarchicalTaskPlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down hierarchical task planner...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"multi-modal-reasoning",children:"Multi-Modal Reasoning"}),"\n",(0,s.jsx)(e.h3,{id:"multi-modal-cognitive-planner",children:"Multi-Modal Cognitive Planner"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport openai\nimport json\nimport threading\nfrom queue import Queue\nimport numpy as np\nimport base64\nimport io\nfrom PIL import Image as PILImage\n\nclass MultiModalCognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'multi_modal_cognitive_planner\')\n\n        # Subscribe to natural language and sensor data\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'/multi_modal_plan\', 10)\n        self.vision_pub = self.create_publisher(String, \'/vision_analysis\', 10)\n\n        # CV Bridge for image processing\n        self.bridge = CvBridge()\n\n        # LLM configuration for vision-language models\n        self.vision_model = "gpt-4-vision-preview"  # For vision tasks\n        self.text_model = "gpt-4"  # For text planning\n        self.max_tokens = 800\n        self.temperature = 0.3\n\n        # Multi-modal data storage\n        self.current_image = None\n        self.current_scan = None\n        self.last_image_time = 0\n        self.image_timeout = 5.0  # seconds\n\n        # Planning queues\n        self.planning_queue = Queue()\n        self.planning_thread = threading.Thread(target=self.planning_worker, daemon=True)\n        self.planning_thread.start()\n\n        # Robot and environment context\n        self.robot_capabilities = [\n            "move_forward", "move_backward", "turn_left", "turn_right",\n            "navigate_to", "pick_object", "place_object", "detect_object",\n            "grasp", "release", "avoid_obstacle"\n        ]\n\n        self.get_logger().info(\'Multi-Modal Cognitive Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command with multi-modal context"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received multi-modal command: "{command}"\')\n\n            # Check if we have recent image data\n            current_time = self.get_clock().now().nanoseconds / 1e9\n            has_recent_image = (current_time - self.last_image_time) < self.image_timeout\n\n            if has_recent_image and self.current_image is not None:\n                # Process with both text and image\n                self.planning_queue.put({\n                    \'command\': command,\n                    \'image\': self.current_image,\n                    \'scan\': self.current_scan,\n                    \'timestamp\': current_time\n                })\n            else:\n                # Process with text only\n                self.get_logger().warn(\'No recent image available, processing text-only command\')\n                self.planning_queue.put({\n                    \'command\': command,\n                    \'image\': None,\n                    \'scan\': self.current_scan,\n                    \'timestamp\': current_time\n                })\n\n    def image_callback(self, msg):\n        """Process camera image"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.current_image = cv_image\n            self.last_image_time = self.get_clock().now().nanoseconds / 1e9\n        except Exception as e:\n            self.get_logger().error(f\'Image callback error: {str(e)}\')\n\n    def scan_callback(self, msg):\n        """Process laser scan"""\n        self.current_scan = msg\n\n    def planning_worker(self):\n        """Worker thread for multi-modal planning"""\n        while rclpy.ok():\n            try:\n                plan_request = self.planning_queue.get(timeout=1.0)\n                self.process_multi_modal_command(plan_request)\n            except:\n                continue\n\n    def process_multi_modal_command(self, plan_request):\n        """Process command using both text and visual information"""\n        command = plan_request[\'command\']\n        image = plan_request[\'image\']\n        scan = plan_request[\'scan\']\n\n        try:\n            if image is not None:\n                # Use vision model for image analysis\n                vision_analysis = self.analyze_image_with_llm(image, command)\n\n                # Publish vision analysis\n                vision_msg = String()\n                vision_msg.data = vision_analysis\n                self.vision_pub.publish(vision_msg)\n\n                # Create comprehensive prompt with vision analysis\n                prompt = self.create_multi_modal_prompt(command, vision_analysis, scan)\n            else:\n                # Text-only processing\n                prompt = self.create_text_only_prompt(command, scan)\n\n            # Generate plan using text model\n            response = openai.ChatCompletion.create(\n                model=self.text_model,\n                messages=[\n                    {"role": "system", "content": self.get_multi_modal_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            plan_json = response.choices[0].message.content.strip()\n\n            # Validate and publish plan\n            if self.validate_multi_modal_plan(plan_json):\n                self.publish_multi_modal_plan(plan_json, command, vision_analysis if \'vision_analysis\' in locals() else None)\n                self.get_logger().info(f\'Generated multi-modal plan for: "{command}"\')\n            else:\n                self.get_logger().error(\'Invalid multi-modal plan generated\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Multi-modal planning error: {str(e)}\')\n\n    def analyze_image_with_llm(self, image, command):\n        """Analyze image using vision-capable LLM"""\n        try:\n            # Convert OpenCV image to base64\n            success, encoded_image = cv2.imencode(\'.jpg\', image)\n            if not success:\n                return None\n\n            image_base64 = base64.b64encode(encoded_image.tobytes()).decode(\'utf-8\')\n\n            # Create vision analysis prompt\n            vision_prompt = f"""\n            Analyze this image in the context of the following command: "{command}"\n\n            Describe what you see in the image that is relevant to executing this command.\n            Identify objects, obstacles, surfaces, and any other relevant visual information.\n            If the command involves finding or interacting with specific objects, note their locations and characteristics.\n            """\n\n            # Call vision model (this is a simplified example - actual implementation would use the vision API)\n            # For now, we\'ll simulate the vision analysis\n            vision_analysis = {\n                "objects_detected": ["table", "chair", "book"],\n                "obstacles": ["chair_at_x_1_y_2"],\n                "relevant_features": ["clear_path_to_table", "book_on_table"],\n                "action_suggestions": ["navigate_to_table", "pick_up_book"]\n            }\n\n            return json.dumps(vision_analysis)\n\n        except Exception as e:\n            self.get_logger().error(f\'Vision analysis error: {str(e)}\')\n            return json.dumps({"error": str(e)})\n\n    def create_multi_modal_prompt(self, command, vision_analysis, scan):\n        """Create prompt combining text command and visual analysis"""\n        scan_info = "No scan data available"\n        if scan is not None:\n            # Extract relevant information from scan\n            valid_ranges = [r for r in scan.ranges if r >= scan.range_min and r <= scan.range_max]\n            if valid_ranges:\n                min_distance = min(valid_ranges) if valid_ranges else float(\'inf\')\n                scan_info = f"Closest obstacle: {min_distance:.2f}m, Valid ranges: {len(valid_ranges)}"\n            else:\n                scan_info = "No obstacles detected in range"\n\n        return f"""\n        You are a multi-modal cognitive planner that combines natural language understanding with visual perception.\n\n        Natural language command: "{command}"\n\n        Visual analysis: {vision_analysis}\n\n        Sensor data (LIDAR): {scan_info}\n\n        Robot capabilities: {\', \'.join(self.robot_capabilities)}\n\n        Create an action plan that leverages both the linguistic command and visual information.\n        Consider how the visual scene should influence the execution of the command.\n\n        Respond with a JSON object containing:\n        - "actions": List of actions with parameters\n        - "visual_guidance": How visual information guides the plan\n        - "safety_considerations": Safety factors based on visual and sensor data\n        - "estimated_time": Time estimate considering visual scene complexity\n        """\n\n    def create_text_only_prompt(self, command, scan):\n        """Create prompt for text-only processing"""\n        scan_info = "No scan data available"\n        if scan is not None:\n            valid_ranges = [r for r in scan.ranges if r >= scan.range_min and r <= scan.range_max]\n            if valid_ranges:\n                min_distance = min(valid_ranges) if valid_ranges else float(\'inf\')\n                scan_info = f"Closest obstacle: {min_distance:.2f}m"\n            else:\n                scan_info = "No obstacles detected in range"\n\n        return f"""\n        You are a cognitive planner for a humanoid robot.\n\n        Natural language command: "{command}"\n\n        Sensor data (LIDAR): {scan_info}\n\n        Robot capabilities: {\', \'.join(self.robot_capabilities)}\n\n        Create an action plan based on the command and available sensor data.\n\n        Respond with a JSON object containing:\n        - "actions": List of actions with parameters\n        - "safety_considerations": Safety factors based on sensor data\n        - "estimated_time": Time estimate\n        """\n\n    def get_multi_modal_system_prompt(self):\n        """System prompt for multi-modal planning"""\n        return """\n        You are an expert multi-modal cognitive planner that combines language understanding with visual perception and sensor data to create robot action plans.\n\n        Guidelines:\n        1. Integrate visual information with linguistic commands\n        2. Consider sensor data for safety and navigation\n        3. Create plans that are aware of the visual scene\n        4. Use visual features to guide action execution\n        5. Account for uncertainties in perception\n        6. Return only valid JSON with the specified structure\n        """\n\n    def validate_multi_modal_plan(self, plan_json_str):\n        """Validate multi-modal plan"""\n        try:\n            plan = json.loads(plan_json_str)\n\n            required_fields = [\'actions\', \'estimated_time\']\n            if not all(field in plan for field in required_fields):\n                return False\n\n            if not isinstance(plan[\'actions\'], list):\n                return False\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception:\n            return False\n\n    def publish_multi_modal_plan(self, plan_json_str, original_command, vision_analysis):\n        """Publish multi-modal plan"""\n        plan_data = json.loads(plan_json_str)\n        plan_data[\'original_command\'] = original_command\n        plan_data[\'vision_analysis\'] = vision_analysis\n        plan_data[\'generation_time\'] = time.time()\n\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan_data)\n        self.plan_pub.publish(plan_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultiModalCognitivePlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down multi-modal cognitive planner...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    import cv2  # Import here to avoid issues if not available\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"safety-and-validation-in-llm-driven-robotics",children:"Safety and Validation in LLM-Driven Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"safe-llm-planning-with-validation",children:"Safe LLM Planning with Validation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nfrom action_msgs.msg import GoalStatus\nimport openai\nimport json\nimport threading\nfrom queue import Queue\nimport time\nimport math\n\nclass SafeLLMPlanner(Node):\n    def __init__(self):\n        super().__init__(\'safe_llm_planner\')\n\n        # Subscribe to commands and sensor data\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'/safe_plan\', 10)\n        self.validation_pub = self.create_publisher(String, \'/validation_result\', 10)\n        self.emergency_stop_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # LLM configuration\n        self.model = "gpt-4"\n        self.max_tokens = 600\n        self.temperature = 0.1  # Lower temperature for more consistent safety reasoning\n\n        # Safety parameters\n        self.safety_thresholds = {\n            \'min_obstacle_distance\': 0.5,  # meters\n            \'max_navigation_speed\': 0.3,   # m/s\n            \'max_rotation_speed\': 0.5,     # rad/s\n            \'max_plan_length\': 50          # max number of actions\n        }\n\n        # Current sensor data\n        self.current_scan = None\n        self.last_scan_time = 0\n\n        # Planning queues\n        self.planning_queue = Queue()\n        self.planning_thread = threading.Thread(target=self.planning_worker, daemon=True)\n        self.planning_thread.start()\n\n        # Safety monitoring\n        self.safety_violations = 0\n        self.max_safety_violations = 5\n\n        self.get_logger().info(\'Safe LLM Planner initialized\')\n\n    def command_callback(self, msg):\n        """Process command with safety validation"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received command for safe planning: "{command}"\')\n\n            # Check safety before planning\n            if self.is_environment_safe():\n                self.planning_queue.put({\n                    \'command\': command,\n                    \'timestamp\': time.time(),\n                    \'scan\': self.current_scan\n                })\n            else:\n                self.get_logger().error(\'Unsafe environment detected, rejecting command\')\n                self.publish_validation_result("unsafe_environment", command)\n\n    def scan_callback(self, msg):\n        """Update sensor data"""\n        self.current_scan = msg\n        self.last_scan_time = time.time()\n\n    def is_environment_safe(self):\n        """Check if environment is safe for robot operation"""\n        if self.current_scan is None:\n            return False  # No sensor data, assume unsafe\n\n        # Check for obstacles too close\n        valid_ranges = [r for r in self.current_scan.ranges\n                       if self.current_scan.range_min <= r <= self.current_scan.range_max]\n\n        if valid_ranges:\n            min_distance = min(valid_ranges)\n            if min_distance < self.safety_thresholds[\'min_obstacle_distance\']:\n                self.get_logger().warn(f\'Obstacle too close: {min_distance:.2f}m < {self.safety_thresholds["min_obstacle_distance"]}m\')\n                return False\n\n        return True\n\n    def planning_worker(self):\n        """Worker thread for safe planning"""\n        while rclpy.ok():\n            try:\n                plan_request = self.planning_queue.get(timeout=1.0)\n                self.safe_plan_with_llm(plan_request)\n            except:\n                continue\n\n    def safe_plan_with_llm(self, plan_request):\n        """Generate plan with safety considerations using LLM"""\n        command = plan_request[\'command\']\n        scan = plan_request[\'scan\']\n\n        try:\n            # Create safety-aware prompt\n            prompt = self.create_safety_aware_prompt(command, scan)\n\n            # Generate plan with LLM\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.get_safety_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            plan_json = response.choices[0].message.content.strip()\n\n            # Validate plan safety\n            if self.validate_plan_safety(plan_json, scan):\n                # Additional safety checks\n                if self.perform_safety_analysis(plan_json):\n                    self.publish_safe_plan(plan_json, command)\n                    self.get_logger().info(f\'Generated safe plan for: "{command}"\')\n                else:\n                    self.get_logger().error(\'Plan failed additional safety analysis\')\n                    self.publish_validation_result("failed_safety_analysis", command)\n            else:\n                self.get_logger().error(\'Plan failed safety validation\')\n                self.publish_validation_result("unsafe_plan", command)\n\n        except Exception as e:\n            self.get_logger().error(f\'Safe planning error: {str(e)}\')\n            self.publish_validation_result(f"planning_error: {str(e)}", command)\n\n    def create_safety_aware_prompt(self, command, scan):\n        """Create prompt that emphasizes safety"""\n        scan_info = "No scan data available"\n        if scan is not None:\n            valid_ranges = [r for r in scan.ranges if scan.range_min <= r <= scan.range_max]\n            if valid_ranges:\n                min_distance = min(valid_ranges)\n                scan_info = f"Closest obstacle: {min_distance:.2f}m, Safe threshold: {self.safety_thresholds[\'min_obstacle_distance\']}m"\n\n        return f"""\n        You are a safety-critical cognitive planner for a humanoid robot. Safety is the highest priority.\n\n        Command: "{command}"\n\n        Sensor data: {scan_info}\n\n        Safety requirements:\n        1. Maintain minimum distance of {self.safety_thresholds[\'min_obstacle_distance\']}m from obstacles\n        2. Limit navigation speed to {self.safety_thresholds[\'max_navigation_speed\']} m/s\n        3. Limit rotation speed to {self.safety_thresholds[\'max_rotation_speed\']} rad/s\n        4. Plan maximum {self.safety_thresholds[\'max_plan_length\']} actions\n        5. Include safety checks between navigation actions\n        6. Plan escape routes and safe positions\n        7. Verify each action is physically possible\n\n        Robot capabilities: move_forward, move_backward, turn_left, turn_right,\n                           navigate_to, stop, detect_obstacle, avoid_obstacle\n\n        Generate a safe action plan with detailed safety considerations.\n\n        Respond with JSON containing:\n        - "actions": List of safe actions\n        - "safety_analysis": Detailed safety analysis of the plan\n        - "risk_assessment": Potential risks and mitigation strategies\n        - "safety_confidence": Confidence level in plan safety (0-1)\n        """\n\n    def get_safety_system_prompt(self):\n        """System prompt emphasizing safety"""\n        return """\n        You are an expert safety-critical robotic planner. Your primary responsibility is to ensure all planned actions are safe for the robot and environment.\n\n        Safety requirements:\n        1. Always prioritize safety over task completion\n        2. Verify each action against physical constraints\n        3. Include safety checks and validations\n        4. Plan for contingencies and error recovery\n        5. Consider sensor limitations and uncertainties\n        6. Include escape routes and safe positions\n        7. Verify plan feasibility before returning\n        8. Return only valid JSON with the specified structure\n        """\n\n    def validate_plan_safety(self, plan_json_str, scan):\n        """Validate plan safety against constraints"""\n        try:\n            plan = json.loads(plan_json_str)\n\n            # Check required fields\n            required_fields = [\'actions\', \'safety_analysis\', \'safety_confidence\']\n            if not all(field in plan for field in required_fields):\n                return False\n\n            # Validate safety confidence\n            safety_confidence = plan.get(\'safety_confidence\', 0)\n            if safety_confidence < 0.7:  # Require high safety confidence\n                self.get_logger().warn(f\'Low safety confidence: {safety_confidence}\')\n                return False\n\n            # Validate action count\n            actions = plan.get(\'actions\', [])\n            if len(actions) > self.safety_thresholds[\'max_plan_length\']:\n                self.get_logger().warn(f\'Too many actions: {len(actions)} > {self.safety_thresholds["max_plan_length"]}\')\n                return False\n\n            # Check for safe navigation parameters\n            for action in actions:\n                if action.get(\'action\') in [\'move_forward\', \'move_backward\', \'navigate_to\']:\n                    # Check if parameters are safe (simplified check)\n                    params = action.get(\'parameters\', {})\n                    speed = params.get(\'speed\', 0.5)\n                    if speed > self.safety_thresholds[\'max_navigation_speed\']:\n                        self.get_logger().warn(f\'Unsafe speed: {speed} > {self.safety_thresholds["max_navigation_speed"]}\')\n                        return False\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception as e:\n            self.get_logger().error(f\'Plan safety validation error: {str(e)}\')\n            return False\n\n    def perform_safety_analysis(self, plan_json_str):\n        """Perform additional safety analysis on the plan"""\n        try:\n            plan = json.loads(plan_json_str)\n            actions = plan.get(\'actions\', [])\n\n            # Analyze each action for safety\n            for i, action in enumerate(actions):\n                action_type = action.get(\'action\')\n\n                if action_type in [\'navigate_to\', \'move_to\']:\n                    # Check if navigation target is safe based on scan data\n                    if not self.is_navigation_target_safe(action):\n                        self.get_logger().warn(f\'Unsafe navigation target in action {i}\')\n                        return False\n\n                elif action_type in [\'move_forward\', \'move_backward\']:\n                    # Check if movement direction is safe\n                    if not self.is_movement_safe(action):\n                        self.get_logger().warn(f\'Unsafe movement in action {i}\')\n                        return False\n\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f\'Safety analysis error: {str(e)}\')\n            return False\n\n    def is_navigation_target_safe(self, action):\n        """Check if navigation target is safe"""\n        # In a real system, this would check the target location against map/scan data\n        # For simulation, assume safe if we have valid coordinates\n        params = action.get(\'parameters\', {})\n        x = params.get(\'x\')\n        y = params.get(\'y\')\n\n        return x is not None and y is not None and abs(x) < 100 and abs(y) < 100\n\n    def is_movement_safe(self, action):\n        """Check if movement action is safe"""\n        # In a real system, this would check movement direction against scan data\n        # For simulation, assume safe\n        return True\n\n    def publish_safe_plan(self, plan_json_str, original_command):\n        """Publish validated safe plan"""\n        plan_data = json.loads(plan_json_str)\n        plan_data[\'original_command\'] = original_command\n        plan_data[\'generation_time\'] = time.time()\n        plan_data[\'validated_safe\'] = True\n\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan_data)\n        self.plan_pub.publish(plan_msg)\n\n        # Publish validation success\n        validation_msg = String()\n        validation_msg.data = f"success: {original_command}"\n        self.validation_pub.publish(validation_msg)\n\n    def publish_validation_result(self, result, command):\n        """Publish validation result"""\n        validation_msg = String()\n        validation_msg.data = f"{result}: {command}"\n        self.validation_pub.publish(validation_msg)\n\n        # Increment safety violations counter\n        self.safety_violations += 1\n\n        if self.safety_violations >= self.max_safety_violations:\n            self.get_logger().error(\'Too many safety violations, emergency stop\')\n            self.emergency_stop()\n            self.safety_violations = 0  # Reset counter after emergency stop\n\n    def emergency_stop(self):\n        """Publish emergency stop command"""\n        stop_cmd = Twist()\n        stop_cmd.linear.x = 0.0\n        stop_cmd.angular.z = 0.0\n        self.emergency_stop_pub.publish(stop_cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SafeLLMPlanner()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down safe LLM planner...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-lab-complete-cognitive-planning-system",children:"Hands-on Lab: Complete Cognitive Planning System"}),"\n",(0,s.jsx)(e.p,{children:"In this lab, you'll create a complete cognitive planning system that integrates all components."}),"\n",(0,s.jsx)(e.h3,{id:"step-1-create-the-cognitive-planning-system-launch-file",children:"Step 1: Create the Cognitive Planning System Launch File"}),"\n",(0,s.jsxs)(e.p,{children:["Create ",(0,s.jsx)(e.code,{children:"cognitive_planning_system_launch.py"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Cognitive planning system nodes\n    llm_planner = Node(\n        package='ai_robo_learning',\n        executable='llm_cognitive_planner',\n        name='llm_cognitive_planner',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    context_planner = Node(\n        package='ai_robo_learning',\n        executable='context_aware_planner',\n        name='context_aware_planner',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    hierarchical_planner = Node(\n        package='ai_robo_learning',\n        executable='hierarchical_task_planner',\n        name='hierarchical_task_planner',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    multimodal_planner = Node(\n        package='ai_robo_learning',\n        executable='multi_modal_cognitive_planner',\n        name='multi_modal_cognitive_planner',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    safe_planner = Node(\n        package='ai_robo_learning',\n        executable='safe_llm_planner',\n        name='safe_llm_planner',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add all nodes\n    ld.add_action(llm_planner)\n    ld.add_action(context_planner)\n    ld.add_action(hierarchical_planner)\n    ld.add_action(multimodal_planner)\n    ld.add_action(safe_planner)\n\n    return ld\n"})}),"\n",(0,s.jsx)(e.h3,{id:"step-2-create-the-complete-cognitive-planning-node",children:"Step 2: Create the Complete Cognitive Planning Node"}),"\n",(0,s.jsxs)(e.p,{children:["Create ",(0,s.jsx)(e.code,{children:"complete_cognitive_planning_system.py"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan, Image\nfrom action_msgs.msg import GoalStatus\nimport openai\nimport json\nimport threading\nfrom queue import Queue\nimport time\nfrom typing import Dict, List, Any, Optional\n\nclass CompleteCognitivePlanningSystem(Node):\n    def __init__(self):\n        super().__init__(\'complete_cognitive_planning_system\')\n\n        # Subscribe to inputs\n        self.command_sub = self.create_subscription(\n            String,\n            \'/natural_language_command\',\n            self.command_callback,\n            10\n        )\n\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'/complete_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'/cognitive_status\', 10)\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # LLM configuration\n        self.model = "gpt-4"\n        self.max_tokens = 1000\n        self.temperature = 0.2\n\n        # System components\n        self.current_scan = None\n        self.last_scan_time = 0\n        self.command_queue = Queue()\n\n        # Robot state and capabilities\n        self.robot_state = {\n            \'position\': [0.0, 0.0, 0.0],\n            \'battery\': 100.0,\n            \'status\': \'idle\'\n        }\n\n        self.robot_capabilities = {\n            \'navigation\': {\n                \'max_speed\': 0.5,\n                \'min_obstacle_distance\': 0.5\n            },\n            \'manipulation\': {\n                \'max_weight\': 2.0,\n                \'reach_distance\': 1.0\n            }\n        }\n\n        # Planning threads\n        self.planning_thread = threading.Thread(target=self.planning_worker, daemon=True)\n        self.planning_thread.start()\n\n        # System monitoring\n        self.system_monitor_timer = self.create_timer(5.0, self.system_monitor)\n\n        # Safety parameters\n        self.safety_thresholds = {\n            \'min_scan_age\': 5.0,  # seconds\n            \'max_battery_usage\': 20.0  # percent per task\n        }\n\n        self.get_logger().info(\'Complete Cognitive Planning System initialized\')\n\n    def command_callback(self, msg):\n        """Process natural language command"""\n        command = msg.data.strip()\n        if command:\n            self.get_logger().info(f\'Received cognitive command: "{command}"\')\n\n            # Check if we have recent sensor data\n            current_time = time.time()\n            scan_age = current_time - self.last_scan_time if self.last_scan_time > 0 else float(\'inf\')\n\n            if scan_age > self.safety_thresholds[\'min_scan_age\']:\n                self.get_logger().warn(\'Sensor data too old, waiting for fresh data\')\n                return\n\n            # Add to planning queue\n            self.command_queue.put({\n                \'command\': command,\n                \'timestamp\': current_time,\n                \'robot_state\': self.robot_state.copy(),\n                \'scan\': self.current_scan\n            })\n\n    def scan_callback(self, msg):\n        """Update sensor data"""\n        self.current_scan = msg\n        self.last_scan_time = time.time()\n\n    def planning_worker(self):\n        """Worker thread for cognitive planning"""\n        while rclpy.ok():\n            try:\n                plan_request = self.command_queue.get(timeout=1.0)\n                self.generate_cognitive_plan(plan_request)\n            except:\n                continue\n\n    def generate_cognitive_plan(self, plan_request):\n        """Generate complete cognitive plan using LLM"""\n        command = plan_request[\'command\']\n        robot_state = plan_request[\'robot_state\']\n        scan = plan_request[\'scan\']\n\n        try:\n            # Create comprehensive prompt\n            prompt = self.create_comprehensive_prompt(command, robot_state, scan)\n\n            # Call LLM\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.get_comprehensive_system_prompt()},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=self.temperature\n            )\n\n            plan_json = response.choices[0].message.content.strip()\n\n            # Validate plan\n            if self.validate_cognitive_plan(plan_json):\n                # Publish plan\n                self.publish_cognitive_plan(plan_json, command, robot_state)\n\n                # Update robot state\n                self.robot_state[\'status\'] = \'planning\'\n\n                self.get_logger().info(f\'Generated cognitive plan for: "{command}"\')\n            else:\n                self.get_logger().error(\'Invalid cognitive plan generated\')\n                self.publish_error_status(f"Invalid plan for command: {command}")\n\n        except Exception as e:\n            self.get_logger().error(f\'Cognitive planning error: {str(e)}\')\n            self.publish_error_status(f"Planning error: {str(e)}")\n\n    def create_comprehensive_prompt(self, command, robot_state, scan):\n        """Create comprehensive prompt for cognitive planning"""\n        scan_info = "No scan data available"\n        if scan is not None:\n            valid_ranges = [r for r in scan.ranges if scan.range_min <= r <= scan.range_max]\n            if valid_ranges:\n                min_distance = min(valid_ranges)\n                avg_distance = sum(valid_ranges) / len(valid_ranges)\n                scan_info = f"Min obstacle distance: {min_distance:.2f}m, Avg: {avg_distance:.2f}m, Valid readings: {len(valid_ranges)}"\n\n        return f"""\n        You are an advanced cognitive planning system for a humanoid robot. Generate a comprehensive plan that considers all aspects of the task.\n\n        Command: "{command}"\n\n        Robot state: {json.dumps(robot_state, indent=2)}\n        Robot capabilities: {json.dumps(self.robot_capabilities, indent=2)}\n        Sensor data: {scan_info}\n\n        Create a detailed cognitive plan that includes:\n        1. High-level task decomposition\n        2. Environmental analysis and obstacle considerations\n        3. Safety validation and risk assessment\n        4. Resource management (battery, time, etc.)\n        5. Action sequencing with dependencies\n        6. Contingency planning for failures\n        7. Success criteria and validation steps\n\n        Consider the robot\'s current state and capabilities when creating the plan.\n\n        Respond with a JSON object containing:\n        - "task_decomposition": Breakdown of main tasks\n        - "environmental_analysis": Analysis of current environment\n        - "safety_assessment": Safety considerations and validations\n        - "action_sequence": Ordered list of actions to execute\n        - "resource_plan": Battery and time usage estimates\n        - "contingency_plans": Backup plans for common failures\n        - "success_criteria": How to verify task completion\n        - "estimated_completion_time": Time estimate in seconds\n        - "confidence_level": Confidence in plan success (0-1)\n        """\n\n    def get_comprehensive_system_prompt(self):\n        """System prompt for comprehensive cognitive planning"""\n        return """\n        You are an expert cognitive planning system for humanoid robots. Create comprehensive, safe, and executable plans that consider all aspects of task execution.\n\n        Requirements:\n        1. Decompose complex tasks into manageable subtasks\n        2. Consider environmental constraints and obstacles\n        3. Validate safety at each step\n        4. Manage robot resources efficiently\n        5. Plan for contingencies and error recovery\n        6. Sequence actions logically with proper dependencies\n        7. Provide realistic time and resource estimates\n        8. Return only valid JSON with the specified structure\n        """\n\n    def validate_cognitive_plan(self, plan_json_str):\n        """Validate cognitive plan"""\n        try:\n            plan = json.loads(plan_json_str)\n\n            required_fields = [\n                \'task_decomposition\', \'action_sequence\',\n                \'success_criteria\', \'confidence_level\'\n            ]\n            if not all(field in plan for field in required_fields):\n                return False\n\n            if not isinstance(plan[\'action_sequence\'], list):\n                return False\n\n            if not (0 <= plan.get(\'confidence_level\', 0) <= 1):\n                return False\n\n            # Validate action sequence\n            for action in plan[\'action_sequence\']:\n                if not isinstance(action, dict) or \'action\' not in action:\n                    return False\n\n            return True\n\n        except json.JSONDecodeError:\n            return False\n        except Exception:\n            return False\n\n    def publish_cognitive_plan(self, plan_json_str, original_command, robot_state):\n        """Publish cognitive plan"""\n        plan_data = json.loads(plan_json_str)\n        plan_data[\'original_command\'] = original_command\n        plan_data[\'robot_state_at_generation\'] = robot_state\n        plan_data[\'generation_time\'] = time.time()\n        plan_data[\'system_version\'] = \'1.0\'\n\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan_data)\n        self.plan_pub.publish(plan_msg)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"plan_generated: {original_command}"\n        self.status_pub.publish(status_msg)\n\n    def publish_error_status(self, error_msg):\n        """Publish error status"""\n        status_msg = String()\n        status_msg.data = f"error: {error_msg}"\n        self.status_pub.publish(status_msg)\n\n    def system_monitor(self):\n        """Monitor system status"""\n        status_msg = String()\n        status_msg.data = f"active: {len(self.command_queue.queue)} commands pending"\n        self.status_pub.publish(status_msg)\n\n        self.get_logger().info(f\'System status - Commands pending: {len(self.command_queue.queue)}, Robot state: {self.robot_state["status"]}\')\n\n    def destroy_node(self):\n        """Clean up resources"""\n        # Stop planning thread gracefully\n        # In a real system, you might want to implement proper shutdown\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CompleteCognitivePlanningSystem()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print("Shutting down complete cognitive planning system...")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    # Set your OpenAI API key here or via environment variable\n    # openai.api_key = "your-api-key-here"\n    main()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"step-3-test-the-cognitive-planning-system",children:"Step 3: Test the Cognitive Planning System"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Make sure you have the required dependencies:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"pip3 install openai\n# For multi-modal (optional):\npip3 install pillow opencv-python\n# For NLP (optional):\npip3 install numpy\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"2",children:["\n",(0,s.jsx)(e.li,{children:"Set your OpenAI API key:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'export OPENAI_API_KEY="your-api-key-here"\n'})}),"\n",(0,s.jsxs)(e.ol,{start:"3",children:["\n",(0,s.jsx)(e.li,{children:"Run the complete cognitive planning system:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"python3 complete_cognitive_planning_system.py\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"4",children:["\n",(0,s.jsx)(e.li,{children:"Send test commands:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Send a natural language command\nros2 topic pub /natural_language_command std_msgs/String \"data: 'Navigate to the kitchen and bring me a cup'\"\n"})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety First"}),": Always validate plans for safety before execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Context Awareness"}),": Use environmental and sensor data to inform planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Decomposition"}),": Break complex commands into manageable subtasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Error Handling"}),": Plan for contingencies and error recovery"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Resource Management"}),": Consider battery, time, and computational constraints"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Validation"}),": Verify plan feasibility and safety before execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-Modal Integration"}),": Combine text, vision, and sensor data for better planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Continuous Monitoring"}),": Monitor plan execution and adapt as needed"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, you'll be ready to learn about computer vision for robotics in Chapter 4, where you'll explore how robots can perceive and understand their visual environment."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(m,{...n})}):m(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>r});var t=a(6540);const s={},i=t.createContext(s);function o(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);