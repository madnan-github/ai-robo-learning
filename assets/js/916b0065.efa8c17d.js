"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[210],{3953:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2/chapter-4/sensor-simulation","title":"Sensor Simulation","description":"This chapter covers the simulation of various sensors in Gazebo, including LiDAR, cameras, depth sensors, and IMUs, which are essential for humanoid robot perception and navigation.","source":"@site/docs/module-2/chapter-4/sensor-simulation.md","sourceDirName":"module-2/chapter-4","slug":"/module-2/chapter-4/sensor-simulation","permalink":"/ai-robo-learning/docs/module-2/chapter-4/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-2/chapter-4/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Sensor Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Creating Robot Models in Gazebo","permalink":"/ai-robo-learning/docs/module-2/chapter-3/robot-models-gazebo"},"next":{"title":"Unity Integration","permalink":"/ai-robo-learning/docs/module-2/chapter-5/unity-integration"}}');var s=i(4848),r=i(8453);const o={sidebar_position:4,title:"Sensor Simulation"},t="Sensor Simulation",l={},d=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Understanding LiDAR in Simulation",id:"understanding-lidar-in-simulation",level:3},{value:"2D LiDAR Configuration",id:"2d-lidar-configuration",level:3},{value:"3D LiDAR Configuration",id:"3d-lidar-configuration",level:3},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"RGB Camera Configuration",id:"rgb-camera-configuration",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Stereo Camera Configuration",id:"stereo-camera-configuration",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"Processing Sensor Data in ROS 2",id:"processing-sensor-data-in-ros-2",level:2},{value:"LiDAR Data Processing Node",id:"lidar-data-processing-node",level:3},{value:"Camera Data Processing Node",id:"camera-data-processing-node",level:3},{value:"Hands-on Lab: Multi-Sensor Humanoid Robot",id:"hands-on-lab-multi-sensor-humanoid-robot",level:2},{value:"Step 1: Create a Multi-Sensor Robot URDF",id:"step-1-create-a-multi-sensor-robot-urdf",level:3},{value:"Step 2: Create a Sensor Fusion Node",id:"step-2-create-a-sensor-fusion-node",level:3},{value:"Step 3: Test the Multi-Sensor Robot",id:"step-3-test-the-multi-sensor-robot",level:3},{value:"Sensor Calibration and Validation",id:"sensor-calibration-and-validation",level:2},{value:"Validating Sensor Data",id:"validating-sensor-data",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"sensor-simulation",children:"Sensor Simulation"})}),"\n",(0,s.jsx)(e.p,{children:"This chapter covers the simulation of various sensors in Gazebo, including LiDAR, cameras, depth sensors, and IMUs, which are essential for humanoid robot perception and navigation."}),"\n",(0,s.jsx)(e.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsx)(e.p,{children:"In this chapter, you'll explore:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"LiDAR simulation for environment mapping"}),"\n",(0,s.jsx)(e.li,{children:"Camera and depth camera simulation"}),"\n",(0,s.jsx)(e.li,{children:"IMU and inertial sensor simulation"}),"\n",(0,s.jsx)(e.li,{children:"Sensor fusion concepts in simulation"}),"\n",(0,s.jsx)(e.li,{children:"Troubleshooting sensor data issues"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Completion of Module 1 and 2, Chapters 1-3"}),"\n",(0,s.jsx)(e.li,{children:"Gazebo Harmonic installed"}),"\n",(0,s.jsx)(e.li,{children:"Basic understanding of sensor principles"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 Humble installed"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"understanding-lidar-in-simulation",children:"Understanding LiDAR in Simulation"}),"\n",(0,s.jsx)(e.p,{children:"LiDAR (Light Detection and Ranging) sensors provide 2D or 3D distance measurements. In Gazebo, LiDAR sensors can be either CPU-based or GPU-based:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CPU-based"}),": More accurate but slower"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPU-based"}),": Faster but may have limitations with transparency"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2d-lidar-configuration",children:"2D LiDAR Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<link name="laser_link">\n  <inertial>\n    <mass value="0.1"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n  <visual>\n    <geometry>\n      <cylinder length="0.05" radius="0.02"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <cylinder length="0.05" radius="0.02"/>\n    </geometry>\n  </collision>\n</link>\n\n<joint name="laser_joint" type="fixed">\n  <parent link="base_link"/>\n  <child link="laser_link"/>\n  <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="laser_link">\n  <sensor name="laser_scan" type="gpu_lidar">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <visualize>false</visualize>\n    <topic>scan</topic>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\n          <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>\n    </noise>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"3d-lidar-configuration",children:"3D LiDAR Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="lidar_3d_link">\n  <sensor name="lidar_3d" type="gpu_lidar">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <visualize>false</visualize>\n    <topic>points2</topic>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>640</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle> \x3c!-- -180 degrees --\x3e\n          <max_angle>3.14159</max_angle>   \x3c!-- 180 degrees --\x3e\n        </horizontal>\n        <vertical>\n          <samples>16</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.2618</min_angle> \x3c!-- -15 degrees --\x3e\n          <max_angle>0.2618</max_angle>   \x3c!-- 15 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>100.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.02</stddev>\n    </noise>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"rgb-camera-configuration",children:"RGB Camera Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<link name="camera_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n  <visual>\n    <geometry>\n      <box size="0.02 0.04 0.02"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.02 0.04 0.02"/>\n    </geometry>\n  </collision>\n</link>\n\n<joint name="camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="camera_link"/>\n  <origin xyz="0.05 0 0" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="camera_link">\n  <sensor name="camera" type="camera">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <visualize>true</visualize>\n    <topic>camera/image_raw</topic>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees in radians --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="depth_camera" type="depth_camera">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <visualize>true</visualize>\n    <topic>camera/depth/image_raw</topic>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees in radians --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10</far>\n      </clip>\n    </camera>\n    <output_type>depths</output_type>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"stereo-camera-configuration",children:"Stereo Camera Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Left camera --\x3e\n<link name="left_camera_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n  <visual>\n    <geometry>\n      <box size="0.01 0.02 0.01"/>\n    </geometry>\n  </visual>\n</link>\n\n<joint name="left_camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="left_camera_link"/>\n  <origin xyz="0.06 0.05 0" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Right camera --\x3e\n<link name="right_camera_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n  <visual>\n    <geometry>\n      <box size="0.01 0.02 0.01"/>\n    </geometry>\n  </visual>\n</link>\n\n<joint name="right_camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="right_camera_link"/>\n  <origin xyz="0.06 -0.05 0" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="left_camera_link">\n  <sensor name="left_camera" type="camera">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <visualize>false</visualize>\n    <topic>stereo/left/image_raw</topic>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n  </sensor>\n</gazebo>\n\n<gazebo reference="right_camera_link">\n  <sensor name="right_camera" type="camera">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <visualize>false</visualize>\n    <topic>stereo/right/image_raw</topic>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<link name="imu_link">\n  <inertial>\n    <mass value="0.01"/>\n    <origin xyz="0 0 0"/>\n    <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n  </inertial>\n  <visual>\n    <geometry>\n      <box size="0.01 0.01 0.01"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.01 0.01 0.01"/>\n    </geometry>\n  </collision>\n</link>\n\n<joint name="imu_joint" type="fixed">\n  <parent link="torso"/>\n  <child link="imu_link"/>\n  <origin xyz="0 0 0.1" rpy="0 0 0"/>\n</joint>\n\n<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <topic>imu/data</topic>\n    <visualize>false</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"processing-sensor-data-in-ros-2",children:"Processing Sensor Data in ROS 2"}),"\n",(0,s.jsx)(e.h3,{id:"lidar-data-processing-node",children:"LiDAR Data Processing Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom std_msgs.msg import String\nimport numpy as np\n\nclass LidarProcessor(Node):\n\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        # Subscribe to laser scan data\n        self.subscription = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.lidar_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n\n        # Publisher for processed data\n        self.obstacle_publisher = self.create_publisher(\n            String,\n            'obstacle_detection',\n            10)\n\n        self.get_logger().info('LiDAR Processor initialized')\n\n    def lidar_callback(self, msg):\n        # Convert ranges to numpy array\n        ranges = np.array(msg.ranges)\n\n        # Handle invalid readings\n        ranges[np.isnan(ranges)] = msg.range_max\n        ranges[np.isinf(ranges)] = msg.range_max\n\n        # Detect obstacles within 1 meter\n        obstacle_ranges = ranges[ranges < 1.0]\n        obstacle_angle_indices = np.where(ranges < 1.0)[0]\n\n        if len(obstacle_ranges) > 0:\n            # Calculate angles for obstacles\n            angle_min = msg.angle_min\n            angle_increment = msg.angle_increment\n            obstacle_angles = angle_min + obstacle_angle_indices * angle_increment\n\n            # Find closest obstacle\n            min_distance_idx = np.argmin(obstacle_ranges)\n            closest_distance = obstacle_ranges[min_distance_idx]\n            closest_angle = obstacle_angles[min_distance_idx]\n\n            # Publish obstacle information\n            obstacle_msg = String()\n            obstacle_msg.data = f'Obstacle at {closest_angle:.2f} rad, distance: {closest_distance:.2f}m'\n            self.obstacle_publisher.publish(obstacle_msg)\n\n            self.get_logger().info(f'Obstacle detected: {obstacle_msg.data}')\n        else:\n            self.get_logger().info('No obstacles detected')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    lidar_processor = LidarProcessor()\n\n    try:\n        rclpy.spin(lidar_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lidar_processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"camera-data-processing-node",children:"Camera Data Processing Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass CameraProcessor(Node):\n\n    def __init__(self):\n        super().__init__('camera_processor')\n\n        # Create CV bridge to convert ROS images to OpenCV\n        self.bridge = CvBridge()\n\n        # Subscribe to camera image\n        self.subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n\n        self.get_logger().info('Camera Processor initialized')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Perform basic image processing\n            # Example: Detect edges using Canny edge detection\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n\n            # Find contours\n            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            # Draw contours on the original image\n            contour_image = cv_image.copy()\n            cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n\n            # Display the processed image\n            cv2.imshow(\"Camera Feed with Contours\", contour_image)\n            cv2.waitKey(1)  # Refresh the display\n\n            # Log information about detected contours\n            self.get_logger().info(f'Detected {len(contours)} contours')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    camera_processor = CameraProcessor()\n\n    try:\n        rclpy.spin(camera_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        camera_processor.destroy_node()\n        cv2.destroyAllWindows()  # Close OpenCV windows\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-lab-multi-sensor-humanoid-robot",children:"Hands-on Lab: Multi-Sensor Humanoid Robot"}),"\n",(0,s.jsx)(e.p,{children:"In this lab, you'll create a humanoid robot with multiple sensors and process the sensor data."}),"\n",(0,s.jsx)(e.h3,{id:"step-1-create-a-multi-sensor-robot-urdf",children:"Step 1: Create a Multi-Sensor Robot URDF"}),"\n",(0,s.jsxs)(e.p,{children:["Create ",(0,s.jsx)(e.code,{children:"multi_sensor_humanoid.urdf"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="multi_sensor_humanoid">\n\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.15 0.15 0.1"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 0.8 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <box size="0.15 0.15 0.1"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="3.0"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.03"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Torso --\x3e\n  <link name="torso">\n    <visual>\n      <geometry>\n        <box size="0.2 0.15 0.4"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1 1 1 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <box size="0.2 0.15 0.4"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="5.0"/>\n      <origin xyz="0 0 0.2"/>\n      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.05"/>\n    </inertial>\n  </link>\n\n  <joint name="torso_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="torso"/>\n    <origin xyz="0 0 0.15"/>\n  </joint>\n\n  \x3c!-- Head --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.08"/>\n      </geometry>\n      <material name="skin">\n        <color rgba="1 0.8 0.6 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <sphere radius="0.08"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="1.0"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.002" ixy="0.0" ixz="0.0" iyy="0.002" iyz="0.0" izz="0.002"/>\n    </inertial>\n  </link>\n\n  <joint name="neck_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.35"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-0.5" upper="0.5" effort="10" velocity="1"/>\n  </joint>\n\n  \x3c!-- LiDAR sensor --\x3e\n  <link name="lidar_link">\n    <inertial>\n      <mass value="0.1"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n    </inertial>\n    <visual>\n      <geometry>\n        <cylinder length="0.05" radius="0.02"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.05" radius="0.02"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Camera sensor --\x3e\n  <link name="camera_link">\n    <inertial>\n      <mass value="0.01"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n    </inertial>\n    <visual>\n      <geometry>\n        <box size="0.02 0.04 0.02"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.02 0.04 0.02"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="head"/>\n    <child link="camera_link"/>\n    <origin xyz="0.05 0 0" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- IMU sensor --\x3e\n  <link name="imu_link">\n    <inertial>\n      <mass value="0.01"/>\n      <origin xyz="0 0 0"/>\n      <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n    </inertial>\n    <visual>\n      <geometry>\n        <box size="0.01 0.01 0.01"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.01 0.01 0.01"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="imu_joint" type="fixed">\n    <parent link="torso"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins and sensors --\x3e\n  <gazebo>\n    \x3c!-- Joint state publisher --\x3e\n    <plugin filename="gz-sim-joint-state-publisher-system" name="gz::sim::systems::JointStatePublisher">\n      <joint_name>neck_joint</joint_name>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- LiDAR sensor --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="laser_scan" type="gpu_lidar">\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <visualize>false</visualize>\n      <topic>scan</topic>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-1.570796</min_angle>\n            <max_angle>1.570796</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Camera sensor --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <visualize>false</visualize>\n      <topic>camera/image_raw</topic>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>100</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- IMU sensor --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <topic>imu/data</topic>\n      <visualize>false</visualize>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Gazebo materials --\x3e\n  <gazebo reference="base_link">\n    <material>Gazebo/Blue</material>\n  </gazebo>\n\n  <gazebo reference="torso">\n    <material>Gazebo/White</material>\n  </gazebo>\n\n  <gazebo reference="head">\n    <material>Gazebo/Yellow</material>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"step-2-create-a-sensor-fusion-node",children:"Step 2: Create a Sensor Fusion Node"}),"\n",(0,s.jsxs)(e.p,{children:["Create ",(0,s.jsx)(e.code,{children:"sensor_fusion_node.py"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nimport time\n\nclass SensorFusionNode(Node):\n\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to sensors\n        self.lidar_subscription = self.create_subscription(\n            LaserScan, 'scan', self.lidar_callback, 10)\n        self.camera_subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.camera_callback, 10)\n        self.imu_subscription = self.create_subscription(\n            Imu, 'imu/data', self.imu_callback, 10)\n\n        # Publisher for robot commands\n        self.cmd_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n\n        # Store sensor data\n        self.lidar_data = None\n        self.camera_data = None\n        self.imu_data = None\n\n        # Lock for thread safety\n        self.data_lock = threading.Lock()\n\n        # Timer for decision making\n        self.timer = self.create_timer(0.1, self.fusion_callback)\n\n        self.get_logger().info('Sensor Fusion Node initialized')\n\n    def lidar_callback(self, msg):\n        with self.data_lock:\n            self.lidar_data = msg\n\n    def camera_callback(self, msg):\n        with self.data_lock:\n            try:\n                self.camera_data = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            except Exception as e:\n                self.get_logger().error(f'Error converting image: {str(e)}')\n\n    def imu_callback(self, msg):\n        with self.data_lock:\n            self.imu_data = msg\n\n    def fusion_callback(self):\n        with self.data_lock:\n            # Check if we have all sensor data\n            if self.lidar_data is None or self.imu_data is None:\n                return\n\n            # Process LiDAR data for obstacle detection\n            ranges = np.array(self.lidar_data.ranges)\n            ranges[np.isnan(ranges)] = self.lidar_data.range_max\n            ranges[np.isinf(ranges)] = self.lidar_data.range_max\n\n            # Detect obstacles in front\n            front_ranges = ranges[len(ranges)//2-30:len(ranges)//2+30]\n            min_front_distance = np.min(front_ranges)\n\n            # Process IMU data for orientation\n            orientation_z = self.imu_data.orientation.z\n\n            # Create robot command based on sensor fusion\n            cmd_vel = Twist()\n\n            if min_front_distance < 0.8:  # Obstacle detected\n                # Turn away from obstacle\n                cmd_vel.linear.x = 0.2\n                cmd_vel.angular.z = 0.5 if orientation_z > 0 else -0.5\n                self.get_logger().info(f'Obstacle detected! Distance: {min_front_distance:.2f}m, turning')\n            else:\n                # Move forward\n                cmd_vel.linear.x = 0.5\n                cmd_vel.angular.z = 0.0\n                self.get_logger().info(f'Clear path, moving forward. Distance: {min_front_distance:.2f}m')\n\n        # Publish the command\n        self.cmd_publisher.publish(cmd_vel)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"step-3-test-the-multi-sensor-robot",children:"Step 3: Test the Multi-Sensor Robot"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Launch Gazebo with your world:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"gz sim -r your_world.sdf\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"2",children:["\n",(0,s.jsx)(e.li,{children:"Spawn your robot:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"ros2 run gazebo_ros spawn_entity.py -entity multi_sensor_humanoid -file multi_sensor_humanoid.urdf -x 0 -y 0 -z 1\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"3",children:["\n",(0,s.jsx)(e.li,{children:"Run the sensor fusion node:"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"python3 sensor_fusion_node.py\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-calibration-and-validation",children:"Sensor Calibration and Validation"}),"\n",(0,s.jsx)(e.h3,{id:"validating-sensor-data",children:"Validating Sensor Data"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorValidator(Node):\n\n    def __init__(self):\n        super().__init__('sensor_validator')\n\n        self.bridge = CvBridge()\n\n        # Subscribe to all sensors\n        self.lidar_subscription = self.create_subscription(\n            LaserScan, 'scan', self.validate_lidar, 10)\n        self.camera_subscription = self.create_subscription(\n            Image, 'camera/image_raw', self.validate_camera, 10)\n        self.imu_subscription = self.create_subscription(\n            Imu, 'imu/data', self.validate_imu, 10)\n\n    def validate_lidar(self, msg):\n        # Check for valid ranges\n        valid_ranges = [r for r in msg.ranges if not (np.isnan(r) or np.isinf(r))]\n\n        # Log validation results\n        if len(valid_ranges) == 0:\n            self.get_logger().warn('LiDAR: No valid ranges detected')\n        elif min(valid_ranges) < msg.range_min:\n            self.get_logger().warn(f'LiDAR: Range below minimum: {min(valid_ranges)} < {msg.range_min}')\n        elif max(valid_ranges) > msg.range_max:\n            self.get_logger().warn(f'LiDAR: Range above maximum: {max(valid_ranges)} > {msg.range_max}')\n        else:\n            self.get_logger().info(f'LiDAR: OK - {len(valid_ranges)} valid ranges, min: {min(valid_ranges):.2f}, max: {max(valid_ranges):.2f}')\n\n    def validate_camera(self, msg):\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            height, width = cv_image.shape[:2]\n\n            if height != msg.height or width != msg.width:\n                self.get_logger().warn(f'Camera: Image size mismatch - expected {msg.width}x{msg.height}, got {width}x{height}')\n            else:\n                self.get_logger().info(f'Camera: OK - {width}x{height} image')\n        except Exception as e:\n            self.get_logger().error(f'Camera: Error processing image - {str(e)}')\n\n    def validate_imu(self, msg):\n        # Check if orientation values are reasonable\n        orientation_valid = (\n            -1 <= msg.orientation.x <= 1 and\n            -1 <= msg.orientation.y <= 1 and\n            -1 <= msg.orientation.z <= 1 and\n            -1 <= msg.orientation.w <= 1\n        )\n\n        if not orientation_valid:\n            self.get_logger().warn('IMU: Orientation values out of range')\n        else:\n            self.get_logger().info('IMU: OK - orientation values valid')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SensorValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Noise Modeling"}),": Include realistic noise in sensor simulations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Update Rates"}),": Match simulation update rates to real sensor capabilities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visualization"}),": Use Gazebo's visualization tools to verify sensor behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Data Validation"}),": Always validate sensor data before processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor Placement"}),": Place sensors where they would be on a real robot"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Performance"}),": Balance sensor fidelity with simulation performance"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, you'll be ready to learn about Unity integration for advanced simulation in Chapter 5."})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var a=i(6540);const s={},r=a.createContext(s);function o(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);