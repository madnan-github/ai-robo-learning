"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[829],{5487:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3/chapter-6/perception-systems","title":"Perception Systems","description":"This chapter covers advanced perception systems for humanoid robots, including computer vision, depth sensing, object recognition, and environment understanding that enable autonomous navigation and interaction.","source":"@site/docs/module-3/chapter-6/perception-systems.md","sourceDirName":"module-3/chapter-6","slug":"/module-3/chapter-6/perception-systems","permalink":"/ai-robo-learning/docs/module-3/chapter-6/perception-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-3/chapter-6/perception-systems.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Perception Systems"},"sidebar":"tutorialSidebar","previous":{"title":"Path Planning for Humanoids","permalink":"/ai-robo-learning/docs/module-3/chapter-5/path-planning-humanoids"},"next":{"title":"AI Integration in Robotics","permalink":"/ai-robo-learning/docs/module-4/chapter-1/ai-integration"}}');var r=t(4848),i=t(8453);const o={sidebar_position:6,title:"Perception Systems"},a="Perception Systems",l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Multi-Sensor Fusion for Humanoid Perception",id:"multi-sensor-fusion-for-humanoid-perception",level:2},{value:"Sensor Fusion Architecture",id:"sensor-fusion-architecture",level:3},{value:"Computer Vision for Object Detection",id:"computer-vision-for-object-detection",level:2},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:3},{value:"Depth Estimation and 3D Reconstruction",id:"depth-estimation-and-3d-reconstruction",level:2},{value:"Depth Processing Node",id:"depth-processing-node",level:3},{value:"SLAM for Environment Mapping",id:"slam-for-environment-mapping",level:2},{value:"Visual-Inertial SLAM Node",id:"visual-inertial-slam-node",level:3},{value:"Scene Understanding and Object Recognition",id:"scene-understanding-and-object-recognition",level:2},{value:"Scene Understanding Node",id:"scene-understanding-node",level:3},{value:"Hands-on Lab: Complete Perception System",id:"hands-on-lab-complete-perception-system",level:2},{value:"Step 1: Create the Perception System Launch File",id:"step-1-create-the-perception-system-launch-file",level:3},{value:"Step 2: Create the Complete Perception Node",id:"step-2-create-the-complete-perception-node",level:3},{value:"Step 3: Test the Perception System",id:"step-3-test-the-perception-system",level:3},{value:"Optimization Techniques",id:"optimization-techniques",level:2},{value:"Real-time Perception Optimization",id:"real-time-perception-optimization",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"perception-systems",children:"Perception Systems"})}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers advanced perception systems for humanoid robots, including computer vision, depth sensing, object recognition, and environment understanding that enable autonomous navigation and interaction."}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:"In this chapter, you'll explore:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multi-sensor fusion for humanoid perception"}),"\n",(0,r.jsx)(n.li,{children:"Computer vision algorithms for object detection"}),"\n",(0,r.jsx)(n.li,{children:"Depth estimation and 3D reconstruction"}),"\n",(0,r.jsx)(n.li,{children:"SLAM for environment mapping"}),"\n",(0,r.jsx)(n.li,{children:"Object recognition and scene understanding"}),"\n",(0,r.jsx)(n.li,{children:"Real-time perception optimization"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completion of Module 1-3, Chapters 1-5"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of sensor integration"}),"\n",(0,r.jsx)(n.li,{children:"Basic computer vision knowledge"}),"\n",(0,r.jsx)(n.li,{children:"Experience with ROS 2 message types"}),"\n",(0,r.jsx)(n.li,{children:"Knowledge of machine learning concepts"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"multi-sensor-fusion-for-humanoid-perception",children:"Multi-Sensor Fusion for Humanoid Perception"}),"\n",(0,r.jsx)(n.h3,{id:"sensor-fusion-architecture",children:"Sensor Fusion Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots typically use multiple sensors that need to be fused for robust perception:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2, Imu, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom std_msgs.msg import Float32MultiArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom scipy.spatial.transform import Rotation as R\nfrom collections import deque\nimport threading\n\nclass MultiSensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('multi_sensor_fusion_node')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to all sensors\n        self.camera_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10\n        )\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.lidar_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.info_callback,\n            10\n        )\n\n        # Publishers\n        self.fused_data_pub = self.create_publisher(Float32MultiArray, '/fused_sensor_data', 10)\n        self.environment_map_pub = self.create_publisher(PointCloud2, '/environment_map', 10)\n\n        # Sensor data storage\n        self.camera_data = None\n        self.lidar_data = None\n        self.imu_data = None\n        self.depth_data = None\n        self.camera_info = None\n\n        # Sensor fusion state\n        self.sensor_buffer = deque(maxlen=10)\n        self.fusion_lock = threading.Lock()\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        self.get_logger().info('Multi-Sensor Fusion Node initialized')\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.camera_data = {\n                'image': cv_image,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.process_sensor_fusion()\n        except Exception as e:\n            self.get_logger().error(f'Camera callback error: {str(e)}')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data\"\"\"\n        try:\n            ranges = np.array(msg.ranges)\n            angles = np.array([msg.angle_min + i * msg.angle_increment for i in range(len(ranges))])\n\n            # Filter valid ranges\n            valid_mask = (ranges >= msg.range_min) & (ranges <= msg.range_max) & (~np.isnan(ranges)) & (~np.isinf(ranges))\n            valid_ranges = ranges[valid_mask]\n            valid_angles = angles[valid_mask]\n\n            # Convert to Cartesian coordinates\n            x_points = valid_ranges * np.cos(valid_angles)\n            y_points = valid_ranges * np.sin(valid_angles)\n\n            self.lidar_data = {\n                'x': x_points,\n                'y': y_points,\n                'ranges': valid_ranges,\n                'angles': valid_angles,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.process_sensor_fusion()\n        except Exception as e:\n            self.get_logger().error(f'LiDAR callback error: {str(e)}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        try:\n            self.imu_data = {\n                'orientation': [\n                    msg.orientation.x,\n                    msg.orientation.y,\n                    msg.orientation.z,\n                    msg.orientation.w\n                ],\n                'angular_velocity': [\n                    msg.angular_velocity.x,\n                    msg.angular_velocity.y,\n                    msg.angular_velocity.z\n                ],\n                'linear_acceleration': [\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ],\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.process_sensor_fusion()\n        except Exception as e:\n            self.get_logger().error(f'IMU callback error: {str(e)}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth data\"\"\"\n        try:\n            depth_image = self.bridge.imgmsg_to_cv2(msg, \"32FC1\")\n            self.depth_data = {\n                'image': depth_image,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.process_sensor_fusion()\n        except Exception as e:\n            self.get_logger().error(f'Depth callback error: {str(e)}')\n\n    def info_callback(self, msg):\n        \"\"\"Process camera info\"\"\"\n        try:\n            self.camera_info = msg\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n        except Exception as e:\n            self.get_logger().error(f'Camera info callback error: {str(e)}')\n\n    def process_sensor_fusion(self):\n        \"\"\"Process and fuse sensor data\"\"\"\n        with self.fusion_lock:\n            # Check if we have all necessary sensor data\n            if not all([self.camera_data, self.lidar_data, self.imu_data, self.depth_data]):\n                return\n\n            # Perform sensor fusion\n            fused_data = self.fuse_sensors()\n\n            # Publish fused data\n            if fused_data is not None:\n                fused_msg = Float32MultiArray()\n                fused_msg.data = fused_data.flatten().tolist()\n                self.fused_data_pub.publish(fused_msg)\n\n    def fuse_sensors(self):\n        \"\"\"Fuse data from multiple sensors\"\"\"\n        try:\n            # Create a unified representation of the environment\n            fused_data = np.zeros((100, 6))  # [x, y, z, r, g, b] for points\n\n            # Process LiDAR data to get 2D points\n            lidar_points = np.column_stack((self.lidar_data['x'], self.lidar_data['y']))\n\n            # Process depth data to get 3D points (if camera matrix is available)\n            if self.camera_matrix is not None and self.depth_data is not None:\n                depth_points = self.process_depth_to_3d()\n\n                # Combine LiDAR and depth data\n                # This is a simplified approach - in practice, you'd use more sophisticated fusion\n                for i, (x, y) in enumerate(lidar_points[:min(len(lidar_points), len(depth_points))]):\n                    fused_data[i, 0] = x  # x coordinate\n                    fused_data[i, 1] = y  # y coordinate\n                    fused_data[i, 2] = depth_points[i, 2] if i < len(depth_points) else 0  # z coordinate\n                    fused_data[i, 3] = 0.5  # dummy red value\n                    fused_data[i, 4] = 0.5  # dummy green value\n                    fused_data[i, 5] = 0.5  # dummy blue value\n\n            return fused_data\n\n        except Exception as e:\n            self.get_logger().error(f'Sensor fusion error: {str(e)}')\n            return None\n\n    def process_depth_to_3d(self):\n        \"\"\"Convert depth image to 3D points\"\"\"\n        try:\n            depth_image = self.depth_data['image']\n            height, width = depth_image.shape\n\n            # Generate coordinate grids\n            u_coords, v_coords = np.meshgrid(np.arange(width), np.arange(height))\n            u_coords = u_coords.astype(np.float32)\n            v_coords = v_coords.astype(np.float32)\n\n            # Apply camera matrix to convert to 3D\n            if self.camera_matrix is not None:\n                fx = self.camera_matrix[0, 0]\n                fy = self.camera_matrix[1, 1]\n                cx = self.camera_matrix[0, 2]\n                cy = self.camera_matrix[1, 2]\n\n                # Convert to 3D coordinates\n                x_coords = (u_coords - cx) * depth_image / fx\n                y_coords = (v_coords - cy) * depth_image / fy\n                z_coords = depth_image\n\n                # Flatten and stack\n                points_3d = np.column_stack((\n                    x_coords.flatten(),\n                    y_coords.flatten(),\n                    z_coords.flatten()\n                ))\n\n                # Filter out invalid points (where depth is 0 or invalid)\n                valid_mask = (z_coords.flatten() > 0) & (np.isfinite(z_coords.flatten()))\n                points_3d = points_3d[valid_mask]\n\n                return points_3d\n\n        except Exception as e:\n            self.get_logger().error(f'Depth to 3D conversion error: {str(e)}')\n\n        return np.zeros((0, 3))  # Return empty array if conversion fails\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultiSensorFusionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"computer-vision-for-object-detection",children:"Computer Vision for Object Detection"}),"\n",(0,r.jsx)(n.h3,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nfrom visualization_msgs.msg import MarkerArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(Image, '/detection_output', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/detection_markers', 10)\n\n        # Object detection parameters\n        self.confidence_threshold = 0.5\n        self.nms_threshold = 0.4\n\n        # Load YOLO model (simplified - in practice, use a real model)\n        # For this example, we'll use OpenCV's DNN module with a pre-trained model\n        try:\n            # In a real implementation, you would load a model like this:\n            # self.net = cv2.dnn.readNetFromDarknet('yolo.cfg', 'yolo.weights')\n            # self.layer_names = self.net.getLayerNames()\n            # self.output_layers = [self.layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n\n            # For simulation, we'll use a mock detector\n            self.detector_available = False\n            self.get_logger().info('Object detection model not loaded (simulation mode)')\n        except:\n            self.detector_available = False\n            self.get_logger().info('Object detection model not available (using mock)')\n\n        # Class names for detection\n        self.class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        self.detection_lock = threading.Lock()\n\n        self.get_logger().info('Object Detection Node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image for object detection\"\"\"\n        with self.detection_lock:\n            try:\n                # Convert ROS Image to OpenCV\n                cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n                # Perform object detection\n                if self.detector_available:\n                    detections = self.detect_objects(cv_image)\n                else:\n                    # Mock detection for simulation\n                    detections = self.mock_detection(cv_image)\n\n                # Draw detections on image\n                output_image = self.draw_detections(cv_image, detections)\n\n                # Publish detection output\n                output_msg = self.bridge.cv2_to_imgmsg(output_image, \"bgr8\")\n                output_msg.header = msg.header\n                self.detection_pub.publish(output_msg)\n\n                # Publish visualization markers\n                self.publish_detection_markers(detections, msg.header)\n\n            except Exception as e:\n                self.get_logger().error(f'Error in object detection: {str(e)}')\n\n    def mock_detection(self, image):\n        \"\"\"Mock object detection for simulation\"\"\"\n        # In a real system, this would be replaced with actual detection\n        # For simulation, we'll create some mock detections\n        height, width = image.shape[:2]\n\n        # Create some mock detections (person, chair, bottle)\n        mock_detections = [\n            {\n                'label': 'person',\n                'confidence': 0.85,\n                'bbox': [width * 0.4, height * 0.3, width * 0.6, height * 0.7]\n            },\n            {\n                'label': 'chair',\n                'confidence': 0.78,\n                'bbox': [width * 0.2, height * 0.5, width * 0.35, height * 0.8]\n            },\n            {\n                'label': 'bottle',\n                'confidence': 0.65,\n                'bbox': [width * 0.7, height * 0.6, width * 0.75, height * 0.8]\n            }\n        ]\n\n        return mock_detections\n\n    def detect_objects(self, image):\n        \"\"\"Perform object detection using loaded model\"\"\"\n        # This is where the actual detection would happen\n        # Using OpenCV DNN module with YOLO\n        height, width, channels = image.shape\n\n        # Create blob from image\n        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n        self.net.setInput(blob)\n        outs = self.net.forward(self.output_layers)\n\n        # Process outputs\n        class_ids = []\n        confidences = []\n        boxes = []\n\n        for out in outs:\n            for detection in out:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > self.confidence_threshold:\n                    # Object detected\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    # Rectangle coordinates\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confidence_threshold, self.nms_threshold)\n\n        # Format detections\n        detections = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                x, y, w, h = boxes[i]\n                detections.append({\n                    'label': self.class_names[class_ids[i]] if class_ids[i] < len(self.class_names) else 'unknown',\n                    'confidence': confidences[i],\n                    'bbox': [x, y, x + w, y + h]\n                })\n\n        return detections\n\n    def draw_detections(self, image, detections):\n        \"\"\"Draw detection results on image\"\"\"\n        output_image = image.copy()\n\n        for detection in detections:\n            x1, y1, x2, y2 = map(int, detection['bbox'])\n            label = f\"{detection['label']}: {detection['confidence']:.2f}\"\n\n            # Draw bounding box\n            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw label\n            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n            cv2.rectangle(output_image, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), (0, 255, 0), cv2.FILLED)\n            cv2.putText(output_image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n\n        return output_image\n\n    def publish_detection_markers(self, detections, header):\n        \"\"\"Publish visualization markers for detections\"\"\"\n        from visualization_msgs.msg import MarkerArray, Marker\n        from geometry_msgs.msg import Point\n\n        marker_array = MarkerArray()\n\n        for i, detection in enumerate(detections):\n            # Create marker for bounding box\n            marker = Marker()\n            marker.header = header\n            marker.ns = \"object_detections\"\n            marker.id = i\n            marker.type = Marker.LINE_STRIP\n            marker.action = Marker.ADD\n\n            # Set position (simplified - in real system, would use depth to get 3D position)\n            marker.pose.position.z = 1.0  # Default height\n            marker.pose.orientation.w = 1.0\n\n            # Set scale\n            marker.scale.x = 0.05  # Line width\n\n            # Set color based on confidence\n            confidence = detection['confidence']\n            marker.color.r = 1.0 - confidence  # Red decreases with confidence\n            marker.color.g = confidence  # Green increases with confidence\n            marker.color.b = 0.0\n            marker.color.a = 1.0\n\n            # Define points for the bounding box\n            x1, y1, x2, y2 = detection['bbox']\n            points = [\n                Point(x=x1, y=y1, z=0),\n                Point(x=x2, y=y1, z=0),\n                Point(x=x2, y=y2, z=0),\n                Point(x=x1, y=y2, z=0),\n                Point(x=x1, y=y1, z=0)  # Close the loop\n            ]\n\n            marker.points = points\n            marker_array.markers.append(marker)\n\n            # Create text marker for label\n            text_marker = Marker()\n            text_marker.header = header\n            text_marker.ns = \"detection_labels\"\n            text_marker.id = i + len(detections)  # Different ID to avoid conflict\n            text_marker.type = Marker.TEXT_VIEW_FACING\n            text_marker.action = Marker.ADD\n\n            text_marker.pose.position.x = x1\n            text_marker.pose.position.y = y1 - 10\n            text_marker.pose.position.z = 1.0\n            text_marker.pose.orientation.w = 1.0\n\n            text_marker.scale.z = 0.2  # Text size\n            text_marker.color.r = 1.0\n            text_marker.color.g = 1.0\n            text_marker.color.b = 1.0\n            text_marker.color.a = 1.0\n\n            text_marker.text = f\"{detection['label']} ({detection['confidence']:.2f})\"\n\n            marker_array.markers.append(text_marker)\n\n        self.marker_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetectionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"depth-estimation-and-3d-reconstruction",children:"Depth Estimation and 3D Reconstruction"}),"\n",(0,r.jsx)(n.h3,{id:"depth-processing-node",children:"Depth Processing Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import Header\nfrom visualization_msgs.msg import MarkerArray\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom sensor_msgs_py import point_cloud2\nfrom sensor_msgs.msg import PointField\nimport struct\n\nclass DepthProcessingNode(Node):\n    def __init__(self):\n        super().__init__(\'depth_processing_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to depth and camera info\n        self.depth_sub = self.create_subscription(\n            Image,\n            \'/camera/depth/image_raw\',\n            self.depth_callback,\n            10\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/camera_info\',\n            self.info_callback,\n            10\n        )\n\n        # Publishers\n        self.pointcloud_pub = self.create_publisher(PointCloud2, \'/pointcloud\', 10)\n        self.processed_depth_pub = self.create_publisher(Image, \'/processed_depth\', 10)\n        self.obstacle_pub = self.create_publisher(PointStamped, \'/obstacle_point\', 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n        self.camera_info = None\n\n        # Depth processing parameters\n        self.depth_scale = 0.001  # Default: millimeters to meters\n        self.obstacle_distance_threshold = 1.0  # meters\n\n        self.get_logger().info(\'Depth Processing Node initialized\')\n\n    def info_callback(self, msg):\n        """Process camera info to get intrinsic parameters"""\n        try:\n            self.camera_info = msg\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n            self.width = msg.width\n            self.height = msg.height\n        except Exception as e:\n            self.get_logger().error(f\'Camera info callback error: {str(e)}\')\n\n    def depth_callback(self, msg):\n        """Process depth image and convert to point cloud"""\n        try:\n            # Convert ROS Image to OpenCV\n            depth_image = self.bridge.imgmsg_to_cv2(msg, "32FC1")\n\n            # Process depth image\n            processed_depth = self.process_depth_image(depth_image)\n\n            # Convert to point cloud if camera parameters are available\n            if self.camera_matrix is not None:\n                pointcloud_msg = self.depth_to_pointcloud(depth_image, msg.header)\n                self.pointcloud_pub.publish(pointcloud_msg)\n\n            # Publish processed depth image\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_depth, "32FC1")\n            processed_msg.header = msg.header\n            self.processed_depth_pub.publish(processed_msg)\n\n            # Detect obstacles\n            self.detect_obstacles(depth_image, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f\'Depth callback error: {str(e)}\')\n\n    def process_depth_image(self, depth_image):\n        """Process depth image for noise reduction and filtering"""\n        # Apply median filter to reduce noise\n        filtered_depth = cv2.medianBlur(depth_image, 5)\n\n        # Apply bilateral filter for edge-preserving smoothing\n        filtered_depth = cv2.bilateralFilter(filtered_depth, 9, 75, 75)\n\n        # Remove invalid depth values (NaN, infinity)\n        filtered_depth = np.nan_to_num(filtered_depth, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Apply depth range filtering\n        filtered_depth = np.clip(filtered_depth, 0.1, 10.0)  # Clamp to 0.1m - 10m\n\n        return filtered_depth\n\n    def depth_to_pointcloud(self, depth_image, header):\n        """Convert depth image to PointCloud2 message"""\n        if self.camera_matrix is None:\n            return PointCloud2()\n\n        height, width = depth_image.shape\n\n        # Generate coordinate grids\n        u_coords, v_coords = np.meshgrid(np.arange(width), np.arange(height))\n        u_coords = u_coords.astype(np.float32)\n        v_coords = v_coords.astype(np.float32)\n\n        # Get camera intrinsic parameters\n        fx = self.camera_matrix[0, 0]\n        fy = self.camera_matrix[1, 1]\n        cx = self.camera_matrix[0, 2]\n        cy = self.camera_matrix[1, 2]\n\n        # Convert pixel coordinates to 3D coordinates\n        x_coords = (u_coords - cx) * depth_image / fx\n        y_coords = (v_coords - cy) * depth_image / fy\n        z_coords = depth_image\n\n        # Flatten arrays\n        x_flat = x_coords.flatten()\n        y_flat = y_coords.flatten()\n        z_flat = z_coords.flatten()\n\n        # Filter out invalid points\n        valid_mask = (z_flat > 0) & (np.isfinite(z_flat)) & (np.isfinite(x_flat)) & (np.isfinite(y_flat))\n        x_valid = x_flat[valid_mask]\n        y_valid = y_flat[valid_mask]\n        z_valid = z_flat[valid_mask]\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        # Create point data\n        points = []\n        for i in range(len(x_valid)):\n            points.append([x_valid[i], y_valid[i], z_valid[i]])\n\n        # Create PointCloud2 message\n        pointcloud_msg = PointCloud2()\n        pointcloud_msg.header = header\n        pointcloud_msg.height = 1\n        pointcloud_msg.width = len(points)\n        pointcloud_msg.fields = fields\n        pointcloud_msg.is_bigendian = False\n        pointcloud_msg.is_dense = True\n        pointcloud_msg.point_step = 12  # 3 * 4 bytes (float32)\n        pointcloud_msg.row_step = pointcloud_msg.point_step * pointcloud_msg.width\n\n        # Pack point data\n        data = []\n        for point in points:\n            for value in point:\n                data.append(struct.pack(\'f\', value))\n\n        pointcloud_msg.data = b\'\'.join(data)\n\n        return pointcloud_msg\n\n    def detect_obstacles(self, depth_image, header):\n        """Detect obstacles in the depth image"""\n        # Define ROI for obstacle detection (front of robot)\n        height, width = depth_image.shape\n        roi_top = int(height * 0.3)\n        roi_bottom = int(height * 0.7)\n        roi_left = int(width * 0.2)\n        roi_right = int(width * 0.8)\n\n        roi_depth = depth_image[roi_top:roi_bottom, roi_left:roi_right]\n\n        # Find points closer than threshold\n        obstacle_mask = (roi_depth > 0) & (roi_depth < self.obstacle_distance_threshold)\n        obstacle_points = np.where(obstacle_mask)\n\n        if len(obstacle_points[0]) > 0:  # If obstacles detected\n            # Calculate average obstacle position in ROI\n            avg_y = np.mean(obstacle_points[0]) + roi_top\n            avg_x = np.mean(obstacle_points[1]) + roi_left\n\n            # Convert to 3D coordinates\n            if self.camera_matrix is not None:\n                fx = self.camera_matrix[0, 0]\n                fy = self.camera_matrix[1, 1]\n                cx = self.camera_matrix[0, 2]\n                cy = self.camera_matrix[1, 2]\n\n                # Get average depth\n                avg_depth = np.mean(roi_depth[obstacle_mask])\n\n                # Convert to 3D coordinates\n                x_3d = (avg_x - cx) * avg_depth / fx\n                y_3d = (avg_y - cy) * avg_depth / fy\n                z_3d = avg_depth\n\n                # Publish obstacle point\n                obstacle_point = PointStamped()\n                obstacle_point.header = header\n                obstacle_point.point.x = float(x_3d)\n                obstacle_point.point.y = float(y_3d)\n                obstacle_point.point.z = float(z_3d)\n\n                self.obstacle_pub.publish(obstacle_point)\n\n                self.get_logger().info(f\'Obstacle detected at ({x_3d:.2f}, {y_3d:.2f}, {z_3d:.2f})\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = DepthProcessingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"slam-for-environment-mapping",children:"SLAM for Environment Mapping"}),"\n",(0,r.jsx)(n.h3,{id:"visual-inertial-slam-node",children:"Visual-Inertial SLAM Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom nav_msgs.msg import OccupancyGrid\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom collections import deque\nimport threading\n\nclass VISLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'vislam_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to sensors\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers\n        self.pose_pub = self.create_publisher(PoseStamped, \'/vislam_pose\', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, \'/vislam_map\', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # SLAM state\n        self.prev_image = None\n        self.prev_keypoints = None\n        self.prev_descriptors = None\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\n        self.imu_data = None\n        self.trajectory = deque(maxlen=1000)\n\n        # Feature detector\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # SLAM lock\n        self.slam_lock = threading.Lock()\n\n        self.get_logger().info(\'Visual-Inertial SLAM Node initialized\')\n\n    def imu_callback(self, msg):\n        """Process IMU data for sensor fusion"""\n        with self.slam_lock:\n            self.imu_data = {\n                \'angular_velocity\': np.array([\n                    msg.angular_velocity.x,\n                    msg.angular_velocity.y,\n                    msg.angular_velocity.z\n                ]),\n                \'linear_acceleration\': np.array([\n                    msg.linear_acceleration.x,\n                    msg.linear_acceleration.y,\n                    msg.linear_acceleration.z\n                ]),\n                \'orientation\': np.array([\n                    msg.orientation.x,\n                    msg.orientation.y,\n                    msg.orientation.z,\n                    msg.orientation.w\n                ])\n            }\n\n    def image_callback(self, msg):\n        """Process image for SLAM"""\n        with self.slam_lock:\n            try:\n                # Convert to OpenCV\n                current_image = self.bridge.imgmsg_to_cv2(msg, "mono8")\n\n                if self.prev_image is not None:\n                    # Extract features from current image\n                    current_keypoints, current_descriptors = self.feature_detector.detectAndCompute(\n                        current_image, None\n                    )\n\n                    if (self.prev_keypoints is not None and\n                        current_keypoints is not None and\n                        current_descriptors is not None and\n                        len(self.prev_keypoints) > 10 and\n                        len(current_keypoints) > 10):\n\n                        # Match features\n                        matches = self.matcher.knnMatch(\n                            self.prev_descriptors, current_descriptors, k=2\n                        )\n\n                        # Apply Lowe\'s ratio test\n                        good_matches = []\n                        for match_pair in matches:\n                            if len(match_pair) == 2:\n                                m, n = match_pair\n                                if m.distance < 0.75 * n.distance:\n                                    good_matches.append(m)\n\n                        if len(good_matches) >= 10:\n                            # Get matched points\n                            prev_points = np.float32([\n                                self.prev_keypoints[m.queryIdx].pt for m in good_matches\n                            ]).reshape(-1, 1, 2)\n                            curr_points = np.float32([\n                                current_keypoints[m.trainIdx].pt for m in good_matches\n                            ]).reshape(-1, 1, 2)\n\n                            # Estimate essential matrix\n                            E, mask = cv2.findEssentialMat(\n                                curr_points, prev_points,\n                                focal=500, pp=(320, 240), method=cv2.RANSAC, prob=0.999, threshold=1.0\n                            )\n\n                            if E is not None:\n                                # Recover pose\n                                _, R, t, _ = cv2.recoverPose(\n                                    E, curr_points, prev_points\n                                )\n\n                                # Create transformation matrix\n                                motion = np.eye(4)\n                                motion[:3, :3] = R\n                                motion[:3, 3] = t.flatten()\n\n                                # Apply motion to current pose\n                                self.current_pose = self.current_pose @ motion\n\n                                # Store in trajectory\n                                position = self.current_pose[:3, 3]\n                                self.trajectory.append({\n                                    \'position\': position,\n                                    \'timestamp\': msg.header.stamp\n                                })\n\n                                # Publish pose and TF\n                                self.publish_pose_and_tf(msg.header)\n\n                # Store current frame data\n                self.prev_image = current_image\n                self.prev_keypoints = current_keypoints\n                self.prev_descriptors = current_descriptors\n\n            except Exception as e:\n                self.get_logger().error(f\'VISLAM error: {str(e)}\')\n\n    def publish_pose_and_tf(self, header):\n        """Publish pose and transform"""\n        # Publish pose\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.header.frame_id = "map"\n\n        pose_msg.pose.position.x = float(self.current_pose[0, 3])\n        pose_msg.pose.position.y = float(self.current_pose[1, 3])\n        pose_msg.pose.position.z = float(self.current_pose[2, 3])\n\n        # Convert rotation matrix to quaternion\n        R = self.current_pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(R)\n        pose_msg.pose.orientation.w = qw\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n\n        self.pose_pub.publish(pose_msg)\n\n        # Publish TF transform\n        t = TransformStamped()\n        t.header.stamp = header.stamp\n        t.header.frame_id = "map"\n        t.child_frame_id = "base_link"\n\n        t.transform.translation.x = float(self.current_pose[0, 3])\n        t.transform.translation.y = float(self.current_pose[1, 3])\n        t.transform.translation.z = float(self.current_pose[2, 3])\n\n        t.transform.rotation.w = qw\n        t.transform.rotation.x = qx\n        t.transform.rotation.y = qy\n        t.transform.rotation.z = qz\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VISLAMNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"scene-understanding-and-object-recognition",children:"Scene Understanding and Object Recognition"}),"\n",(0,r.jsx)(n.h3,{id:"scene-understanding-node",children:"Scene Understanding Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom collections import Counter\n\nclass SceneUnderstandingNode(Node):\n    def __init__(self):\n        super().__init__(\'scene_understanding_node\')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to sensors\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2,\n            \'/pointcloud\',\n            self.pointcloud_callback,\n            10\n        )\n\n        # Publishers\n        self.scene_desc_pub = self.create_publisher(String, \'/scene_description\', 10)\n        self.object_map_pub = self.create_publisher(String, \'/object_map\', 10)\n\n        # Scene understanding state\n        self.scene_objects = []\n        self.scene_layout = {}\n\n        self.get_logger().info(\'Scene Understanding Node initialized\')\n\n    def image_callback(self, msg):\n        """Process image for scene understanding"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Analyze scene content\n            scene_analysis = self.analyze_scene(cv_image)\n\n            # Publish scene description\n            scene_desc_msg = String()\n            scene_desc_msg.data = scene_analysis\n            self.scene_desc_pub.publish(scene_desc_msg)\n\n            self.get_logger().info(f\'Scene analysis: {scene_analysis}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Scene understanding error: {str(e)}\')\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud for 3D scene understanding"""\n        try:\n            # Convert PointCloud2 to numpy array (simplified)\n            # In practice, use sensor_msgs_py.point_cloud2.read_points\n            points_3d = self.pointcloud_to_array(msg)\n\n            if points_3d is not None:\n                # Analyze 3D scene structure\n                scene_structure = self.analyze_3d_structure(points_3d)\n\n                # Publish object map\n                object_map_msg = String()\n                object_map_msg.data = str(scene_structure)\n                self.object_map_pub.publish(object_map_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Point cloud processing error: {str(e)}\')\n\n    def analyze_scene(self, image):\n        """Analyze 2D scene content"""\n        height, width = image.shape[:2]\n\n        # Use simple color and texture analysis for scene understanding\n        # In practice, use deep learning models for scene segmentation\n\n        # Convert to different color spaces for analysis\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Analyze dominant colors\n        dominant_colors = self.get_dominant_colors(image)\n\n        # Analyze texture\n        texture_analysis = self.analyze_texture(gray)\n\n        # Analyze scene layout based on color distribution\n        scene_layout = self.analyze_layout(hsv)\n\n        # Combine analyses\n        scene_description = {\n            \'dominant_colors\': dominant_colors,\n            \'texture_type\': texture_analysis,\n            \'layout\': scene_layout,\n            \'estimated_objects\': self.estimate_objects(image),\n            \'scene_type\': self.classify_scene(dominant_colors, texture_analysis)\n        }\n\n        return str(scene_description)\n\n    def get_dominant_colors(self, image, k=5):\n        """Extract dominant colors from image"""\n        # Reshape image to be a list of pixels\n        pixels = image.reshape((-1, 3))\n\n        # Convert to float\n        pixels = np.float32(pixels)\n\n        # Apply K-means clustering\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1)\n        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n\n        # Convert back to uint8\n        centers = np.uint8(centers)\n\n        # Count the frequency of each color\n        unique, counts = np.unique(labels, return_counts=True)\n        color_freq = dict(zip(centers, counts))\n\n        # Sort by frequency\n        sorted_colors = sorted(color_freq.items(), key=lambda x: x[1], reverse=True)\n\n        return [color.tolist() for color, freq in sorted_colors[:3]]\n\n    def analyze_texture(self, gray_image):\n        """Analyze texture properties of image"""\n        # Use Local Binary Pattern for texture analysis\n        # Simplified version - in practice, use more sophisticated methods\n\n        # Calculate gradients\n        grad_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n\n        # Calculate gradient magnitude\n        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n\n        # Analyze texture based on gradient statistics\n        mean_grad = np.mean(grad_magnitude)\n        std_grad = np.std(grad_magnitude)\n\n        if mean_grad < 20:\n            return "smooth"\n        elif mean_grad < 50:\n            return "moderate"\n        else:\n            return "rough"\n\n    def analyze_layout(self, hsv_image):\n        """Analyze scene layout based on color distribution"""\n        height, width = hsv_image.shape[:2]\n\n        # Divide image into regions\n        regions = {\n            \'top\': hsv_image[:height//3, :],\n            \'middle\': hsv_image[height//3:2*height//3, :],\n            \'bottom\': hsv_image[2*height//3:, :],\n            \'left\': hsv_image[:, :width//3],\n            \'center\': hsv_image[:, width//3:2*width//3],\n            \'right\': hsv_image[:, 2*width//3:]\n        }\n\n        layout_analysis = {}\n        for region_name, region in regions.items():\n            if region.size > 0:\n                # Analyze dominant hue in region\n                dominant_hue = np.mean(region[:, :, 0])\n                saturation_mean = np.mean(region[:, :, 1])\n                value_mean = np.mean(region[:, :, 2])\n\n                layout_analysis[region_name] = {\n                    \'dominant_hue\': float(dominant_hue),\n                    \'saturation\': float(saturation_mean),\n                    \'brightness\': float(value_mean)\n                }\n\n        return layout_analysis\n\n    def estimate_objects(self, image):\n        """Estimate objects in scene using simple methods"""\n        # In practice, use object detection models\n        # For simulation, use simple shape detection\n\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        edged = cv2.Canny(blurred, 50, 150)\n\n        # Find contours\n        contours, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        object_types = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 100:  # Filter small contours\n                # Approximate contour to get shape\n                perimeter = cv2.arcLength(contour, True)\n                approx = cv2.approxPolyDP(contour, 0.04 * perimeter, True)\n\n                # Estimate shape based on number of vertices\n                if len(approx) == 3:\n                    object_types.append("triangle")\n                elif len(approx) == 4:\n                    # Check if it\'s square or rectangle\n                    x, y, w, h = cv2.boundingRect(approx)\n                    aspect_ratio = float(w) / h\n                    if 0.75 <= aspect_ratio <= 1.25:\n                        object_types.append("square")\n                    else:\n                        object_types.append("rectangle")\n                elif len(approx) > 4:\n                    object_types.append("circle")  # Approximate as circle\n                else:\n                    object_types.append("other")\n\n        # Count object types\n        object_counts = Counter(object_types)\n        return dict(object_counts)\n\n    def classify_scene(self, dominant_colors, texture_type):\n        """Classify scene type based on features"""\n        # Simple classification based on dominant colors and texture\n        # In practice, use trained models\n\n        blue_count = sum(1 for color in dominant_colors if color[0] < 100 and color[1] < 150 and color[2] > 100)  # Blue-like\n        green_count = sum(1 for color in dominant_colors if color[1] > color[0] and color[1] > color[2])  # Green-like\n        brown_count = sum(1 for color in dominant_colors if all(50 < c < 150 for c in color))  # Brown-like\n\n        if blue_count > 0:\n            return "outdoor/nature"\n        elif green_count > 0:\n            return "indoor/garden"\n        elif brown_count > 0 and texture_type == "rough":\n            return "indoor/office"\n        else:\n            return "unknown"\n\n    def pointcloud_to_array(self, pointcloud_msg):\n        """Convert PointCloud2 message to numpy array"""\n        # This is a simplified version\n        # In practice, use sensor_msgs_py.point_cloud2.read_points\n        try:\n            # For simulation purposes, return a dummy array\n            # In real implementation, extract x, y, z coordinates\n            return np.random.rand(100, 3) * 10  # Random points for simulation\n        except:\n            return None\n\n    def analyze_3d_structure(self, points_3d):\n        """Analyze 3D structure from point cloud"""\n        if len(points_3d) == 0:\n            return {}\n\n        # Use DBSCAN for clustering to identify objects\n        clustering = DBSCAN(eps=0.2, min_samples=10).fit(points_3d)\n        labels = clustering.labels_\n\n        # Count clusters (potential objects)\n        unique_labels = set(labels)\n        n_clusters = len(unique_labels) - (1 if -1 in labels else 0)  # Don\'t count noise\n\n        # Analyze ground plane (lowest Z values)\n        z_values = points_3d[:, 2]\n        ground_z = np.percentile(z_values, 5)  # Approximate ground level\n\n        # Analyze height distribution\n        height_distribution = {\n            \'min_height\': float(np.min(z_values)),\n            \'max_height\': float(np.max(z_values)),\n            \'avg_height\': float(np.mean(z_values)),\n            \'ground_level\': float(ground_z)\n        }\n\n        return {\n            \'n_objects\': n_clusters,\n            \'height_distribution\': height_distribution,\n            \'estimated_room_size\': self.estimate_room_size(points_3d)\n        }\n\n    def estimate_room_size(self, points_3d):\n        """Estimate room dimensions from point cloud"""\n        if len(points_3d) == 0:\n            return {}\n\n        x_range = [float(np.min(points_3d[:, 0])), float(np.max(points_3d[:, 0]))]\n        y_range = [float(np.min(points_3d[:, 1])), float(np.max(points_3d[:, 1]))]\n        z_range = [float(np.min(points_3d[:, 2])), float(np.max(points_3d[:, 2]))]\n\n        return {\n            \'x_range\': x_range,\n            \'y_range\': y_range,\n            \'z_range\': z_range,\n            \'width\': float(x_range[1] - x_range[0]),\n            \'depth\': float(y_range[1] - y_range[0]),\n            \'height\': float(z_range[1] - z_range[0])\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SceneUnderstandingNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-lab-complete-perception-system",children:"Hands-on Lab: Complete Perception System"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you'll integrate all perception components into a complete system."}),"\n",(0,r.jsx)(n.h3,{id:"step-1-create-the-perception-system-launch-file",children:"Step 1: Create the Perception System Launch File"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"perception_system_launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n\n    # Perception system nodes\n    multi_sensor_fusion = Node(\n        package='ai_robo_learning',\n        executable='multi_sensor_fusion_node',\n        name='multi_sensor_fusion',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    object_detection = Node(\n        package='ai_robo_learning',\n        executable='object_detection_node',\n        name='object_detection',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    depth_processing = Node(\n        package='ai_robo_learning',\n        executable='depth_processing_node',\n        name='depth_processing',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    vislam = Node(\n        package='ai_robo_learning',\n        executable='vislam_node',\n        name='vislam',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    scene_understanding = Node(\n        package='ai_robo_learning',\n        executable='scene_understanding_node',\n        name='scene_understanding',\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Return launch description\n    ld = LaunchDescription()\n\n    # Add all nodes\n    ld.add_action(multi_sensor_fusion)\n    ld.add_action(object_detection)\n    ld.add_action(depth_processing)\n    ld.add_action(vislam)\n    ld.add_action(scene_understanding)\n\n    return ld\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-create-the-complete-perception-node",children:"Step 2: Create the Complete Perception Node"}),"\n",(0,r.jsxs)(n.p,{children:["Create ",(0,r.jsx)(n.code,{children:"complete_perception_system.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2, Imu, CameraInfo\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import OccupancyGrid\nfrom std_msgs.msg import String, Bool\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom collections import deque\nimport threading\n\nclass CompletePerceptionSystem(Node):\n    def __init__(self):\n        super().__init__('complete_perception_system')\n\n        # Create CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to all sensors\n        self.camera_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.camera_callback,\n            10\n        )\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.lidar_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.info_callback,\n            10\n        )\n\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2,\n            '/pointcloud',\n            self.pointcloud_callback,\n            10\n        )\n\n        # Publishers\n        self.environment_map_pub = self.create_publisher(OccupancyGrid, '/environment_map', 10)\n        self.obstacle_map_pub = self.create_publisher(OccupancyGrid, '/obstacle_map', 10)\n        self.status_pub = self.create_publisher(Bool, '/perception_status', 10)\n\n        # Perception system state\n        self.camera_data = None\n        self.lidar_data = None\n        self.imu_data = None\n        self.depth_data = None\n        self.pointcloud_data = None\n        self.camera_info = None\n\n        # Perception buffers\n        self.perception_buffer = deque(maxlen=100)\n        self.fusion_lock = threading.Lock()\n\n        # System status\n        self.system_active = True\n        self.perception_quality = 0.0\n\n        self.get_logger().info('Complete Perception System initialized')\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            self.camera_data = {\n                'image': cv_image,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.update_perception_quality()\n        except Exception as e:\n            self.get_logger().error(f'Camera callback error: {str(e)}')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LiDAR data\"\"\"\n        try:\n            self.lidar_data = {\n                'ranges': np.array(msg.ranges),\n                'intensities': np.array(msg.intensities) if msg.intensities else None,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id,\n                'angle_min': msg.angle_min,\n                'angle_max': msg.angle_max,\n                'angle_increment': msg.angle_increment,\n                'range_min': msg.range_min,\n                'range_max': msg.range_max\n            }\n            self.update_perception_quality()\n        except Exception as e:\n            self.get_logger().error(f'LiDAR callback error: {str(e)}')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        try:\n            self.imu_data = {\n                'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n                'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n                'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z],\n                'timestamp': msg.header.stamp\n            }\n            self.update_perception_quality()\n        except Exception as e:\n            self.get_logger().error(f'IMU callback error: {str(e)}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth data\"\"\"\n        try:\n            depth_image = self.bridge.imgmsg_to_cv2(msg, \"32FC1\")\n            self.depth_data = {\n                'image': depth_image,\n                'timestamp': msg.header.stamp,\n                'frame_id': msg.header.frame_id\n            }\n            self.update_perception_quality()\n        except Exception as e:\n            self.get_logger().error(f'Depth callback error: {str(e)}')\n\n    def info_callback(self, msg):\n        \"\"\"Process camera info\"\"\"\n        try:\n            self.camera_info = msg\n        except Exception as e:\n            self.get_logger().error(f'Camera info callback error: {str(e)}')\n\n    def pointcloud_callback(self, msg):\n        \"\"\"Process point cloud data\"\"\"\n        try:\n            self.pointcloud_data = msg\n            self.update_perception_quality()\n        except Exception as e:\n            self.get_logger().error(f'Point cloud callback error: {str(e)}')\n\n    def update_perception_quality(self):\n        \"\"\"Update overall perception system quality\"\"\"\n        with self.fusion_lock:\n            # Calculate perception quality based on available sensor data\n            quality_score = 0.0\n\n            if self.camera_data is not None:\n                quality_score += 0.25\n            if self.lidar_data is not None:\n                quality_score += 0.25\n            if self.imu_data is not None:\n                quality_score += 0.15\n            if self.depth_data is not None:\n                quality_score += 0.25\n            if self.pointcloud_data is not None:\n                quality_score += 0.10\n\n            self.perception_quality = quality_score\n\n            # Publish system status\n            status_msg = Bool()\n            status_msg.data = (quality_score > 0.5)  # System is active if quality > 50%\n            self.status_pub.publish(status_msg)\n\n    def process_environment_mapping(self):\n        \"\"\"Process all sensor data to create environment map\"\"\"\n        with self.fusion_lock:\n            if not self.system_active:\n                return\n\n            # Create environment map by combining sensor data\n            if self.lidar_data is not None:\n                # Create occupancy grid from LiDAR data\n                occupancy_grid = self.create_occupancy_grid_from_lidar()\n                self.environment_map_pub.publish(occupancy_grid)\n\n            if self.lidar_data is not None and self.camera_data is not None:\n                # Create obstacle map combining LiDAR and vision data\n                obstacle_map = self.create_obstacle_map()\n                self.obstacle_map_pub.publish(obstacle_map)\n\n    def create_occupancy_grid_from_lidar(self):\n        \"\"\"Create occupancy grid from LiDAR data\"\"\"\n        from nav_msgs.msg import OccupancyGrid\n        from geometry_msgs.msg import Point\n\n        # Create a simple occupancy grid\n        grid = OccupancyGrid()\n        grid.header.stamp = self.get_clock().now().to_msg()\n        grid.header.frame_id = \"map\"\n\n        # Define grid parameters\n        resolution = 0.1  # 10cm resolution\n        width = 200  # 20m x 20m area\n        height = 200\n        grid.info.resolution = resolution\n        grid.info.width = width\n        grid.info.height = height\n        grid.info.origin.position.x = -10.0\n        grid.info.origin.position.y = -10.0\n        grid.info.origin.position.z = 0.0\n        grid.info.origin.orientation.w = 1.0\n\n        # Initialize grid with unknown (-1)\n        grid.data = [-1] * (width * height)\n\n        # Process LiDAR ranges\n        ranges = self.lidar_data['ranges']\n        angle_min = self.lidar_data['angle_min']\n        angle_increment = self.lidar_data['angle_increment']\n\n        for i, range_val in enumerate(ranges):\n            if range_val > self.lidar_data['range_min'] and range_val < self.lidar_data['range_max']:\n                angle = angle_min + i * angle_increment\n                x = range_val * np.cos(angle)\n                y = range_val * np.sin(angle)\n\n                # Convert to grid coordinates\n                grid_x = int((x - grid.info.origin.position.x) / resolution)\n                grid_y = int((y - grid.info.origin.position.y) / resolution)\n\n                # Check bounds\n                if 0 <= grid_x < width and 0 <= grid_y < height:\n                    # Mark as occupied (100)\n                    grid.data[grid_y * width + grid_x] = 100\n\n        return grid\n\n    def create_obstacle_map(self):\n        \"\"\"Create obstacle map combining multiple sensors\"\"\"\n        # This would combine LiDAR, camera, and depth data\n        # For simplicity, we'll use the same approach as occupancy grid\n        return self.create_occupancy_grid_from_lidar()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CompletePerceptionSystem()\n\n    try:\n        # Create a timer to periodically process environment mapping\n        timer = node.create_timer(0.1, node.process_environment_mapping)  # 10 Hz\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutting down perception system...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-test-the-perception-system",children:"Step 3: Test the Perception System"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Run the complete perception system:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 complete_perception_system.py\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Monitor the outputs in RViz:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Visualize the environment map\nros2 run rviz2 rviz2\n"})}),"\n",(0,r.jsx)(n.h2,{id:"optimization-techniques",children:"Optimization Techniques"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-perception-optimization",children:"Real-time Perception Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32\nimport time\nimport threading\n\nclass PerceptionOptimizer(Node):\n    def __init__(self):\n        super().__init__('perception_optimizer')\n\n        # Publishers for performance metrics\n        self.fps_pub = self.create_publisher(Float32, '/perception_fps', 10)\n        self.cpu_usage_pub = self.create_publisher(Float32, '/cpu_usage', 10)\n\n        # Performance tracking\n        self.frame_times = []\n        self.max_frame_times = 100\n        self.last_frame_time = time.time()\n\n        # Optimization parameters\n        self.target_fps = 10.0\n        self.current_processing_rate = 1.0  # Process every Nth frame\n\n        # Timer for performance monitoring\n        self.timer = self.create_timer(1.0, self.update_performance_metrics)\n\n        self.get_logger().info('Perception Optimizer initialized')\n\n    def update_performance_metrics(self):\n        \"\"\"Update and publish performance metrics\"\"\"\n        if len(self.frame_times) > 1:\n            avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n            fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0.0\n\n            fps_msg = Float32()\n            fps_msg.data = fps\n            self.fps_pub.publish(fps_msg)\n\n            # Adjust processing rate based on performance\n            if fps < self.target_fps * 0.8:  # Too slow, reduce processing\n                self.current_processing_rate = min(5.0, self.current_processing_rate + 0.1)\n            elif fps > self.target_fps * 1.2:  # Too fast, can process more\n                self.current_processing_rate = max(1.0, self.current_processing_rate - 0.1)\n\n            self.get_logger().info(f'Perception FPS: {fps:.2f}, Processing rate: {self.current_processing_rate:.1f}')\n\n    def record_frame_time(self):\n        \"\"\"Record time taken for frame processing\"\"\"\n        current_time = time.time()\n        frame_time = current_time - self.last_frame_time\n        self.last_frame_time = current_time\n\n        if frame_time > 0:\n            self.frame_times.append(frame_time)\n            if len(self.frame_times) > self.max_frame_times:\n                self.frame_times.pop(0)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionOptimizer()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Sensor Fusion"}),": Combine multiple sensor inputs for robust perception"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Processing"}),": Optimize algorithms for real-time performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibration"}),": Ensure proper calibration of all sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Filtering"}),": Apply appropriate filtering to reduce sensor noise"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Validation"}),": Continuously validate perception results against ground truth"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallback Systems"}),": Implement backup perception methods for critical functions"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you'll have a comprehensive understanding of perception systems for humanoid robots. In Module 4, you'll learn about Vision-Language-Action (VLA) pipelines that integrate these perception capabilities with AI decision-making."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);