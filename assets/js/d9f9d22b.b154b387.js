"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[175],{8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>o});var a=i(6540);const t={},s=a.createContext(t);function r(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(s.Provider,{value:e},n.children)}},9114:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-3/chapter-2/isaac-sim","title":"NVIDIA Isaac Sim","description":"This chapter covers NVIDIA Isaac Sim, a comprehensive robotics simulation platform that provides photorealistic simulation, synthetic data generation, and AI training capabilities for humanoid robots and other robotic systems.","source":"@site/docs/module-3/chapter-2/isaac-sim.md","sourceDirName":"module-3/chapter-2","slug":"/module-3/chapter-2/isaac-sim","permalink":"/ai-robo-learning/docs/module-3/chapter-2/isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/madnan-github/ai-robo-learning/docs/module-3/chapter-2/isaac-sim.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"NVIDIA Isaac Sim"},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Basics with Gazebo","permalink":"/ai-robo-learning/docs/module-3/chapter-1/simulation-basics"},"next":{"title":"Isaac ROS and VSLAM","permalink":"/ai-robo-learning/docs/module-3/chapter-3/isaac-ros-vslam"}}');var t=i(4848),s=i(8453);const r={sidebar_position:2,title:"NVIDIA Isaac Sim"},o="NVIDIA Isaac Sim",l={},d=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding NVIDIA Isaac Sim",id:"understanding-nvidia-isaac-sim",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Setting Up Isaac Sim",id:"setting-up-isaac-sim",level:2},{value:"Installation Requirements",id:"installation-requirements",level:3},{value:"Basic Setup",id:"basic-setup",level:3},{value:"Creating Robot Models in Isaac Sim",id:"creating-robot-models-in-isaac-sim",level:2},{value:"USD Format for Robot Models",id:"usd-format-for-robot-models",level:3},{value:"Robot Configuration File",id:"robot-configuration-file",level:3},{value:"Programming Isaac Sim with Python",id:"programming-isaac-sim-with-python",level:2},{value:"Basic Robot Control Script",id:"basic-robot-control-script",level:3},{value:"Creating Photorealistic Environments",id:"creating-photorealistic-environments",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Data Generation Script",id:"data-generation-script",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"ROS 2 Bridge Setup",id:"ros-2-bridge-setup",level:3},{value:"Hands-on Lab: Isaac Sim Humanoid Robot Simulation",id:"hands-on-lab-isaac-sim-humanoid-robot-simulation",level:2},{value:"Step 1: Create the Environment Setup Script",id:"step-1-create-the-environment-setup-script",level:3},{value:"Step 2: Run the Simulation",id:"step-2-run-the-simulation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Optimization Techniques",id:"optimization-techniques",level:3},{value:"Configuration for Performance",id:"configuration-for-performance",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"nvidia-isaac-sim",children:"NVIDIA Isaac Sim"})}),"\n",(0,t.jsx)(e.p,{children:"This chapter covers NVIDIA Isaac Sim, a comprehensive robotics simulation platform that provides photorealistic simulation, synthetic data generation, and AI training capabilities for humanoid robots and other robotic systems."}),"\n",(0,t.jsx)(e.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,t.jsx)(e.p,{children:"In this chapter, you'll explore:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"NVIDIA Isaac Sim architecture and capabilities"}),"\n",(0,t.jsx)(e.li,{children:"Setting up Isaac Sim for humanoid robot simulation"}),"\n",(0,t.jsx)(e.li,{children:"Creating photorealistic environments"}),"\n",(0,t.jsx)(e.li,{children:"Synthetic data generation for AI training"}),"\n",(0,t.jsx)(e.li,{children:"Integration with ROS 2 and other frameworks"}),"\n",(0,t.jsx)(e.li,{children:"Performance optimization techniques"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Completion of Module 1 and 2"}),"\n",(0,t.jsx)(e.li,{children:"NVIDIA GPU with CUDA support"}),"\n",(0,t.jsx)(e.li,{children:"Isaac Sim installed (Omniverse-based)"}),"\n",(0,t.jsx)(e.li,{children:"Basic understanding of computer graphics and AI concepts"}),"\n",(0,t.jsx)(e.li,{children:"ROS 2 Humble installed"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"understanding-nvidia-isaac-sim",children:"Understanding NVIDIA Isaac Sim"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA Isaac Sim is built on the Omniverse platform and provides:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Photorealistic rendering"}),": Using RTX technology for realistic lighting and materials"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physics simulation"}),": PhysX engine for accurate physics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synthetic data generation"}),": Large-scale data generation for AI training"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 integration"}),": Native ROS 2 support for robotics workflows"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"AI training environment"}),": Reinforcement learning and imitation learning support"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High-Fidelity Graphics"}),": RTX-accelerated rendering for photorealistic environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Large-Scale Simulation"}),": Ability to run thousands of parallel simulations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synthetic Data Generation"}),": Generate labeled training data for computer vision"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot Simulation"}),": Support for various robot types including humanoid robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"ROS 2 Bridge"}),": Seamless integration with ROS 2 ecosystem"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"AI Training Framework"}),": Built-in support for reinforcement learning"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"setting-up-isaac-sim",children:"Setting Up Isaac Sim"}),"\n",(0,t.jsx)(e.h3,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"GPU"}),": NVIDIA RTX series GPU with CUDA support"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"OS"}),": Ubuntu 20.04 or 22.04 (recommended)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RAM"}),": 32GB or more recommended"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Storage"}),": 100GB+ free space for assets"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Omniverse"}),": Isaac Sim runs on NVIDIA Omniverse platform"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"basic-setup",children:"Basic Setup"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim can be launched through Omniverse Launcher or directly:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Launch Isaac Sim\nisaac-sim\n\n# Or launch with specific configuration\nisaac-sim --config=robot_config.yaml\n"})}),"\n",(0,t.jsx)(e.h2,{id:"creating-robot-models-in-isaac-sim",children:"Creating Robot Models in Isaac Sim"}),"\n",(0,t.jsx)(e.h3,{id:"usd-format-for-robot-models",children:"USD Format for Robot Models"}),"\n",(0,t.jsx)(e.p,{children:"Isaac Sim uses Universal Scene Description (USD) format for 3D assets:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-usd",children:'# Example USD file for a simple robot (robot.usd)\n#usda 1.0\n\ndef Xform "Robot" (\n    prepend references = @./base_link.usd@\n)\n{\n    def Xform "LeftLeg" (\n        prepend references = @./leg.usd@\n    )\n    {\n        # Joint constraints and properties\n    }\n\n    def Xform "RightLeg" (\n        prepend references = @./leg.usd@\n    )\n    {\n        # Joint constraints and properties\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"robot-configuration-file",children:"Robot Configuration File"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:'# robot_config.yaml\nrobot:\n  usd_path: "/path/to/robot.usd"\n  position: [0, 0, 1.0]\n  orientation: [0, 0, 0, 1]  # quaternion (x, y, z, w)\n\njoints:\n  left_hip:\n    type: "revolute"\n    limits: [-1.57, 1.57]\n    drive: "position"\n  left_knee:\n    type: "revolute"\n    limits: [0, 1.57]\n    drive: "position"\n  right_hip:\n    type: "revolute"\n    limits: [-1.57, 1.57]\n    drive: "position"\n  right_knee:\n    type: "revolute"\n    limits: [0, 1.57]\n    drive: "position"\n\nsensors:\n  camera:\n    type: "rgb"\n    position: [0.1, 0, 0.8]\n    orientation: [0, 0, 0, 1]\n    resolution: [640, 480]\n    fov: 60\n  lidar:\n    type: "lidar"\n    position: [0.1, 0, 0.5]\n    resolution: [720, 1]\n    range: [0.1, 30.0]\n'})}),"\n",(0,t.jsx)(e.h2,{id:"programming-isaac-sim-with-python",children:"Programming Isaac Sim with Python"}),"\n",(0,t.jsx)(e.h3,{id:"basic-robot-control-script",children:"Basic Robot Control Script"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport omni\nimport carb\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.sensor import _sensor\nimport numpy as np\nimport asyncio\n\nclass IsaacSimRobotController:\n    def __init__(self):\n        self.world = None\n        self.robot = None\n        self.lidar_sensor = None\n        self.camera_sensor = None\n\n    async def setup_simulation(self):\n        """Initialize Isaac Sim world and load robot"""\n        # Create world instance\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Load robot from USD file\n        robot_path = "/path/to/your/robot.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Robot")\n\n        # Add robot to world\n        self.robot = self.world.scene.add(\n            Articulation(\n                prim_path="/World/Robot",\n                name="humanoid_robot",\n                position=np.array([0, 0, 1.0]),\n                orientation=np.array([0, 0, 0, 1])\n            )\n        )\n\n        # Add sensors\n        self.add_sensors()\n\n        # Play the simulation\n        self.world.play()\n\n    def add_sensors(self):\n        """Add sensors to the robot"""\n        # Add camera sensor\n        from omni.isaac.sensor import Camera\n        self.camera_sensor = Camera(\n            prim_path="/World/Robot/Camera",\n            name="camera_sensor",\n            position=np.array([0.1, 0, 0.8]),\n            frequency=30\n        )\n\n        # Add LiDAR sensor\n        from omni.isaac.sensor import RotatingLidarPhysX\n        self.lidar_sensor = RotatingLidarPhysX(\n            prim_path="/World/Robot/Lidar",\n            name="lidar_sensor",\n            translation=np.array([0.1, 0, 0.5]),\n            orientation=np.array([0, 0, 0, 1])\n        )\n\n    async def run_robot_control(self):\n        """Main robot control loop"""\n        for i in range(1000):  # Run for 1000 simulation steps\n            if i % 100 == 0:  # Print every 100 steps\n                print(f"Simulation step: {i}")\n\n            # Get sensor data\n            if self.lidar_sensor:\n                lidar_data = self.lidar_sensor.get_sensor_data()\n                if lidar_data:\n                    ranges = lidar_data.get(\'ranges\', [])\n                    if ranges is not None and len(ranges) > 0:\n                        min_range = np.min(ranges)\n                        print(f"Min LiDAR range: {min_range:.2f}")\n\n            # Control robot based on sensor data\n            await self.control_robot()\n\n            # Step simulation\n            self.world.step(render=True)\n\n            # Small delay to prevent overwhelming the system\n            await asyncio.sleep(0.01)\n\n    async def control_robot(self):\n        """Robot control logic"""\n        # Simple walking pattern\n        current_positions = self.robot.get_joints_state().position\n        target_positions = current_positions.copy()\n\n        # Apply walking gait (simplified)\n        time_step = self.world.current_time_step_index\n        phase = (time_step % 200) / 200.0  # 0 to 1 phase\n\n        # Left leg\n        target_positions[0] = 0.5 * np.sin(2 * np.pi * phase)  # hip\n        target_positions[1] = 0.3 * np.sin(2 * np.pi * phase + np.pi)  # knee\n\n        # Right leg\n        target_positions[2] = 0.5 * np.sin(2 * np.pi * phase + np.pi)  # hip\n        target_positions[3] = 0.3 * np.sin(2 * np.pi * phase)  # knee\n\n        # Apply joint positions\n        self.robot.set_joints_state(positions=target_positions)\n\n    async def cleanup(self):\n        """Clean up simulation"""\n        if self.world:\n            self.world.stop()\n            await self.world.reset_async()\n\n# Main execution\nasync def main():\n    controller = IsaacSimRobotController()\n\n    try:\n        await controller.setup_simulation()\n        await controller.run_robot_control()\n    except Exception as e:\n        print(f"Error during simulation: {e}")\n    finally:\n        await controller.cleanup()\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,t.jsx)(e.h2,{id:"creating-photorealistic-environments",children:"Creating Photorealistic Environments"}),"\n",(0,t.jsx)(e.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import define_prim\nfrom omni.isaac.core.utils.semantics import add_semantics\nfrom pxr import UsdGeom, Gf\nimport numpy as np\n\nclass IsaacSimEnvironment:\n    def __init__(self):\n        self.world = None\n\n    def create_office_environment(self):\n        """Create a photorealistic office environment"""\n        # Add ground plane\n        add_reference_to_stage(\n            usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Environments/Simple_Room/simple_room.usd",\n            prim_path="/World/simple_room"\n        )\n\n        # Add furniture\n        self.add_furniture()\n\n        # Add lighting\n        self.add_lighting()\n\n        # Add textures and materials\n        self.add_materials()\n\n    def add_furniture(self):\n        """Add office furniture"""\n        # Add desk\n        add_reference_to_stage(\n            usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/Materials/metal_desk.usd",\n            prim_path="/World/Desk"\n        )\n\n        # Add chair\n        add_reference_to_stage(\n            usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/Chair/chair.usd",\n            prim_path="/World/Chair"\n        )\n\n        # Position furniture\n        from omni.isaac.core.utils.transforms import set_local_pose\n        set_local_pose("/World/Desk", position=np.array([2, 0, 0]))\n        set_local_pose("/World/Chair", position=np.array([1.5, 0, 0]))\n\n    def add_lighting(self):\n        """Add realistic lighting"""\n        # Add dome light for environment lighting\n        from omni.isaac.core.utils.prims import create_prim\n        create_prim(\n            prim_path="/World/DomeLight",\n            prim_type="DomeLight",\n            position=np.array([0, 0, 0]),\n            attributes={"color": (0.5, 0.5, 0.5), "intensity": 3000}\n        )\n\n        # Add key light\n        create_prim(\n            prim_path="/World/KeyLight",\n            prim_type="DistantLight",\n            position=np.array([5, 5, 10]),\n            attributes={"color": (1, 1, 1), "intensity": 1000}\n        )\n\n    def add_materials(self):\n        """Add realistic materials"""\n        # Create and assign materials\n        from omni.isaac.core.utils.materials import create_diffuse_material\n        create_diffuse_material(\n            prim_path="/World/Looks/wood_material",\n            color=(0.6, 0.4, 0.2)\n        )\n\ndef setup_environment():\n    """Main environment setup function"""\n    env = IsaacSimEnvironment()\n    env.create_office_environment()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,t.jsx)(e.h3,{id:"data-generation-script",children:"Data Generation Script"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport cv2\nimport os\nfrom PIL import Image\nimport json\n\nclass SyntheticDataGenerator:\n    def __init__(self, output_dir="synthetic_data"):\n        self.world = None\n        self.camera = None\n        self.output_dir = output_dir\n        self.data_helper = SyntheticDataHelper()\n\n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n        os.makedirs(f"{output_dir}/images", exist_ok=True)\n        os.makedirs(f"{output_dir}/labels", exist_ok=True)\n\n    async def setup_data_generation(self):\n        """Setup Isaac Sim for data generation"""\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Add robot\n        add_reference_to_stage(\n            usd_path="/path/to/robot.usd",\n            prim_path="/World/Robot"\n        )\n\n        # Add camera for data capture\n        self.camera = Camera(\n            prim_path="/World/Robot/Camera",\n            name="data_camera",\n            position=np.array([0.1, 0, 0.8]),\n            frequency=10  # 10 Hz for data collection\n        )\n\n        # Add various objects for training\n        self.add_training_objects()\n\n        self.world.play()\n\n    def add_training_objects(self):\n        """Add various objects for synthetic data generation"""\n        object_types = [\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/Blocks/block_20cm.usd",\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/YCB/Axis_Aligned/002_master_chef_can.usd",\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/YCB/Axis_Aligned/003_cracker_box.usd",\n            "omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Props/YCB/Axis_Aligned/004_sugar_box.usd"\n        ]\n\n        positions = [\n            np.array([1, 1, 0.1]),\n            np.array([-1, 1, 0.1]),\n            np.array([1, -1, 0.1]),\n            np.array([-1, -1, 0.1])\n        ]\n\n        for i, (obj_path, pos) in enumerate(zip(object_types, positions)):\n            add_reference_to_stage(\n                usd_path=obj_path,\n                prim_path=f"/World/Object_{i}"\n            )\n            from omni.isaac.core.utils.transforms import set_local_pose\n            set_local_pose(f"/World/Object_{i}", position=pos)\n\n    async def generate_dataset(self, num_samples=1000):\n        """Generate synthetic dataset"""\n        data_samples = []\n\n        for i in range(num_samples):\n            # Move objects randomly for variation\n            self.randomize_scene()\n\n            # Step simulation to let objects settle\n            for _ in range(10):\n                self.world.step(render=True)\n\n            # Capture data\n            sample_data = await self.capture_sample(i)\n            data_samples.append(sample_data)\n\n            if i % 100 == 0:\n                print(f"Generated {i}/{num_samples} samples")\n\n        # Save dataset metadata\n        self.save_metadata(data_samples)\n\n    def randomize_scene(self):\n        """Randomize object positions and lighting"""\n        import random\n\n        # Randomize object positions\n        for i in range(4):  # 4 objects\n            new_pos = np.array([\n                random.uniform(-2, 2),\n                random.uniform(-2, 2),\n                random.uniform(0.1, 0.5)\n            ])\n            from omni.isaac.core.utils.transforms import set_local_pose\n            set_local_pose(f"/World/Object_{i}", position=new_pos)\n\n        # Randomize lighting\n        # (Implementation would depend on specific lighting setup)\n\n    async def capture_sample(self, sample_id):\n        """Capture a single data sample"""\n        # Get RGB image\n        rgb_data = self.camera.get_rgb()\n        if rgb_data is not None:\n            rgb_image = Image.fromarray((rgb_data * 255).astype(np.uint8))\n            rgb_path = f"{self.output_dir}/images/rgb_{sample_id:06d}.png"\n            rgb_image.save(rgb_path)\n\n        # Get depth image\n        depth_data = self.camera.get_depth()\n        if depth_data is not None:\n            depth_image = Image.fromarray((depth_data * 1000).astype(np.uint16))  # Scale for 16-bit\n            depth_path = f"{self.output_dir}/images/depth_{sample_id:06d}.png"\n            depth_image.save(depth_path)\n\n        # Get segmentation\n        seg_data = self.camera.get_semantic_segmentation()\n        if seg_data is not None:\n            seg_image = Image.fromarray((seg_data).astype(np.uint16))\n            seg_path = f"{self.output_dir}/images/seg_{sample_id:06d}.png"\n            seg_image.save(seg_path)\n\n        # Create sample metadata\n        sample_metadata = {\n            "id": sample_id,\n            "rgb_path": f"images/rgb_{sample_id:06d}.png",\n            "depth_path": f"images/depth_{sample_id:06d}.png",\n            "seg_path": f"images/seg_{sample_id:06d}.png",\n            "timestamp": self.world.current_time_step_index,\n            "robot_pose": self.get_robot_pose(),\n            "object_poses": self.get_object_poses()\n        }\n\n        return sample_metadata\n\n    def get_robot_pose(self):\n        """Get current robot pose"""\n        # Implementation would depend on robot setup\n        return {"position": [0, 0, 0], "orientation": [0, 0, 0, 1]}\n\n    def get_object_poses(self):\n        """Get poses of all objects in scene"""\n        poses = {}\n        for i in range(4):\n            # Implementation would get actual object poses\n            poses[f"object_{i}"] = {"position": [0, 0, 0], "orientation": [0, 0, 0, 1]}\n        return poses\n\n    def save_metadata(self, data_samples):\n        """Save dataset metadata"""\n        metadata = {\n            "dataset_name": "Humanoid Robot Perception Dataset",\n            "num_samples": len(data_samples),\n            "created": "2024-01-01",\n            "description": "Synthetic dataset for humanoid robot perception",\n            "samples": data_samples\n        }\n\n        with open(f"{self.output_dir}/metadata.json", "w") as f:\n            json.dump(metadata, f, indent=2)\n\n# Main execution\nasync def generate_synthetic_data():\n    generator = SyntheticDataGenerator()\n    await generator.setup_data_generation()\n    await generator.generate_dataset(num_samples=1000)\n    print("Synthetic dataset generation completed!")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,t.jsx)(e.h3,{id:"ros-2-bridge-setup",children:"ROS 2 Bridge Setup"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, JointState, CameraInfo\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport asyncio\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera, RotatingLidarPhysX\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.articulations import Articulation\n\nclass IsaacSimROSBridge(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sim_ros_bridge\')\n\n        # ROS publishers\n        self.image_pub = self.create_publisher(Image, \'/camera/image_raw\', 10)\n        self.scan_pub = self.create_publisher(LaserScan, \'/scan\', 10)\n        self.joint_state_pub = self.create_publisher(JointState, \'/joint_states\', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, \'/camera/camera_info\', 10)\n\n        # ROS subscribers\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.cmd_vel_callback, 10)\n        self.joint_cmd_sub = self.create_subscription(\n            JointState, \'/joint_commands\', self.joint_cmd_callback, 10)\n\n        # CV bridge for image conversion\n        self.cv_bridge = CvBridge()\n\n        # Isaac Sim components\n        self.world = None\n        self.robot = None\n        self.camera = None\n        self.lidar = None\n\n        # Robot command storage\n        self.current_cmd_vel = Twist()\n        self.current_joint_cmd = JointState()\n\n        # Setup Isaac Sim\n        self.setup_isaac_sim()\n\n        # Timer for publishing sensor data\n        self.timer = self.create_timer(0.1, self.publish_sensor_data)\n\n    def setup_isaac_sim(self):\n        """Setup Isaac Sim environment"""\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Load robot\n        add_reference_to_stage(\n            usd_path="/path/to/robot.usd",\n            prim_path="/World/Robot"\n        )\n\n        self.robot = self.world.scene.add(\n            Articulation(\n                prim_path="/World/Robot",\n                name="humanoid_robot",\n                position=np.array([0, 0, 1.0])\n            )\n        )\n\n        # Add sensors\n        self.camera = self.world.scene.add(\n            Camera(\n                prim_path="/World/Robot/Camera",\n                name="camera_sensor",\n                position=np.array([0.1, 0, 0.8]),\n                frequency=30\n            )\n        )\n\n        self.lidar = self.world.scene.add(\n            RotatingLidarPhysX(\n                prim_path="/World/Robot/Lidar",\n                name="lidar_sensor",\n                translation=np.array([0.1, 0, 0.5])\n            )\n        )\n\n        self.world.play()\n\n    def cmd_vel_callback(self, msg):\n        """Handle velocity commands from ROS"""\n        self.current_cmd_vel = msg\n        # Process velocity command for robot movement\n        self.process_velocity_command(msg)\n\n    def joint_cmd_callback(self, msg):\n        """Handle joint commands from ROS"""\n        self.current_joint_cmd = msg\n        # Apply joint commands to robot\n        self.apply_joint_commands(msg)\n\n    def process_velocity_command(self, cmd_vel):\n        """Process velocity command and convert to joint movements"""\n        # Simple differential drive to joint conversion (example)\n        linear_x = cmd_vel.linear.x\n        angular_z = cmd_vel.angular.z\n\n        # Convert to joint velocities for humanoid (simplified)\n        # In reality, this would involve inverse kinematics\n        pass\n\n    def apply_joint_commands(self, joint_cmd):\n        """Apply joint commands to the robot"""\n        if self.robot and joint_cmd.position:\n            try:\n                self.robot.set_joints_state(positions=np.array(joint_cmd.position))\n            except Exception as e:\n                self.get_logger().error(f"Error setting joint states: {e}")\n\n    def publish_sensor_data(self):\n        """Publish sensor data to ROS topics"""\n        if not self.world:\n            return\n\n        # Step simulation\n        self.world.step(render=False)\n\n        # Publish joint states\n        self.publish_joint_states()\n\n        # Publish camera data\n        self.publish_camera_data()\n\n        # Publish LiDAR data\n        self.publish_lidar_data()\n\n    def publish_joint_states(self):\n        """Publish joint state information"""\n        if not self.robot:\n            return\n\n        try:\n            joint_positions = self.robot.get_joints_state().position\n            joint_velocities = self.robot.get_joints_state().velocity\n            joint_efforts = self.robot.get_joints_state().effort\n\n            msg = JointState()\n            msg.name = [f"joint_{i}" for i in range(len(joint_positions))]\n            msg.position = list(joint_positions)\n            msg.velocity = list(joint_velocities)\n            msg.effort = list(joint_efforts)\n            msg.header.stamp = self.get_clock().now().to_msg()\n            msg.header.frame_id = "base_link"\n\n            self.joint_state_pub.publish(msg)\n        except Exception as e:\n            self.get_logger().error(f"Error publishing joint states: {e}")\n\n    def publish_camera_data(self):\n        """Publish camera image data"""\n        if not self.camera:\n            return\n\n        try:\n            rgb_data = self.camera.get_rgb()\n            if rgb_data is not None:\n                # Convert to ROS Image message\n                ros_image = self.cv_bridge.cv2_to_imgmsg(rgb_data, "rgb8")\n                ros_image.header.stamp = self.get_clock().now().to_msg()\n                ros_image.header.frame_id = "camera_link"\n\n                self.image_pub.publish(ros_image)\n\n                # Publish camera info\n                self.publish_camera_info()\n        except Exception as e:\n            self.get_logger().error(f"Error publishing camera data: {e}")\n\n    def publish_camera_info(self):\n        """Publish camera info"""\n        info_msg = CameraInfo()\n        info_msg.header.stamp = self.get_clock().now().to_msg()\n        info_msg.header.frame_id = "camera_link"\n        info_msg.height = 480\n        info_msg.width = 640\n        info_msg.distortion_model = "plumb_bob"\n        info_msg.d = [0.0, 0.0, 0.0, 0.0, 0.0]  # Distortion coefficients\n        info_msg.k = [320.0, 0.0, 320.0, 0.0, 320.0, 240.0, 0.0, 0.0, 1.0]  # Camera matrix\n        info_msg.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  # Rectification matrix\n        info_msg.p = [320.0, 0.0, 320.0, 0.0, 0.0, 320.0, 240.0, 0.0, 0.0, 0.0, 1.0, 0.0]  # Projection matrix\n\n        self.camera_info_pub.publish(info_msg)\n\n    def publish_lidar_data(self):\n        """Publish LiDAR scan data"""\n        if not self.lidar:\n            return\n\n        try:\n            lidar_data = self.lidar.get_sensor_data()\n            if lidar_data and \'ranges\' in lidar_data:\n                ranges = lidar_data[\'ranges\']\n\n                if ranges is not None and len(ranges) > 0:\n                    scan_msg = LaserScan()\n                    scan_msg.header.stamp = self.get_clock().now().to_msg()\n                    scan_msg.header.frame_id = "lidar_link"\n                    scan_msg.angle_min = -np.pi\n                    scan_msg.angle_max = np.pi\n                    scan_msg.angle_increment = (2 * np.pi) / len(ranges)\n                    scan_msg.time_increment = 0.0\n                    scan_msg.scan_time = 0.1\n                    scan_msg.range_min = 0.1\n                    scan_msg.range_max = 30.0\n                    scan_msg.ranges = list(ranges)\n                    scan_msg.intensities = []  # Empty for now\n\n                    self.scan_pub.publish(scan_msg)\n        except Exception as e:\n            self.get_logger().error(f"Error publishing lidar data: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Initialize Isaac Sim in a separate thread\n    import threading\n\n    def run_isaac_sim():\n        # Run Isaac Sim loop\n        import asyncio\n        async def sim_loop():\n            bridge = IsaacSimROSBridge()\n\n            try:\n                # Keep running the bridge\n                while rclpy.ok():\n                    # Process ROS callbacks\n                    rclpy.spin_once(bridge, timeout_sec=0.01)\n\n                    # Update Isaac Sim\n                    if bridge.world:\n                        bridge.world.step(render=False)\n\n                    # Small delay to prevent overwhelming\n                    await asyncio.sleep(0.01)\n            except Exception as e:\n                bridge.get_logger().error(f"Isaac Sim loop error: {e}")\n            finally:\n                bridge.destroy_node()\n\n        asyncio.run(sim_loop())\n\n    # Start Isaac Sim in background thread\n    sim_thread = threading.Thread(target=run_isaac_sim, daemon=True)\n    sim_thread.start()\n\n    # Keep main thread alive\n    try:\n        sim_thread.join()\n    except KeyboardInterrupt:\n        pass\n\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-lab-isaac-sim-humanoid-robot-simulation",children:"Hands-on Lab: Isaac Sim Humanoid Robot Simulation"}),"\n",(0,t.jsx)(e.p,{children:"In this lab, you'll create a complete Isaac Sim environment with a humanoid robot and ROS integration."}),"\n",(0,t.jsx)(e.h3,{id:"step-1-create-the-environment-setup-script",children:"Step 1: Create the Environment Setup Script"}),"\n",(0,t.jsxs)(e.p,{children:["Create ",(0,t.jsx)(e.code,{children:"isaac_sim_humanoid.py"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.sensor import Camera, RotatingLidarPhysX\nfrom omni.isaac.core.utils.semantics import add_semantics\nimport numpy as np\nimport asyncio\n\nclass IsaacSimHumanoidLab:\n    def __init__(self):\n        self.world = None\n        self.robot = None\n        self.camera = None\n        self.lidar = None\n\n    async def setup_environment(self):\n        """Setup complete Isaac Sim environment"""\n        print("Setting up Isaac Sim environment...")\n\n        # Initialize world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Add ground plane\n        add_reference_to_stage(\n            usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Environments/Simple_Room/simple_room.usd",\n            prim_path="/World/defaultGroundPlane"\n        )\n\n        # Add robot\n        # For this example, we\'ll use a simple robot USD\n        # In practice, you would use your own humanoid robot model\n        robot_path = "omniverse://localhost/NVIDIA/Assets/Isaac/4.1/Isaac/Robots/Franka/franka.usd"\n        add_reference_to_stage(\n            usd_path=robot_path,\n            prim_path="/World/Robot"\n        )\n\n        # Add robot to scene\n        self.robot = self.world.scene.add(\n            Articulation(\n                prim_path="/World/Robot",\n                name="humanoid_robot",\n                position=np.array([0, 0, 0.5]),\n                orientation=np.array([0, 0, 0, 1])\n            )\n        )\n\n        # Add sensors\n        self.camera = self.world.scene.add(\n            Camera(\n                prim_path="/World/Robot/panda_hand/Camera",\n                name="camera_sensor",\n                position=np.array([0.1, 0, 0.1]),\n                frequency=30\n            )\n        )\n\n        self.lidar = self.world.scene.add(\n            RotatingLidarPhysX(\n                prim_path="/World/Robot/panda_hand/Lidar",\n                name="lidar_sensor",\n                translation=np.array([0.15, 0, 0.1])\n            )\n        )\n\n        print("Environment setup complete!")\n\n    async def run_simulation(self):\n        """Run the simulation loop"""\n        print("Starting simulation...")\n\n        # Play the simulation\n        self.world.play()\n\n        # Run for a number of steps\n        for step in range(10000):\n            # Step the world\n            self.world.step(render=True)\n\n            # Every 100 steps, print status\n            if step % 100 == 0:\n                print(f"Simulation step: {step}")\n\n                # Get and print sensor data\n                if self.lidar:\n                    lidar_data = self.lidar.get_sensor_data()\n                    if lidar_data and \'ranges\' in lidar_data:\n                        ranges = lidar_data[\'ranges\']\n                        if ranges is not None and len(ranges) > 0:\n                            min_range = np.min(ranges)\n                            print(f"  Min LiDAR range: {min_range:.2f}m")\n\n            # Small delay to prevent overwhelming the system\n            await asyncio.sleep(0.01)\n\n    async def cleanup(self):\n        """Clean up the simulation"""\n        if self.world:\n            self.world.stop()\n            await self.world.reset_async()\n        print("Simulation cleaned up!")\n\nasync def main():\n    lab = IsaacSimHumanoidLab()\n\n    try:\n        await lab.setup_environment()\n        await lab.run_simulation()\n    except Exception as e:\n        print(f"Error during simulation: {e}")\n    finally:\n        await lab.cleanup()\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,t.jsx)(e.h3,{id:"step-2-run-the-simulation",children:"Step 2: Run the Simulation"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Launch Isaac Sim:"}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"isaac-sim\n"})}),"\n",(0,t.jsxs)(e.ol,{start:"2",children:["\n",(0,t.jsx)(e.li,{children:"Run your Python script:"}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"python3 isaac_sim_humanoid.py\n"})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"optimization-techniques",children:"Optimization Techniques"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Level of Detail (LOD)"}),": Use simpler models when the camera is far away"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Culling"}),": Don't render objects outside the camera's view"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Batch Processing"}),": Process multiple simulation instances in parallel"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Texture Streaming"}),": Load textures on demand rather than all at once"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physics Simplification"}),": Use simpler collision meshes than visual meshes"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"configuration-for-performance",children:"Configuration for Performance"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Performance optimization settings\ndef configure_performance_settings():\n    """Configure Isaac Sim for optimal performance"""\n\n    # Set rendering quality\n    carb.settings.get_settings().set("/app/window/spp", 1)  # Samples per pixel\n    carb.settings.get_settings().set("/rtx/ambientOcclusion/enabled", False)\n    carb.settings.get_settings().set("/rtx/dlss/enable", True)  # If DLSS is available\n\n    # Physics settings\n    carb.settings.get_settings().set("/physics_solver_frequency", 60)\n    carb.settings.get_settings().set("/physics_max_substeps", 1)\n\n    # Rendering settings\n    carb.settings.get_settings().set("/app/renderer/enabled", True)\n    carb.settings.get_settings().set("/app/renderer/resolution/width", 640)\n    carb.settings.get_settings().set("/app/renderer/resolution/height", 480)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Asset Management"}),": Organize your 3D assets efficiently"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene Complexity"}),": Balance visual quality with performance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Data Pipeline"}),": Create efficient workflows for synthetic data generation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Validation"}),": Always validate simulation results against real-world data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Documentation"}),": Document your simulation setups and configurations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Version Control"}),": Use version control for your USD scenes and configurations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"After completing this chapter, you'll be ready to learn about Isaac ROS and hardware-accelerated perception in Chapter 3."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}}}]);